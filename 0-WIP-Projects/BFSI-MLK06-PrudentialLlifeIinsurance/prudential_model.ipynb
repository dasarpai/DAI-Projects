{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This framework prepared by Hari Thapliyal (PGDS - IIITB, Cohort10) hari.prasad@vedavit-ps.com__<br>\n",
    "__Any suggestion to improve are most welcome__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the steps mentioned are suggestive. Based on the datasize, number of features, \n",
    "type of columns, goal of ML project (regression, classification, clustering, timeseries-regression) you can opt-out some steps or add some more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=6>__Loading All Needed Libraries__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc # for deleting unused variables\n",
    "\n",
    "#Number Manupulation & Plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Model Selection Libraries\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "#Preprocessing Libraries\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Liner Model Libraries\n",
    "#from sklearn import linear_model\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.linear_model import Ridge\n",
    "#from sklearn.linear_model import Lasso\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "\n",
    "#pipeline libraries\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, roc_curve, auc\n",
    "\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.feature_selection import RFE\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#folder= \"/content/drive/My Drive/Colab Notebooks/Datasets-Kaggle/\"\n",
    "folder=\"\"\n",
    "file1 = folder + \"prudential_train_clean_data.csv\"\n",
    "file2 = folder + \"prudential_all_fldmap.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=6>__Define Objective__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=6>__Understanding Dataset__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Identify Fields which has more than 80% null values__ <br>\n",
    "_Drop those columns or write down your approach here how will you impute those_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prepare list of different type of columns for processing them in group<br>\n",
    "Record Identified, typical \"Id\"<br>\n",
    "Target Variable, typical \"Response\", \"Target\", \"Class\" etc, this is also called y, which you have to predict<br>\n",
    "Categorical Nominal Columns<br>\n",
    "Categorical Oridinal Columns<br>\n",
    "Date/time columns<br>\n",
    "Numerical Columns<br>\n",
    "Id & Target Variable never scaled<br>\n",
    "Oridnal columns has hotencoding<br>\n",
    "Nomial Columns has dummy columns<br>__\n",
    "_ONLY Numerical columns need to be scaled<br>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"Response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"prudential_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df uses {0} MB'.format(df.memory_usage().sum()/1024**2))\n",
    "#print(df.memory_usage())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(\"int64\").max().sort_values()[::-1]\n",
    "tmp_cols =dict(df.select_dtypes(\"int64\").max().sort_values()[::-1])\n",
    "\n",
    "for k,v in tmp_cols.items():\n",
    "    if v>32000:\n",
    "        df[k] = df[k].astype(\"uint32\")\n",
    "    else:\n",
    "        df[k] = df[k].astype(\"uint16\")\n",
    "\n",
    "#df.select_dtypes(\"float64\").max().sort_values()[::-1]\n",
    "tmp_cols = df.select_dtypes(\"float64\").columns\n",
    "\n",
    "df[tmp_cols] = df[tmp_cols].astype(\"float32\")\n",
    "\n",
    "#df.select_dtypes(\"object\").max().sort_values()[::-1]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Null Value Columns\n",
    "(df.isnull().sum()/len(df)*100).sort_values()[::-1].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Identify Fields which has more than 90% null values\n",
    "f= list((df.isnull().sum() /len(df)*100) >45)\n",
    "cols_45perNullVal = df.iloc[:,f].columns\n",
    "\n",
    "print (\"Dropping {} fields: {}\".format( len(cols_45perNullVal), cols_45perNullVal))\n",
    "df.drop(columns=cols_45perNullVal, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBinary_flds(df):\n",
    "    bin_flds = []\n",
    "    for col in df.columns:\n",
    "        if len(df[col].unique())==2:\n",
    "            bin_flds.append(col)\n",
    "    return bin_flds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of Categorical Variables. This step is useful when there are many fields in dataset\n",
    "#If they are numeric in nature, convert into Object\n",
    "#In final dataset all categorical variable (ordinal, nominal) will be object type\n",
    "def getCat_flds(df):\n",
    "    categories = min(30, int(len(df)*.5))\n",
    "    cat_flds = []\n",
    "    for col in df.columns:\n",
    "        if len(df[col].unique())<categories:\n",
    "            cat_flds.append(col)\n",
    "            print (col, df[col].unique())\n",
    "    return cat_flds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_flds = getCat_flds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#None of these categorical field are ordinal field. \n",
    "#If there is any then we need to list them separately in the below column identification step\n",
    "print (\"There are %d categorical fields in dataset\"%(len(cat_flds)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################All Columns of Different Types\n",
    "##All Numerial Fields in Dataset\n",
    "cols_all_num = list(set(list(df.columns)).difference(set(cat_flds)))\n",
    "\n",
    "##All Categorical Fields in Dataset\n",
    "cols_all_cat = cat_flds\n",
    "\n",
    "#if target variable is categorical then remove from cols_all_cat list and else remove from cols_all_num\n",
    "try:\n",
    "    cols_all_num.remove(target)\n",
    "except: pass\n",
    "\n",
    "try:\n",
    "    cols_all_cat.remove(target)\n",
    "except: pass\n",
    "    \n",
    "##All Numerial Fields in Dataset which has Ordinal Variable\n",
    "cols_all_ord  = []\n",
    "#cols_all_ord  = ['OverallQual','OverallCond','HalfBath','FullBath','BsmtFullBath','BsmtHalfBath',\n",
    "#                 'BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','MoSold',\n",
    "#                 'YrSold','MSSubClass']\n",
    "\n",
    "##All Numerial Fields in Dataset which has Ordinal Variable\n",
    "cols_all_nom = list(set(cols_all_cat).difference(set(cols_all_ord)))\n",
    "\n",
    "\n",
    "############################All Null Value Columns\n",
    "#All Fields which has any Null values\n",
    "fld_TF = list((df.isnull().sum() /len(df)*100) >0)\n",
    "cols_with_NullVal_all = df.iloc[:,fld_TF].columns\n",
    "#Numeric Fields which has any Null values\n",
    "cols_with_NullVal_num= list(  df[cols_all_num].iloc[ :,list(df[cols_all_num].isna().sum()>0) ].columns )\n",
    "#Categorical Fields which has any Null values\n",
    "cols_with_NullVal_cat= list(  df[cols_all_cat].iloc[ :,list(df[cols_all_cat].isna().sum()>0) ].columns )\n",
    "#Ordinal Fields which has Null Values\n",
    "fld_TF = list((df[cols_all_ord].isnull().sum() /len(df)*100) >0)\n",
    "cols_with_NullVal_ord = df.iloc[:,fld_TF].columns\n",
    "\n",
    "\n",
    "###########################All Non-Null Value Columns\n",
    "#All Fields which has any Null values\n",
    "fld_TF = list((df.isnull().sum() /len(df)*100) ==0)\n",
    "cols_with_NonNullVal_all =df.iloc[:,fld_TF].columns\n",
    "#Numeric Fields which has any Null values\n",
    "cols_with_NonNullVal_num= list(  df[cols_all_num].iloc[ :,list(df[cols_all_num].isna().sum()==0) ].columns )\n",
    "#Categorical Fields which has any Null values\n",
    "cols_with_NonNullVal_cat= list(  df[cols_all_cat].iloc[ :,list(df[cols_all_cat].isna().sum()==0) ].columns )\n",
    "#Ordinal Fields which has Null Values\n",
    "fld_TF = list((df[cols_all_ord].isnull().sum() /len(df)*100) ==0)\n",
    "cols_with_NonNullVal_ord =df.iloc[:,fld_TF].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=6>__Check Null Value Fields & Existing Values in the Fields__</font><br>\n",
    "__Null Value Categorical Nominal Columns<br>\n",
    "Null Value Categorical Oridinal Columns<br>\n",
    "Null Value Date/time columns<br>\n",
    "Null Value Numerical Columns<br>\n",
    "Null Value Id & Target Variable never scaled<br>\n",
    "Null Value Oridnal columns has hotencoding<br>\n",
    "Null Value Nomial Columns has dummy columns<br>__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=6>__Check Data Imbalance & Outliers__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <font color=red size=4> Target Variable Data Imbalance</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4> Categorical Variable Data Imbalance</fond>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4> Oridinal Variable Data Imbalance</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4> Numerical Variable Data Distribution</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4> __Decide and take action on outliers at this stage__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=6>__Imputing Null Values__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4> Impute ONLY numerical values using Iterative Imputer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Except Numerical columns restore the dataset as orginal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4> Impute ONLY  Ordinal values using KNN Imputer</font> <br>\n",
    "_I have writen python function for this. This function convert existing non-null values into number, Impute null values using the number and returns a integer column. This function returns a column with new values, & map between old values and new number values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Except Numerical & Oridinal Value columns restore the dataset as orginal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4> Impute ONLY  Nominal values using KNN Imputer</font> <br>\n",
    "_I have writen python function for this. This function convert existing non-null values into number, Impute null values using the number and returns a integer column. This function returns a column with new values, & map between old values and new number values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=5><center>__##########Data Cleaning Ends Here##########__<center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=6>__Visualising Relationship between y & X__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4>Check y & Cateorical Variable Relationship</font> : _scatter plot is the best_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4>Check y & Ordinal Variable Relationship</font> : _scatter plot is the best_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4> Check y & Numeric Variable Relationship</font> : _scatter plot is the best_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4> Check Degree of Correlation between y & Numeric Variable</font> : _Heatmap plot is the best_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=6>__Feature Engineering__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If target variable is not normally distributed. Then perform feature engineering.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=5><center>__####Preprocessing Work before Modelling Starts Here####__<center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=3> Create Dummy Fields</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=green>__PCA (Optional)__</font><br>\n",
    "_Compress Variables in PCA, if number of variables are large_<br>\n",
    "__Decide how many PCA features you need__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=red>If PCA Done, then Visulise Relationship between Selected PCA Features & Target</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=red>If PCA Done, then Feature Engineering on PCA Dataset may be required</font><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=3>Split Dataset in Train & Test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=3>Split Dataset in Train & Test in X & y & id</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=3>Scale Numeric Fields</font><br>\n",
    "__You can use StandardScalar<br>\n",
    "fit_transform on train datset<br>\n",
    "transform on test dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <font color=red> <center> Enable Cell Below if reading cleaned data.</center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(file1)\n",
    "all_fldmap = pd.read_csv(file2)\n",
    "all_fldmap = all_fldmap.drop(columns=\"Unnamed: 0\")\n",
    "df   = df.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "print('df uses {0} MB'.format(df.memory_usage().sum()/1024**2))\n",
    "#print(df.memory_usage())\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_flds = getBinary_flds(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[bin_flds] = df[bin_flds].astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df uses {0} MB'.format(df.memory_usage().sum()/1024**2))\n",
    "#print(df.memory_usage())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset at this stage has dummy fields. All categorical columns converted into dummy fields.\n",
    "#Identify columns which should be scale\n",
    "colname=[]\n",
    "colmin=[]\n",
    "colmax=[]\n",
    "for c in df.columns:\n",
    "    colname.append(c)\n",
    "    colmin.append( df[c].min())\n",
    "    colmax.append( df[c].max())\n",
    "df_minmax = pd.DataFrame( {\"colname\": colname, \"colmin\": colmin, \"colmax\": colmax})\n",
    "#df_minmax.to_csv(\"minmax1.csv\")\n",
    "\n",
    "#if min is 0 and max=1 then it id dummy field. So we need not to scale\n",
    "col_int = list(df_minmax[ ( (df_minmax.colmin==0) & (df_minmax.colmax==1) )].colname)\n",
    "\n",
    "col_to_scale = df_minmax[~ ( (df_minmax.colmin==0) & (df_minmax.colmax==1) )]\n",
    "col_to_scale = list(col_to_scale.colname)\n",
    "col_to_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5>__Split Dataset & Scale Numeric Fields__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Dataset in X & y\n",
    "Xcols = list(df.columns)\n",
    "Xcols.remove(target)\n",
    "\n",
    "X = df[Xcols]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X, y into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size=0.7,\n",
    "                                                    test_size = 0.3, random_state=100)\n",
    "X_train_id = X_train.Id\n",
    "X_test_id = X_test.Id\n",
    "\n",
    "X_train = X_train.drop(columns=\"Id\")\n",
    "X_test = X_test.drop(columns=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.mean())\n",
    "print(y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the numeric feature features in train and test\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "col_to_scale.remove(\"Id\")\n",
    "\n",
    "try:\n",
    "    col_to_scale.remove(target)\n",
    "except Exception as e:\n",
    "    print (e)\n",
    "sc = StandardScaler()\n",
    "X_train[col_to_scale] =pd.DataFrame( sc.fit_transform( X_train[col_to_scale] ), index=X_train.index )\n",
    "X_test[col_to_scale]  =pd.DataFrame( sc.transform( X_test[col_to_scale] ) , index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=5><center>__##########Modelling Start Here##########__<center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=8>__Start Modeling__</font><br>\n",
    "- Here onwards you can build model either PCA dataset or Normal or Both<br>\n",
    "- You need to try multiple models and check the performance of each<br>\n",
    "- Either choose one or combine the results in one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=5><center>__##########Only for Regression Problems##########__<center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5>__Linear Regression__</font><br>\n",
    "_If number of variables are less then this is the best rather going for Lasso/Ridge_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5>__Lasso Regression__</font><br>\n",
    "_Build model using GridSearchCV_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot the score of GridSearchCV with alpha__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finalise the optimical value of alpha for Lasso & Redbuid the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check Lasso Regression R Squre Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=black>__Columns selected by Lasso May Have Multicolineaity So need to Address that__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>__Refining the model using linear regression, RFE and VIF__</font><br>\n",
    "__Build linear regression model using statsmodel<br>\n",
    "Use only those columns which are given by Lasso model<br>\n",
    "Check statsmodel.summary() and drop columns of high p value<br>\n",
    "Check VIF, drop the columns with high VIF<br>__\n",
    "_All above steps are iterative in nature. At every step you remove multicolinearity and strengthen model.<br>\n",
    "For removing features based on vif or pvalue and rebuilding the model I have writen python code_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finally Check Multi Colinearity using VIF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>__Build the final model using Lasso.__<font><br>\n",
    "    Use only above selected features and earlier decided alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check R Square Score of the Final Lasso Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot the Prediction Errors & their Randomness__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5>__Ridge Regression__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ridge Regression Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Elastic Regression__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ElasticNet Regression Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Decision Tree Regression__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__K Nearest Neighbour Regression__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Random Forest__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__RandomForest Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__XGBoosting__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__XGBoosting Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=5><center>__##########Only for Classification Problems Here##########__<center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_report(y_true, y_pred, y_score=None, average='micro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true %s is not the same shape as y_pred %s\" % (\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='weighted'))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred'] = pred_cnt\n",
    "    class_report_df['pred'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_it, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_it])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Logistic Regression__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_logreg= LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "#params = {\"C\": [.1,.2,.3,.4,.5,.6,.7,.8,.9], 'penalty':['l1','l2'], 'class_weight': ['balanced',{0:0.38, 1:0.62}]}\n",
    "#model_logreg = LogisticRegression()\n",
    "#grid_model_logreg = GridSearchCV(model_logreg, params_grid = params, scoring=\"recall\", cv=5 )\n",
    "\n",
    "model_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=model_logreg.predict(X_test), \n",
    "    y_score=model_logreg.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Gaussian Na√Øve Bayes__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model_gnb = GaussianNB(class_weight=\"balanced\")\n",
    "\n",
    "#params = {\"C\": [.1,.2,.3,.4,.5,.6,.7,.8,.9], 'penalty':['l1','l2'], 'class_weight': ['balanced',{0:0.38, 1:0.62}]}\n",
    "#model_logreg = LogisticRegression()\n",
    "#grid_model_logreg = GridSearchCV(model_gnb, params_grid = params, scoring=\"recall\", cv=5 )\n",
    "\n",
    "model_gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=model_gnb.predict(X_test), \n",
    "    y_score=model_gnb.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Stochastic Gradient Descent Classifier__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss=\"modified_huber\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__K-Nearest Neighbors Classifier__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_knn = KNeighborsClassifier(n_neighbors=8)\n",
    "model_knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=model_knn.predict(X_test), \n",
    "    y_score=model_knn.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Hierarchical Tree__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import cut_tree\n",
    "\n",
    "# plot a dendogram complete linkage\n",
    "plt.figure(figsize=(15,5))\n",
    "mergings = linkage(df_pca_country_scaled, method=\"complete\", metric='euclidean')\n",
    "dendrogram(mergings)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__SVM__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svm = SVC(kernel=\"linear\", C=.025, random_state=101)\n",
    "\n",
    "#folds = 3\n",
    "#grid_search_svm = GridSearchCV(model_svm, \n",
    "#                               cv = folds,\n",
    "#                               param_grid=param_grid, \n",
    "#                               scoring = 'roc_auc', \n",
    "#                               return_train_score=True,                         \n",
    "#                               verbose = 1)\n",
    "\n",
    "#grid_search_svm.fit(X_train,  y_train)\n",
    "\n",
    "model_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=model_svm.predict(X_test), \n",
    "    y_score=model_svm.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Decision Tree Classifier__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_dtree = DecisionTreeClassifier(max_depth=10, min_samples_leaf=10, min_impurity_split=10)\n",
    "\n",
    "#folds = 3\n",
    "#grid_search_dtree = GridSearchCV(model_dtree, \n",
    "#                               cv = folds,\n",
    "#                               param_grid=param_grid, \n",
    "#                               scoring = 'roc_auc', \n",
    "#                               return_train_score=True,                         \n",
    "#                               verbose = 1)\n",
    "\n",
    "#grid_search_dtree.fit(X_train,  y_train)\n",
    "\n",
    "model_dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=model_dtree.predict(X_test), \n",
    "    y_score=model_dtree.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Randomforest Classifier__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = {\"base_estimator__max_depth\" : [2, 5],\n",
    "              \"n_estimators\": [200, 400, 600]\n",
    "              }\n",
    "\n",
    "model_rf = RandomForestClassifier(max_features=10, min_samples_leaf=50, min_samples_split=50,\n",
    "                            random_state=100, n_estimators=100)\n",
    "\n",
    "#folds = 3\n",
    "#grid_search_rf = GridSearchCV(model_rf, \n",
    "#                               cv = folds,\n",
    "#                               param_grid=param_grid, \n",
    "#                               scoring = 'roc_auc', \n",
    "#                               return_train_score=True,                         \n",
    "#                               verbose = 1)\n",
    "\n",
    "#grid_search_rf.fit(X_train,  y_train)\n",
    "\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=model_rf.predict(X_test), \n",
    "    y_score=model_rf.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__AdaBoost__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"base_estimator__max_depth\" : [2, 5],\n",
    "              \"n_estimators\": [200, 400, 600]\n",
    "              }\n",
    "              \n",
    "    \n",
    "# base estimator\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# adaboost with the tree as base estimator\n",
    "model_adaboost = AdaBoostClassifier(base_estimator=tree, n_estimators=600, learning_rate=1.5,algorithm=\"SAMME\")\n",
    "\n",
    "#folds = 3\n",
    "#grid_search_adb = GridSearchCV(model_adaboost, \n",
    "#                               cv = folds,\n",
    "#                               param_grid=param_grid, \n",
    "#                               scoring = 'roc_auc', \n",
    "#                               return_train_score=True,                         \n",
    "#                               verbose = 1)\n",
    "\n",
    "#grid_search_adb.fit(X_train,  y_train)\n",
    "model_adaboost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=model_adaboost.predict(X_test), \n",
    "    y_score=model_adaboost.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "cv_results = pd.DataFrame(grid_search_adb.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting AUC with hyperparameter combinations\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "for n, depth in enumerate(param_grid['base_estimator__max_depth']):\n",
    "    \n",
    "\n",
    "    # subplot 1/n\n",
    "    plt.subplot(1,3, n+1)\n",
    "    depth_df = cv_results[cv_results['param_base_estimator__max_depth']==depth]\n",
    "\n",
    "    plt.plot(depth_df[\"param_n_estimators\"], depth_df[\"mean_test_score\"])\n",
    "    plt.plot(depth_df[\"param_n_estimators\"], depth_df[\"mean_train_score\"])\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(\"max_depth={0}\".format(depth))\n",
    "    plt.ylim([0.60, 1])\n",
    "    plt.legend(['test score', 'train score'], loc='upper left')\n",
    "    plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Gradient Boost__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter grid\n",
    "param_grid = {\"learning_rate\": [0.2, 0.6, 0.9],\n",
    "              \"subsample\": [0.3, 0.6, 0.9],\n",
    "              \"num_class\":[1,2,3,4,5,6,7,8]\n",
    "             }\n",
    "# adaboost with the tree as base estimator\n",
    "model_gbc = GradientBoostingClassifier(max_depth=2, n_estimators=200)\n",
    "\n",
    "# run grid search\n",
    "#folds = 3\n",
    "#grid_search_gbc = GridSearchCV(model_gbc, \n",
    "#                               cv = folds,\n",
    "#                               param_grid=param_grid, \n",
    "#                               scoring = 'roc_auc', \n",
    "#                               return_train_score=True,                         \n",
    "#                               verbose = 1)\n",
    "\n",
    "#grid_search_gbc.fit(X_train, y_train)\n",
    "model_gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=gbc.predict(X_test), \n",
    "    y_score=gbc.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__XGBoost__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning with XGBoost\n",
    "\n",
    "# creating a KFold object \n",
    "folds = 3\n",
    "\n",
    "# specify range of hyperparameters\n",
    "param_grid = {'learning_rate': [0.2, 0.6], \n",
    "             'subsample': [0.3, 0.6, 0.9]}          \n",
    "\n",
    "\n",
    "# specify model\n",
    "model_xgbc = XGBClassifier(max_depth=2, objective = 'multi:softmax', n_estimators=200, num_class=8)\n",
    "\n",
    "# set up GridSearchCV()\n",
    "#model_cv = GridSearchCV(estimator = model_xgbc, \n",
    "#                        param_grid = param_grid, \n",
    "#                        scoring= 'roc_auc', \n",
    "#                        cv = folds, \n",
    "#                        verbose = 1,\n",
    "#                        return_train_score=True) \n",
    "#model_cv.fit(X_train, y_train)\n",
    "model_xgbc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_report = classification_report(\n",
    "    digits=6,\n",
    "    y_true=y_test, \n",
    "    y_pred=model_xgbc.predict(X_test))\n",
    "print(sk_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=model_xgbc.predict(X_test), \n",
    "    y_score=model_xgbc.predict_proba(X_test))\n",
    "\n",
    "print(report_with_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=5><center>__##########Only for Clustering Problems##########__<center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__KNN__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us see 10 clusters and size of of each clsuter with kmean\n",
    "kmeans = KMeans(n_clusters=10, max_iter=50, random_state=100)\n",
    "kmeans.fit(df)\n",
    "\n",
    "cluster_labels_km = kmeans.labels_\n",
    "#Using K means we get following number of countries in each each cluster\n",
    "pd.Series(cluster_labels_km).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopkins Statistics:\n",
    "The Hopkins statistic, is a statistic which gives a value which indicates the cluster tendency, in other words: how well the data can be clustered.\n",
    "\n",
    "- If the value is between {0.01, ...,0.3}, the data is regularly spaced.\n",
    "- If the value is around 0.5, it is random.\n",
    "- If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopkins(X):\n",
    "    d = X.shape[1]\n",
    "\n",
    "    n = len(X) # rows\n",
    "    m = int(0.1 * n) \n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n",
    " \n",
    "    rand_X = sample(range(0, n, 1), m)\n",
    " \n",
    "    ujd = []\n",
    "    wjd = []\n",
    "    for j in range(0, m):\n",
    "        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),\n",
    "                                            np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n",
    "        ujd.append(u_dist[0][1])\n",
    "        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n",
    "        wjd.append(w_dist[0][1])\n",
    " \n",
    "    H = sum(ujd) / (sum(ujd) + sum(wjd))\n",
    "    if isnan(H):\n",
    "        print(ujd, wjd)\n",
    "        H = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the Hopkins Statistic function by passing the above dataframe as a paramter\n",
    "hopkins(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=red>__Hierarchical Tree__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import cut_tree\n",
    "\n",
    "# plot a dendogram complete linkage\n",
    "plt.figure(figsize=(15,5))\n",
    "mergings = linkage(df, method=\"complete\", metric='euclidean')\n",
    "dendrogram(mergings)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Silhouette Analysis\n",
    "$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$<br>\n",
    "$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n",
    "$q$ is the mean intra-cluster distance to all the points in its own cluster.<br>\n",
    "\n",
    "* The value of the silhouette score range lies between -1 to 1. \n",
    "* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n",
    "* A score closer to -1 indicates that the data point is not similar to the data points in its cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Analysis using silhouette score & Elbow curve\n",
    "sse_=[]\n",
    "ssd =[]\n",
    "for num_clusters in range(2,11):\n",
    "    \n",
    "    # intialise kmeans\n",
    "    kmeans1 = KMeans(n_clusters=num_clusters, max_iter=50, random_state=100).fit(df_pca_country_scaled)\n",
    "    \n",
    "    # silhouette score\n",
    "    silhouette_avg = silhouette_score(df_pca_country_scaled, kmeans1.labels_)\n",
    "    \n",
    "    sse_.append([num_clusters, silhouette_avg])\n",
    "    \n",
    "    #score for elbow curve\n",
    "    ssd.append(kmeans1.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette analysis\n",
    "# From this analysis it looks ideal number of cluster should be 5\n",
    "for j in sse_:\n",
    "    print(\"For n_clusters={0}, the silhouette score is {1}\".format(j[0], j[1]))\n",
    "    \n",
    "plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow Curve Method\n",
    "plt.plot(ssd)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we cut the dendogram at 5 that will gives has 10 clusters.\n",
    "\n",
    "cluster_labels_hc = cut_tree(mergings, n_clusters=10).reshape(-1, )\n",
    "cluster_labels_hc\n",
    "\n",
    "#Number of contries in each cluster\n",
    "pd.Series(cluster_labels_hc).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=5><center>__##########Model Evaluation Ends Here###########__<center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=red>__Final Model__</font>\n",
    "- Evaluate all the possible model\n",
    "- Take the best possible hyperparamter and their values\n",
    "- Build the final model here\n",
    "- If required combine the results of all models and unify models\n",
    "- Finally predict with final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=5><center>__##########Model Selection Ends Here##########__<center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=red>__Final List of Feature For Management__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_During Dummy Encoding & Hotencoding original value of the categorical field is converted into numbers<br>\n",
    "We need to convert these number into original values, so that management can understand them_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>__Final Prediction Using All the Models__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>__Finally Visualise the Relationship between SalesPrice & Features__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>__Final List of Features & Coefficients For Management__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
