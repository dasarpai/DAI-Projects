{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD persistence helps you to save the frequently-used RDDs or the output for future uses. By storing the intermediate RDDs or output in the cluster, you prevent the repeated task of evaluating the same results again and again. This helps in the efficient management of time and resources available.\n",
    "\n",
    "There are two ways to persist RDDs in Spark:\n",
    "- cache( )\n",
    "- persist( )\n",
    "\n",
    "The cache( ) method is used when you want to store all the data in the in-memory. This helps in speeding up your queries as the RDD is readily available in the memory for use.\n",
    "\n",
    "However, the in-memory storage is limited and different tasks running over the cluster may require separate storage or computing space within the in-memory. This issue is resolved by the persist( ) method. This method allows you to store the intermediate RDDs over both, disk and in-memory storage. There are four different storage levels available in the persist( ) method:\n",
    "\n",
    "- MEMORY_ONLY\n",
    "- MEMORY_AND_DISK\n",
    "- DISK_ONLY\n",
    "- OFF_HEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading SparkContext and SparkConf from PySpark\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "\n",
    "# loading PySpark for cache() and persist()\n",
    "import pyspark\n",
    "\n",
    "conf = SparkConf().setMaster(\"YARN\").setAppName(\"RDD Persistence\") \n",
    "sc = SparkContext().getOrCreate(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data in a RDD\n",
    "text=sc.textFile(\"/common_folder/pyspark_data/Bike-Sharing-Dataset/*.csv\")\n",
    "text.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caching the data in memory\n",
    "text.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpersisting the data from memory\n",
    "text.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persisting the data in disk\n",
    "text.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpersisting the data from memory\n",
    "text.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
