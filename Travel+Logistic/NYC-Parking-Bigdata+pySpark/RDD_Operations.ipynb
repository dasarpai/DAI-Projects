{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading SparkContext and SparkConf from the library pyspark\n",
    "# Setting the application name and cluster mode using SparkConf\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "conf = SparkConf().setMaster(\"YARN\").setAppName(\"RDD Operations\")\n",
    "sc = SparkContext().getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a text file into an RDD\n",
    "# transformation\n",
    "\n",
    "rdd = sc.textFile(\"/common_folder/pyspark_data/blogtext/blogtexts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect: loads the complete data in the driver program\n",
    "# action\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take: returns fixed numner of elements of he RDD\n",
    "# action\n",
    "\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function (same as python)\n",
    "\n",
    "def lowerSplit(lines):\n",
    "      lines = lines.lower()\n",
    "      lines = lines.split()\n",
    "      return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map:implementing the function on each element in the RDD\n",
    "\n",
    "rdd_1 = rdd.map(lowerSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action\n",
    "\n",
    "rdd_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap: implements a function on each element where the output of each element may not be a single element\n",
    "# better than map if you want the each element separately \n",
    "\n",
    "rdd_2 = rdd.flatMap(lowerSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing a list of frequent appearing words from the rdd\n",
    "# filter: filters the elements in the RDD based in the condition provided\n",
    "# filter is a transformation\n",
    "\n",
    "stopwords = ['is','am','are','the','for','a', \"-\", '=', '1',\"of\", \"it\",'–',\"to\",\"in\",\"and\",\"we\",\"can\",\"on\",\"you\",\"how\",\"/\"]\n",
    "rdd_3 = rdd_2.filter(lambda x: x not in stopwords)\n",
    "rdd_3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to pass a function inside the “groupBy” which will take the first 3 characters of each word in “rdd_3”.\n",
    "# The key is the first 3 characters and value is all the words which start with these 3 characters\n",
    "\n",
    "rdd_4 = rdd_3.groupBy(lambda w: w[0:3])\n",
    "print([(k, list(v)) for (k, v) in rdd_4.take(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating the count of similar words together\n",
    "\n",
    "rdd_3_mapped = rdd_3.map(lambda x: (x,1))\n",
    "rdd_3_grouped = rdd_3_mapped.groupByKey()\n",
    "\n",
    "# ('key', [value1, value2, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping all the occurences as 1\n",
    "\n",
    "print(list((j[0], list(j[1])) for j in rdd_3_grouped.take(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating occurences of each word and sorting it based on the count\n",
    "\n",
    "rdd_3_freq_of_words = rdd_3_grouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action\n",
    "\n",
    "rdd_3_freq_of_words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregating occurences of each word using reduceby (instead of groupby) and sorting it in descending order based on the count\n",
    "\n",
    "rdd_3_mapped.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the words in each partition separately\n",
    "\n",
    "def func(iterator):\n",
    "    count_spark = 0\n",
    "    count_apache = 0\n",
    "    for i in iterator:\n",
    "        if i =='spark':\n",
    "            count_spark = count_spark + 1\n",
    "        if i == 'apache':\n",
    "            count_apache = count_apache + 1\n",
    "    return (count_spark,count_apache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitions: mapping the defined function on each partiotion individually\n",
    "# glom(): returns an RDD created by coalescing all elements within each partition into a list\n",
    "\n",
    "rdd_3.mapPartitions(func).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating samples from the data\n",
    "\n",
    "rdd_3_sampled = rdd_3.sample(False, 0.4, 42)\n",
    "print(len(rdd_3.collect()),len(rdd_3_sampled.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking union of two datasets\n",
    "\n",
    "sample_1 = rdd_3.sample(False,0.2,42)\n",
    "sample_2 =rdd_3.sample(False,0.2,42)\n",
    "union_of_sample1_sample2 = sample_1.union(sample_2)\n",
    "print(len(sample_1.collect()), len(sample_2.collect()),len(union_of_sample1_sample2.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joiing two RDDs based on the key\n",
    "\n",
    "sample_1 = rdd_3_mapped.sample(False,.2,42)\n",
    "sample_2 = rdd_3_mapped.sample(False,.2,42)\n",
    "join_on_sample1_sample2 = sample_1.join(sample_2)\n",
    "join_on_sample1_sample2.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distict elements\n",
    "\n",
    "rdd_3_distinct = rdd_3.distinct()\n",
    "len(rdd_3_distinct.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the number of partitions in the rdd\n",
    "\n",
    "rdd_3.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the number of partitions in the rdd\n",
    "\n",
    "rdd_3_coalesce = rdd_3.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the number of partitions in the rdd using repartition\n",
    "# repartition can be used to increase the partitions as well\n",
    "\n",
    "rdd_3_coalesce=rdd_3.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the number of partitions in the rdd\n",
    "\n",
    "rdd_3_coalesce.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using range command to create rdd with parallelize \n",
    "\n",
    "num_rdd = sc.parallelize(range(1,1000))\n",
    "num_rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = range(1,1000)\n",
    "c = lambda x,y: x+y\n",
    "print (  c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of elements in RDD\n",
    "\n",
    "rdd_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions: mathematical operations performed over the RDD\n",
    "\n",
    "num_rdd.max(),num_rdd.min(), num_rdd.sum(),num_rdd.variance(),num_rdd.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark DataFrame and Sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/common_folder/airlines/data_2004-08.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- DayofMonth: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: string (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: string (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: string (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: string (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: string (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------------+\n",
      "|Dest|avg(CAST(DepDelay AS DOUBLE))|\n",
      "+----+-----------------------------+\n",
      "| BIL|                         80.2|\n",
      "| JAC|           37.833333333333336|\n",
      "| GRR|           31.457142857142856|\n",
      "| DAY|            28.16867469879518|\n",
      "| GUC|                         34.3|\n",
      "| CVG|           23.071428571428573|\n",
      "| DSM|            26.61842105263158|\n",
      "| GEG|           23.790697674418606|\n",
      "| TUS|            24.46969696969697|\n",
      "| PSP|            12.26923076923077|\n",
      "| ORD|           16.441696823482108|\n",
      "| BOI|           21.566265060240966|\n",
      "| GSO|                         26.0|\n",
      "| SLC|           21.431556948798328|\n",
      "| ATL|           20.791808873720136|\n",
      "| BTV|           22.102941176470587|\n",
      "| ICT|           17.614285714285714|\n",
      "| OGG|            25.82608695652174|\n",
      "| SMF|           17.330801104972377|\n",
      "| IAH|            16.72151898734177|\n",
      "+----+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Dest,avg(DepDelay)  FROM dfTable group by Dest order by avg(ArrDelay) desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='1', Year='2006', Month='1', DayofMonth='11', DayOfWeek='3', DepTime='743', CRSDepTime='745', ArrTime='1024', CRSArrTime='1018', UniqueCarrier='US', FlightNum='343', TailNum='N657AW', ActualElapsedTime='281', CRSElapsedTime='273', AirTime='223', ArrDelay='6', DepDelay='-2', Origin='ATL', Dest='PHX', Distance='1587', TaxiIn='45', TaxiOut='13', Cancelled='0', CancellationCode=None, Diverted='0', CarrierDelay='0', WeatherDelay='0', NASDelay='0', SecurityDelay='0', LateAircraftDelay='0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, _c0: string, Year: string, Month: string, DayofMonth: string, DayOfWeek: string, DepTime: string, CRSDepTime: string, ArrTime: string, CRSArrTime: string, UniqueCarrier: string, FlightNum: string, TailNum: string, ActualElapsedTime: string, CRSElapsedTime: string, AirTime: string, ArrDelay: string, DepDelay: string, Origin: string, Dest: string, Distance: string, TaxiIn: string, TaxiOut: string, Cancelled: string, CancellationCode: string, Diverted: string, CarrierDelay: string, WeatherDelay: string, NASDelay: string, SecurityDelay: string, LateAircraftDelay: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='1', Year='2006', Month='1', DayofMonth='11', DayOfWeek='3', DepTime='743', CRSDepTime='745', ArrTime='1024', CRSArrTime='1018', UniqueCarrier='US', FlightNum='343', TailNum='N657AW', ActualElapsedTime='281', CRSElapsedTime='273', AirTime='223', ArrDelay='6', DepDelay='-2', Origin='ATL', Dest='PHX', Distance='1587', TaxiIn='45', TaxiOut='13', Cancelled='0', CancellationCode=None, Diverted='0', CarrierDelay='0', WeatherDelay='0', NASDelay='0', SecurityDelay='0', LateAircraftDelay='0'),\n",
       " Row(_c0='2', Year='2006', Month='1', DayofMonth='11', DayOfWeek='3', DepTime='1053', CRSDepTime='1053', ArrTime='1313', CRSArrTime='1318', UniqueCarrier='US', FlightNum='613', TailNum='N834AW', ActualElapsedTime='260', CRSElapsedTime='265', AirTime='214', ArrDelay='-5', DepDelay='0', Origin='ATL', Dest='PHX', Distance='1587', TaxiIn='27', TaxiOut='19', Cancelled='0', CancellationCode=None, Diverted='0', CarrierDelay='0', WeatherDelay='0', NASDelay='0', SecurityDelay='0', LateAircraftDelay='0'),\n",
       " Row(_c0='3', Year='2006', Month='1', DayofMonth='11', DayOfWeek='3', DepTime='1915', CRSDepTime='1915', ArrTime='2110', CRSArrTime='2133', UniqueCarrier='US', FlightNum='617', TailNum='N605AW', ActualElapsedTime='235', CRSElapsedTime='258', AirTime='220', ArrDelay='-23', DepDelay='0', Origin='ATL', Dest='PHX', Distance='1587', TaxiIn='4', TaxiOut='11', Cancelled='0', CancellationCode=None, Diverted='0', CarrierDelay='0', WeatherDelay='0', NASDelay='0', SecurityDelay='0', LateAircraftDelay='0'),\n",
       " Row(_c0='4', Year='2006', Month='1', DayofMonth='11', DayOfWeek='3', DepTime='1753', CRSDepTime='1755', ArrTime='1925', CRSArrTime='1933', UniqueCarrier='US', FlightNum='300', TailNum='N312AW', ActualElapsedTime='152', CRSElapsedTime='158', AirTime='126', ArrDelay='-8', DepDelay='-2', Origin='AUS', Dest='PHX', Distance='872', TaxiIn='16', TaxiOut='10', Cancelled='0', CancellationCode=None, Diverted='0', CarrierDelay='0', WeatherDelay='0', NASDelay='0', SecurityDelay='0', LateAircraftDelay='0'),\n",
       " Row(_c0='5', Year='2006', Month='1', DayofMonth='11', DayOfWeek='3', DepTime='824', CRSDepTime='832', ArrTime='1015', CRSArrTime='1015', UniqueCarrier='US', FlightNum='765', TailNum='N309AW', ActualElapsedTime='171', CRSElapsedTime='163', AirTime='132', ArrDelay='0', DepDelay='-8', Origin='AUS', Dest='PHX', Distance='872', TaxiIn='27', TaxiOut='12', Cancelled='0', CancellationCode=None, Diverted='0', CarrierDelay='0', WeatherDelay='0', NASDelay='0', SecurityDelay='0', LateAircraftDelay='0')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
