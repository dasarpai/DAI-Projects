{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models'\n",
    "\n",
    "vectorfolder_bert = modelfolder + r'\\BERT'\n",
    "vectorfolder_ft   = modelfolder + r'\\fasttext_wiki.hi'\n",
    "vectorfolder_standford = modelfolder + r'\\POS-Tagger-Hindi'\n",
    "\n",
    "file           = datafolder_p + r'\\1-Hinglish_SarcasmCSV.csv'\n",
    "file_clean     = datafolder_p + r\"\\2-Hinglish_Sarcasm_Clean.csv\"\n",
    "file_FE        = datafolder_p + r\"\\3-features_pos.csv\"\n",
    "\n",
    "file_ft        = datafolder_p + r\"\\2-Hinglish_Sarcasm_Clean-fasttext.csv\"\n",
    "train_datafile = datafolder_p + r'\\2-train.csv'\n",
    "test_datafile  = datafolder_p + r'\\2-test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries required\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loadfiles in memory\n",
    "\n",
    "#Load Hindi stopwords\n",
    "df_stopwords = pd.read_csv(datafolder_e + \"\\Stopword_Hindi.csv\")\n",
    "stop_words= list(df_stopwords['words'])\n",
    "stop_words=[] #don't do anything with stopwords\n",
    "\n",
    "#Load 1-Hinglish_SarcasmCSV file\n",
    "df= pd.read_csv(file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove following characters from text '[тАФ\\-`\":тАЬтАЭ~]'\n",
    "def normalize_document(doc):\n",
    "    \n",
    "    #pattern = r'[.,;?ред:-тАФтАЭ`~тАЬ]@#ред'\n",
    "    #pattern = r'[!.,?#ред)()@:\"]'\n",
    "    pattern_sc = r'[,;тАШтАЩтАФ\\-`\":тАЬтАЭ~)(}{*/]' #remplace these special characters with single space\n",
    "    pattern_url = r'^(http|href|ftp|file)s?:\\/\\/.*[\\r\\n]*' #remove any url with null space\n",
    "    pattern_at = r'@*' #remove @\n",
    "    pattern_hash = '#[\\s]*' #remove space after hashtag\n",
    "    pattern_emo = '[\\u210d-\\U0001F9FF]' #emoticon identification\n",
    "    pattern_amp =  '[\\s]+&[\\s]+' #'[\\s][&][\\s]'\n",
    "    \n",
    "    # lower case and remove special characters\\whitespaces.\n",
    "    #doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A) #not required for Hindi script\n",
    "    \n",
    "    #replace any character which has pattern with blank\n",
    "    doc = re.sub( pattern_sc,' ',doc)\n",
    "    doc = re.sub( pattern_url,'',doc)\n",
    "    doc = re.sub( pattern_at,'',doc)\n",
    "\n",
    "    doc = re.sub( '|', \"\",doc)         #sometimes | is used as sentence marker\n",
    "    doc = re.sub( 'редред', \"\", doc)       #this is fullstop marker in Devanagari\n",
    "    doc = re.sub( 'ред', \"\", doc)        #this is fullstop marker in Devanagari\n",
    "    doc = re.sub ('[.]*', \"\", doc)       #sometimes . is fullstop marker by people\n",
    "    doc = re.sub ('[?]+', \"?\", doc)\n",
    "    doc = re.sub ('[!]+', \"!\", doc)\n",
    "    \n",
    "    \n",
    "    #doc = re.sub (' +',' ', doc) #any extra space - No required here. Doing it later.\n",
    "    doc = re.sub ('тАж','', doc)   #special character\n",
    "    doc = re.sub ('_','', doc)   #Arvind_Kejriwal/ рдзрдиреНрдп_рд╣реИ_рднрд╛рд░рдд_рднреВрдорд┐ _ should be removed \n",
    "\n",
    "    #Loop not working for this for some unknown reasons\n",
    "    doc = re.sub ('реж','0', doc)\n",
    "    doc = re.sub ('рез','1', doc)\n",
    "    doc = re.sub ('реи','2', doc)\n",
    "    doc = re.sub ('рей','3', doc)\n",
    "    doc = re.sub ('рек','4', doc)\n",
    "    doc = re.sub ('рел','5', doc)\n",
    "    doc = re.sub ('рем','6', doc)\n",
    "    doc = re.sub ('рен','7', doc)\n",
    "    doc = re.sub ('рео','8', doc)\n",
    "    doc = re.sub ('реп','9', doc)\n",
    "    \n",
    "    \n",
    "    doc = doc.lower() #convert text to lower case.\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    #sometmes emoji and normal text is without space. To create a space between normal word and emoji. \n",
    "    #Otherwise it will tokenization problem.\n",
    "    emo_pos=re.finditer(pattern_emo , doc)\n",
    "\n",
    "    i=0\n",
    "    j=0\n",
    "    doc1=[]\n",
    "    for pos in emo_pos:\n",
    "        j=pos.end()\n",
    "        if j<0:\n",
    "            j=0\n",
    "        doc1.append(doc[i:j-1]+\" \"+doc[j-1]+\" \" )\n",
    "\n",
    "        i=pos.end()\n",
    "\n",
    "    doc1.append( doc[j:]+\" \" )\n",
    "  \n",
    "    if len(doc1)>0: doc=\"\".join(doc1) #use emoticon process text only if text contained any emoticon else leave.\n",
    "\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    \n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    doc = re.sub( pattern_hash,'#',doc)\n",
    "    doc = re.sub( pattern_amp,'&',doc) #remove spaces around &. J & K, A & N, Modi & Shah\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset\n",
    "corpus = list(df.sentence)\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "\n",
    "df.sentence=norm_corpus\n",
    "\n",
    "df.sentence=df.sentence.str.replace('\\u200d','') #This is special character comes in Hindi words.\n",
    "\n",
    "df.sentence=df.sentence.str.replace(' ЁЯЗо ЁЯЗ│ ',' IN ') #In is indian flag while creating space \n",
    "#between emoticon and normal word it is getting split, we need to unite this back.\n",
    "\n",
    "df['sentence'] = df['sentence'].str.replace(' +',' ')  # remove any extra space\n",
    "    \n",
    "df['sentence_wo_emo'] = df['sentence'].str.replace(r'[\\u210d-\\U0001F9FF]','') # Remove emoticon. It does not help in POS tagging.\n",
    "\n",
    "df['sentence_wo_emo'] = df['sentence_wo_emo'].str.replace(' +',' ')  # remove any extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranlit2dev(doc, translit_hashtag=False):\n",
    "    from indicnlp.transliterate import acronym_transliterator\n",
    "    from indic_transliteration.sanscript import transliterate \n",
    "    from indic_transliteration import sanscript\n",
    "    import re\n",
    "\n",
    "    ack_transliterator=acronym_transliterator.LatinToIndicAcronymTransliterator()\n",
    "\n",
    "    words=re.split(r' ',doc) #break sentence on space\n",
    "    sent = []\n",
    "    for w in words:\n",
    "        \n",
    "        #if first letter is not # then Translit is True.\n",
    "        #if first Letter is # and tranlit_hastag=True then Translit is True\n",
    "        #else Translit is false.\n",
    "        Translit = (translit_hashtag and w[0]=='#') or (w[0]!='#')                                                      \n",
    "                                                        \n",
    "        \n",
    "        if Translit:\n",
    "            w1 = re.findall(r'[A-z]+',w) ##is  word Latin word?\n",
    "            if len(w1)>0: #Found english word\n",
    "                w2=w1[0]\n",
    "                pat = re.compile(r'([A-Z][a-z]+)') #English word with camel case. Break camel case into different words\n",
    "                w2= pat.split(w2)\n",
    "                w4=[]\n",
    "                for w3 in w2:\n",
    "                    if len(w3)>0:    \n",
    "                        if w3.upper()==w3: # all letters are capital in the word\n",
    "                            w4.append( ack_transliterator.transliterate(w3,lang='hi') )\n",
    "                        else:\n",
    "                            w3=w3.lower()\n",
    "                            w4.append( transliterate(w3, sanscript.ITRANS, sanscript.DEVANAGARI) )\n",
    "                    else:\n",
    "                        pass\n",
    "                    #print (w4)\n",
    "                w5=\"_\".join(w4)\n",
    "\n",
    "            else:\n",
    "                w5=w\n",
    "        else:\n",
    "            w5=w\n",
    "\n",
    "        sent.append(w5)\n",
    "    return \" \".join(sent)                                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['рдЬреЗрд╣рд╛рджреА рдпрд╛рд╕рд┐рди Malik рдФрд░ рдЖрдЬрд╛рдж Kashmir рдЪрд┐рд▓рд╛рдиреЗ рд╡рд╛рд▓реЗ рдХрд╢реНрдореАрд░ рдХреЛ рдмрдЪрд╛рдиреЗ рдЖрдЧреЗ рдирд╣реАрдВ рдЖрдПрдВрдЧреЗ #JammuKashmirFloods #KashmirFloods',\n",
    "       'рд╕рд░рд┐рдпрд╛ рдХрд╛рдиреВрди рдХреЗ рд╣рд┐рд╕рд╛рдм рд╕реЗ рдЕрдм WIFE IS MOTHER',\n",
    "       'рдЕрдБрдзреЗрд░реА рд░рд╛рддреЛрдВ рдореЗрдВ рд╕реБрдирд╕рд╛рди TL рдкрд░ рдПрдХ рдЕрд╢реНрд▓реАрд▓ рд▓реМрдВрдбрд╛ рдирд┐рдХрд▓рддрд╛ рд╣реИ рдЙрд╕реЗ рд▓реЛрдЧ Hawashmi рдХрд╣рддреЗ рд╣реИ',\n",
    "       'DMK рдиреЗ рд╕рд░рдХрд╛рд░ рдХреЛ рдЖрдзрд╛ рддрд▓рд╛рдХрд╝ рджрд┐рдпрд╛ SP&BSP рдиреЗ рд╕рд░рдХрд╛рд░ рд╕реЗ рдЖрдзрд╛ рд╡рд┐рд╡рд╛рд╣ рдХрд┐рдпрд╛ рдкрд░ рдЪрд╛рд╣рд╛ рдХрд░ рднреА Honeymoon рдирд╣реАрдВ рдордирд╛ рд╕рдХрддреЗ рдЕрдкрд╡рд┐рддреНрд░ рд░рд┐рд╢реНрддреЛрдВ рдореЗрдВ рдРрд╕рд╛ рд╣реА рд╣реЛрддрд╛ рд╣реИ .',\n",
    "       'рджрд┐рд▓реНрд▓реА рдореЗрдВ рд╢рд┐рд▓рд╛ рд╕рд░рдХрд╛рд░ рдиреЗ рдЬреЛ рдЬрдирддрд╛ рдкрд░ рдмрд░реНрдмрд░рддрд╛ рд╕реЗ рдЯрд┐рдпрд░ рдЧреИрд╕ рдФрд░ рдбрдВрдбреЗ рдмрд░рд╕рд╛рдП рдереЗ рдЙрд╕рдХреЗ рдкреАрдЫреЗ рдХрд╣реАрдВ рд╡рд╣ 14 рд▓рд╛рдЦ рдлрд░реНрдЬреА рд╡реЛрдЯ рддреЛ рдкреНрд░реЗрд░рдгрд╛ рдХреЗ рд╢реНрд░реЛрдд рдирд╣реАрдВ рдереЗ AAP',\n",
    "       'рдЕрдЧрд░ рдореБрд▓рд╛рдпрдо рдФрд░ рдЕрдЦрд┐рд▓реЗрд╢ рдЪрд╛рд╣реЗрдВ рддреЛ UP рдореЗрдВ рдПрдХ рджрдВрдЧрд╛ рдФрд░ рдХрд░рд╡рд╛ рдХреЗ рдмрд┐рд▓ рдорд╣реЗрдВрджреНрд░ рд╕рд┐рдВрд╣ рдзреЛрдиреА рдХреЗ рдирд╛рдо рдкреЗ рдлрд╛рдбрд╝ рд╕рдХрддреЗ рд╣реИрдВ. #IndvsEng',\n",
    "       'рджрд┐рд▓ рдмрд╣реБрдд рд╣реА рд╣рд░рд╛рдореА рдХрд╛рдорд┐рдиреА рдЪреАрдЬрд╝ рд╣реЛрддреА рд╣реИ рдЕрдЪреНрдЫреЗ рдЦрд╛рд╕реЗ рд╣рд╕реНрддреЗ рдЦреЗрд▓рддреЗ рдЖрджрдореА рдХреЛ рд╕реЗрдВрдЯрд┐рдпрд╛рдкрд╛ рдХрд╛ рдПрдбрдорд┐рди рдмрдирд╛ рджреЗрддреА рд╣реИ #SadStoriesOfTwitter',\n",
    "       'NIA рдХреА рдЧрд┐рд░рдлреНрдд рдореЗрдВ рддрд╛рдирд┐рдпрд╛ рдкрд░рд╡реАрди рд╣рд╛рдлрд┐рдЬ рд╕рдИрдж рдХреЛ рдХрд░рддреА рдереА рд░рд┐рдкреЛрд░реНрдЯ , рд╣рдерд┐рдпрд╛рд░реЛрдВ рдХреА рдЯреНрд░реЗрдирд┐рдВрдЧ рд▓реЗрдиреЗ рдЬрд╛рдирд╛ рдерд╛ рдкрд╛рдХрд┐рд╕реНрддрд╛рди',\n",
    "      'рдорд┐рдЧ рд╕реЗ F16 рдЙрдбрд╝рд╛рдКрдБ ! рдЪрд┐рдбрд╝рд┐рдпрд╛ рд╕реЗ рдореЗрдВ рдмрд╛рдЬ рдЧрд┐рд░рд╛рдКрдБ ! рдлрд┐рд░ рджреБрд╢реНрдорди рдШрд░ рдореЗрдВ рдЪрд╛рдп рдкреА рдЖрдКрдБ ! рдореИрдВ рдЕрднрд┐рдирдиреНрджрди рдирд╛рдо рдХрд╣рд╛рдКрдБ !',\n",
    "      'рдФрд░ рднреА рдмрд╣реБрдд рдкрддреНрд░рдХрд╛рд░реЛрдВ рдкрд░ F I R рд╣реБрдИ рдереА рд╡рд╣ рддреЛ рддреБрдордХреЛ рдирд╣реАрдВ рджрд┐рдЦреА рдЕрдХреНрд▓ рдХреЗ рдЕрдВрдзреЗ рдирд╛рдо рдирдпрдирд╕реБрдЦ']\n",
    "\n",
    "for doc in docs:\n",
    "    print( tranlit2dev(doc, translit_hashtag=True) )\n",
    "                                                                            \n",
    "\n",
    "print ('='*20)                                                                            \n",
    "for doc in docs:\n",
    "    print( tranlit2dev(doc, translit_hashtag=False) )     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> Seeing above results of transliteration I am deciding not to do any transliteration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index('ID')\n",
    "df['sentence_org']= df.sentence\n",
    "\n",
    "#remove # from the hashtag, else it will create word dic problem.\n",
    "df['sentence'] = df.sentence.str.replace(\"#\",\"\")\n",
    "df['sentence_wo_emo'] = df.sentence_wo_emo.str.replace(\"#\",\"\")\n",
    "\n",
    "\n",
    "df = df[['label','sentence','sentence_org','sentence_wo_emo']]\n",
    "\n",
    "#save file for transliteration\n",
    "df.to_csv(file_clean,sep=\"\\t\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\18-DS\\\\github\\\\SDSHL\\\\data\\\\processed\\\\2-Hinglish_Sarcasm_Clean.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def splitclusters(s):\n",
    "    \"\"\"Generate the grapheme clusters for the string s. (Not the full\n",
    "    Unicode text segmentation algorithm, but probably good enough for\n",
    "    Devanagari.)\n",
    "\n",
    "    \"\"\"\n",
    "    virama = u'\\N{DEVANAGARI SIGN VIRAMA}'\n",
    "    cluster = u''\n",
    "    last = None\n",
    "    for c in s:\n",
    "        cat = unicodedata.category(c)[0]\n",
    "        if cat == 'M' or cat == 'L' and last == virama:\n",
    "            cluster += c\n",
    "        else:\n",
    "            if cluster:\n",
    "                yield cluster\n",
    "            cluster = c\n",
    "        last = c\n",
    "    if cluster:\n",
    "        yield cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"рдореЗрд░рд╛ рдирд╛рдо рд╣рд░реА рдердкреНрд▓рд┐рдпрд╛рд▓ рд╣реИ\"\n",
    "a=\"рд╕рдВрдпреБрдХреНрдд рд░рд╛рдЬреНрдп рдЕрдореЗрд░рд┐рдХрд╛\"\n",
    "list(splitclusters(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub('[тАФ\\-`\":тАЬтАЭ~]',' ','h~a  tтАФha-pliyalтАЭri@pra`\"sтАЬa:d')\n",
    "# re.sub('#[\\s]*','$',\"s # KashmirFloods\")\n",
    "\n",
    "# #re.sub(r'^(http|href|ftp|file)s?:\\/\\/.*[\\r\\n]*', '', 'file://pmlogy.com')\n",
    "\n",
    "\n",
    "# #re.sub(, '', 'hari @prasad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern_extraspace= \"\\s+\"\n",
    "# doc='ЁЯТЩрдЦрд╝реВрдмрд╕реВрд░рддреА рд╣рдореЗрд╢рд╛ рдЖрдкрдХреЗ ЁЯТЩ тЭгя╕ПрдЪреЗрд╣рд░реЗ рдкрд░ рд╕рдЬрддреА рд░рд╣реЗ тЭгя╕П ЁЯТЩрдЦрд╝реБрд╢реА рд╣рдореЗрд╢рд╛ рдЖрдкрдХреА рдЬрд┐рдиреНрджрдЧреА ЁЯТЩ тЭгя╕П рдореЗрдВ рдорд╣рдХрддреА рд░рд╣реЗредтЭгя╕П ЁЯТЯ .'\n",
    "# doc='рдХреБрдЫ рдРрд╕реЗ рднреА рд╣рд╛рджрд╕реЗ рд╣реЛрддреЗ рд╣реИ рдЬрд┐рдВрджрдЧреА рдореЗрдВ рдЗрдВрд╕рд╛рди рдмрдЪ рддреЛ рдЬрд╛рддрд╛ рд╣реИ рдкрд░ рдЬрд╝рд┐рдВрджрд╛ рдирд╣реАрдВ рд░рд╣рддрд╛ рд╢реБрдн рд░рд╛рддреНрд░рд┐ рдорд┐рддреНрд░реЛрдВ'\n",
    "# #sometmes emoji and normal text is without space. To create a space between normal word and emoji. \n",
    "# #Otherwise it will tokenization problem.\n",
    "# pattern_emo = '[\\u210d-\\U0001F900]'\n",
    "# emo_pos=re.finditer(pattern_emo , doc)\n",
    "# i=0\n",
    "# doc1=[]\n",
    "# for pos in emo_pos:\n",
    "#     print (pos)\n",
    "#     j=pos.end()-2\n",
    "#     if j<0:\n",
    "#         j=0\n",
    "#     doc1.append(doc[i:j]+\" \" )\n",
    "#     doc1.append( doc[pos.end()-1]+\" \" )\n",
    "#     #print (j,txt1[i:j],txt1[b.end()-1])\n",
    "#     i=pos.end()\n",
    "\n",
    "# if len(doc1)>0: doc=\"\".join(doc1)\n",
    "# #doc = re.sub( pattern_extraspace,' ',doc) #remove extra space\n",
    "\n",
    "# # tokenize document\n",
    "# tokens = nltk.word_tokenize(doc)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern_emo = '[\\u210d-\\U0001F9FF]'\n",
    "#pattern_emo = '[\\u210d-\\U0001F1F3]'\n",
    "#pattern_emo = '[\\u1F1E6-\\U0001F43B]'\n",
    "#pattern_emo = '[\\u1F600-\\U0001F6FA]'\n",
    "#[\\u263a-\\U0001f645]\n",
    "\n",
    "\n",
    "\n",
    "#a=re.search('\\u270d',a)\n",
    "txt='рддреЗрд░реЗ рдЪреЗрд╣рд░реЗ рдкрд░ рдЬреЛ рдордХреНрдХрд╛рд░реА рд╣реИ рдЙрд╕реЗ рдЫрд┐рдкрд╛рдПрдЧрд╛ рдХреИрд╕реЗ рдореБрд▓реНрдХ рд╕реЗ рдЧрджреНрджрд╛рд░реА рддреЗрд░реЗ рдЦреВрди рдореЗрдВ рд╣реИ рдЙрд╕реЗ рдорд┐рдЯрд╛рдПрдЧрд╛ рдХреИрд╕реЗ тЬНрдХреНрдпрд╛ рдХрд░реЗрдЧрд╛ \\\n",
    "рдЬрд╛рдирдХрд░ рдЕрдЧрд░ рдореБрд▓реНрдХ рдХреЗ рд▓рд┐рдП рдХреБрдЫ ЁЯША рдХрд░рдирд╛ рд╣реА рдерд╛ рдЕрдЪреНрдЫрд╛ рддреЛ рдкрд╣рд▓реЗ рдЬрд╛ рдХрд░ рдЕрдмреНрджреБрд▓ рдХрд▓рд╛рдо рдЬреА рдЬреИрд╕рд╛ рджреЗрд╢ рднрдХреНрдд рдмрдирдХрд░ рджрд┐рдЦрддрд╛ рдЯреБрдЪреНрдЪреЗ рдЖрджрдореА'\n",
    "txt='рдЗрд╕ рдЙрдард╛рдИрдЧрд┐рд░реЗ рдЖрджрдореА рдХреА рдРрд╕реА рдУрдЫреА рд╣рд░рдХрдд рдХреЗ рд▓рд┐рдП рдЯреНрд░реЗрдВрдб рдирд╣реА рдПрдлрдЖрдИрдЖрд░ рд╣реЛрдиреА рдЪрд╛рд╣рд┐рдП рдФрд░ рджреВрд╕рд░реА рдмрд╛рдд рдХрд▓ рд╣реА рдореЗрдВ рдкреНрд░рд┐рдВрдЯрд┐рдВрдЧ рдкреНрд░реЗрд╕ рдкрд░ рдЬрд╛рдХрд░ рдЗрд╕рдХреЗ рдЯреНрд╡реАрдЯ ЁЯСЗрдХрд╛ рдкреНрд░рд┐рдВрдЯрдЖрдЙрдЯ рдирд┐рдХрд▓рдмрд╛ рдХрд░ рдкреВрд░реЗ рдЬрд┐рд▓реЗ рдХреЗ рдПрдХ рдПрдХ рдШрд░ рдореЗрдВ рдмрдЯрд╡рд╛рдЙрдЧрд╛ рддрд╛рдХрд┐ рд▓реЛрдЧреЛрдВ рдХреЛ рдкрддрд╛ рдЪрд▓реЗ рдХрд╛рдВрдЧреНрд░реЗрд╕ рдХреЗ рдиреЗрддрд╛ рдХреА рд╕реЛрдЪ рдХрд┐рддрдиреА рдЧрд┐рд░реА рд╣реБрдИ рдФрд░ рдШрдЯрд┐рдпрд╛ рд╣реИ #рдЕрд░реЗрд╕реНрдЯ_рдкрдВрдХрдЬ_рдкреВрдирд┐рдпрд╛'\n",
    "txt='рдореЗрд░реЗ рджреЗрд╢ рдХреЗ рдиреМрдЬрд╡рд╛рдиреЛрдВ рдЙрдареЛ рдФрд░ рдЬрд▓реНрджреА Fb рдЦреЛрд▓реЛ рд▓рд▓рдХрд┐рдпрд╛рдБ ЁЯШНрдСрдирд▓рд╛рдЗрди рд░рд╣реА рд╣реИред ЁЯШЙЁЯШЬ'\n",
    "txt='рдЖрдЬ рдЬреЛ рдЯреНрд╡реАрдЯ рдХрд░ рд░рд╣реА рд╣реВрдВ рд╡реЛ рдЯреНрд╡реАрдЯ рдЕрдм рддрдХ рдХрд╛ рдмреЗрд╕реНрдЯ рдЯреНрд╡реАрдЯ рд╣реЛрдиреЗ рд╡рд╛рд▓рд╛ рд╣реИред рдЕрдкрдиреЗ рдЕрдкрдиреЗ рдПрдХ рджреЗрд╢ рднрдХреНрдд рд╣реАрд░реЛ рдХрд╛ рдирд╛рдо ЁЯСЗрдХрдореЗрдВрдЯ рдмреЙрдХреНрд╕ рдореЗрдВ рд▓рд┐рдЦреЗ рдореЗрд░реЗ рддреЛ рд╣реИ рдЪрдВрджреНрд░рд╢реЗрдЦрд░ рдЖрдЬрд╛рджред рдФрд░ рдЖрдкрдХреЗ ?'\n",
    "txt='рд░рд╣рдиреЗ рджреЗ рдореБрдЭреЗ #рдЕрдБрдзреЗрд░реЗ рдореЗрдВ рдП рдЧрд╝рд╛рд▓рд┐рдм , ЁЯШТ рдЙрдЬрд╛рд▓реЛ рдореЗрдВ рдореБрдЭреЗ ЁЯдЧрдЕрдкрдиреЛрдВ рдХреЗ #рдЕрд╕рд▓реА рдЪреЗрд╣рд░реЗ рдирдЬрд╝рд░ ЁЯСАрдЖ рдЬрд╛рддреЗ рд╣реИред ! ЁЯШУ #GoodMorningTwitteред'\n",
    "#txt='рдлреНрд░реА рдХреА рд░реЛрдЯреА рддреЛ рдХреБрддреНрддреЗ рднреА рдЦрд╛рддреЗ рд╣реИрдВ , рднреВрдЦ рддреЛ рд░рд╛рд╖реНрдЯреНрд░рд╡рд╛рдж рдХреА рд╣реЛрдиреА рдЪрд╛рд╣рд┐рдП ред рдЬрдпрд╣рд┐рдВрдж ЁЯЗоЁЯЗ│ ЁЯЪйЁЯЩП ЁЯЩПрдЬрдп рд╕рдирд╛рддрди рдзрд░реНрдо ЁЯЩПЁЯП╗'\n",
    "#txt='ЁЯЩЛрдмреЗрдЯреА рдмреАрдорд╛рд░ рд╣реЛ рддреЛ рдмрдбрд╝рд╛ рджреБрдЦ рд╣реЛрддрд╛ рд╣реИ рдмрд╣реВ рдмреАрдорд╛рд░ рд╣реЛ рддреЛ рдбреНрд░рд╛рдорд╛ рд▓рдЧрддрд╛ рд╣реИ ЁЯШКЁЯШК'\n",
    "#txt='OMGЁЯТЩрдЦрд╝реВрдмрд╕реВрд░рддреА рд╣рдореЗрд╢рд╛ рдЖрдкрдХреЗ ЁЯТЩ тЭгя╕ПрдЪреЗрд╣рд░реЗ рдкрд░ рд╕рдЬрддреА рд░рд╣реЗ тЭгя╕П ЁЯТЩрдЦрд╝реБрд╢реА рд╣рдореЗрд╢рд╛ рдЖрдкрдХреА рдЬрд┐рдиреНрджрдЧреА ЁЯТЩ тЭгя╕П рдореЗрдВ рдорд╣рдХрддреА рд░рд╣реЗредтЭгя╕П ЁЯТЯ . рд▓рдЧрддрд╛ рд╣реИ'\n",
    "#txt='OMGЁЯШ│рдпреЗ рд╣реИ рдЗрд╕реНрд▓рд╛рдореА рд╕рд╛рдБрдк рдорджрд░рд╕реЛрдВ рдореЗ рдкрдврд╝реЗ рд╣реБрдП рд╕рдкреЛрд▓реЗ рдЕрдЧрд░ рдорджрд░рд╕реЗ рдмрдВрдж рдирд╣реА рд╣реБрдП рддреЛ рднрд╛рд░рдд рдореЗ рдЖрдк рдХреЗ рд╕рд╛рде рдпрд╣реА рд╣реЛрдЧрд╛'\n",
    "txt=\"рд╕рдм рдЧрдП рдирд╛ 2019 рдореЗрдВ ЁЯСЗ рдХреЛрдИ рдЫреВрдЯрд╛ рддреЛ рдирд╣реАрдВ 2018 рдореЗрдВ ЁЯШЭ ЁЯШЕ ЁЯШВ ЁЯШЖ #2018 #2019 #HappyNewYear2019 #HappyNewYear #рдирд╡рд╡рд░реНрд╖рдХреАрд╢реБрднрдХрд╛рдордирд╛рдПрдБ #рдирд╡рд╡рд░реНрд╖\"\n",
    "txt1=txt\n",
    "a=re.finditer(pattern_emo , txt)\n",
    "\n",
    "i=0\n",
    "txt2=[]\n",
    "for b in a:\n",
    "    j=b.end()\n",
    "    if j<0:\n",
    "        j=0\n",
    "    print (j, txt1[i:j-1]+\" \"+txt1[j-1]+ \" \")\n",
    "    txt2.append(txt1[i:j-1]+\" \"+txt1[j-1]+ \" \" )\n",
    "    #txt2.append( txt1[j:]+\" \" )\n",
    "\n",
    "    i=b.end()\n",
    "\n",
    "txt2.append( txt1[j:]+\" \" )\n",
    "txt2=\"\".join(txt2)\n",
    "txt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hari Hello. What happened'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "a='Hari????? He*llo. What? /happened'\n",
    "re.sub('[,;тАШтАЩтАФ?\\-`\":тАЬтАЭ~)(}{*/]','', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "txt = ' рез рд╕рд╛рдБрдк реи рдорджрд░рд╕реЛрдВ рей рдореЗ рек рдкрдврд╝реЗ релрел рд╣реБрдП ремрен рд╕рдкреЛрд▓реЗ рдЕрдЧрд░ рдорджрд░рд╕реЗ рдмрдВрдж'\n",
    "print( dev2engNumeric(txt) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'hello', re.UNICODE)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_list = re.compile(r'hello')\n",
    "the_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[a-z][A-Z]hello', re.UNICODE)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_list = re.compile(r'[a-z][A-Z]hello')\n",
    "the_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dmk рдиреЗ рд╕рд░рдХрд╛рд░ рдХреЛ рдЖрдзрд╛ рддрд▓рд╛рдХрд╝ рджрд┐рдпрд╛ sp&bsp рдиреЗ рд╕рд░рдХрд╛рд░ рд╕реЗ рдЖрдзрд╛ рд╡рд┐рд╡рд╛рд╣ рдХрд┐рдпрд╛ рдкрд░ рдЪрд╛рд╣рд╛ рдХрд░ рднреА рд╕реБрд╣рд╛рдЧ рд░рд╛рдд рдирд╣реАрдВ рдордирд╛ рд╕рдХрддреЗ рдЕрдкрд╡рд┐рддреНрд░ рд░рд┐рд╢реНрддреЛрдВ рдореЗрдВ рдРрд╕рд╛ рд╣реА рд╣реЛрддрд╛ рд╣реИ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='dmk рдиреЗ рд╕рд░рдХрд╛рд░ рдХреЛ рдЖрдзрд╛ рддрд▓рд╛рдХрд╝ рджрд┐рдпрд╛ sp & bsp рдиреЗ рд╕рд░рдХрд╛рд░ рд╕реЗ рдЖрдзрд╛ рд╡рд┐рд╡рд╛рд╣ рдХрд┐рдпрд╛ рдкрд░ рдЪрд╛рд╣рд╛ рдХрд░ рднреА рд╕реБрд╣рд╛рдЧ рд░рд╛рдд рдирд╣реАрдВ рдордирд╛ рд╕рдХрддреЗ рдЕрдкрд╡рд┐рддреНрд░ рд░рд┐рд╢реНрддреЛрдВ рдореЗрдВ рдРрд╕рд╛ рд╣реА рд╣реЛрддрд╛ рд╣реИ'\n",
    "re.sub('[\\s]+&[\\s]+','&',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
