{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://indicnlp.ai4bharat.org/indicft/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models'\n",
    "resultsfolder = r'D:\\18-DS\\github\\SDSHL\\data\\results'\n",
    "\n",
    "modelfolder_ft       = modelfolder + r'\\fasttext_wiki.hi'\n",
    "modelfolder_ft_ind   = modelfolder + r'\\fasttext_indicnlp.hi'\n",
    "modelfolder_ft_local   = modelfolder + r'\\fasttext_local'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttax pretrained Hindi binary/text format vectors \n",
    "#Common Crawl and Wikipedia \n",
    "cc300 = r'\\cc.hi.300.bin'\n",
    "cc100 = r'\\cc.hi.100.bin'\n",
    "\n",
    "#wiki\n",
    "wiki100    = r'\\wiki.hi.100.bin'\n",
    "wiki300    = r'\\wiki.hi.300.bin'\n",
    "wiki300_vec= r'\\wiki.hi.300.vec'\n",
    "\n",
    "#indicnlp\n",
    "indicnlp300     = r'\\indicnlp.ft.hi.300.bin'\n",
    "indicnlp300_vec = r'\\indicnlp.ft.hi.300.vec'\n",
    "\n",
    "# FastText Model donot need dataset but filename in a specific fasttext format for training\n",
    "train_file = datafolder_p+ r'\\3-train_ft.csv'\n",
    "test_file = datafolder_p+ r'\\3-test_ft.csv'\n",
    "\n",
    "# to predict the result with ID we need this file. Because fasttext format doesnot have ID\n",
    "#train_file1 = datafolder_p+ r'\\2-train.csv'\n",
    "test_file1 = datafolder_p+ r'\\2-test.csv'\n",
    "\n",
    "#This file is needed to create fasttext sentence embedding of full dataset \n",
    "filepath_fulldata = datafolder_p+ r'\\2-Hinglish_Sarcasm_Clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# file = datafolder_p + r'\\2-Hinglish_Sarcasm_Clean.csv'\n",
    "# df = pd.read_csv(file, sep='\\t', index_col=\"ID\")\n",
    "# idx = list(df.index)\n",
    "\n",
    "# file =  datafolder_p+ r'\\2-train.csv' \n",
    "# df = pd.read_csv(file, sep='\\t', index_col=\"ID\")\n",
    "# idx_train = df.index\n",
    "\n",
    "# file =  datafolder_p+ r'\\2-test.csv'\n",
    "# df = pd.read_csv(file, sep='\\t', index_col=\"ID\")\n",
    "# idx_test = df.index\n",
    "\n",
    "# df = pd.read_csv(file_data, sep='\\t', header=None)\n",
    "# df['ID']=idx\n",
    "# df=df.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install pytorch\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "     ---------------------------------------- 68.8/68.8 KB 1.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pybind11>=2.2\n",
      "  Using cached pybind11-2.9.1-py2.py3-none-any.whl (211 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in d:\\users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages (from fasttext) (58.0.4)\n",
      "Requirement already satisfied: numpy in d:\\users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages (from fasttext) (1.21.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py): started\n",
      "  Building wheel for fasttext (setup.py): finished with status 'done'\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-win_amd64.whl size=245583 sha256=27b9eb2f96d76054ccb5a24c606f476f2cfc7571cb1134cbb25a2cffa7b56445\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\4e\\ca\\bf\\b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in d:\\users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in d:\\users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in d:\\users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.3.15-cp37-cp37m-win_amd64.whl (273 kB)\n",
      "     -------------------------------------- 273.8/273.8 KB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in d:\\users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata in d:\\users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages (from click->nltk) (4.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib-metadata->click->nltk) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in d:\\users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.3.15\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "#!pip uninstall fasttext\n",
    "import fasttext\n",
    "from fasttext import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('gutenberg')\n",
    "#nltk.download('genesis')\n",
    "#nltk.download('inaugural')\n",
    "#nltk.download('brown')\n",
    "#nltk.download('webtext')\n",
    "#nltk.download('nps_chat')\n",
    "nltk.download('treebank')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "#C:\\Users\\admin\\AppData\\Roaming\\nltk_data.\n",
    "from nltk.book import nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(train_file, index_col=None, header=None, sep='\\t')\n",
    "corpus = list(df[1])\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in corpus]\n",
    "df['sentence_tkn'] = tokenized_corpus\n",
    "\n",
    "#This is required to create word embedding of full dataset\n",
    "df_full =pd.read_csv(filepath_fulldata, usecols=['ID','label','sentence'], sep=\"\\t\", index_col=\"ID\")\n",
    "corpus_full = list(df_full['sentence'])\n",
    "tokenized_corpus_full = [nltk.word_tokenize(doc) for doc in corpus_full]\n",
    "df_full['sentence_tkn'] = tokenized_corpus_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load 2-train data in memory. This file is required for lookup and creating wordvec for each sentence.\n",
    "# df =pd.read_csv(train_file1, usecols=['ID','label','sentence'], sep=\"\\t\", index_col=\"ID\")\n",
    "# corpus = list(df['sentence'])\n",
    "# tokenized_corpus = [nltk.word_tokenize(doc) for doc in corpus]\n",
    "# df['sentence_tkn'] = tokenized_corpus\n",
    "\n",
    "df_test=pd.read_csv(test_file1, usecols=['ID','label','sentence'], sep=\"\\t\", index_col=\"ID\")\n",
    "y_test =np.array(df_test['label'])\n",
    "X_test=df_test['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification using fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftmodel_lang = fasttext.load_model(vectorfolder+'\\lid.176.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n',len(ftmodel_lang.words) )\n",
    "# print('\\n',ftmodel_lang.get_dimension() )\n",
    "# #print(ftmodel_lang.get_labels())\n",
    "# print('\\n',ftmodel_lang.get_line(\"hari\"))\n",
    "# print('\\n',ftmodel_lang.get_analogies('apple','orange','juice',10) )\n",
    "\n",
    "# print ('\\n', ftmodel_lang.get_subwords('Hari') )\n",
    "# #print( ftmodel_lang.get_words())\n",
    "# print ('\\n',ftmodel_lang.get_sentence_vector('Hari Thapliyal'))\n",
    "# print ('\\n',ftmodel_lang.get_subword_id('apple') )\n",
    "# print ('\\n',ftmodel_lang.get_nearest_neighbors('apple') )\n",
    "# ftmodel_lang.predict(['Thapliyal', 'हर'])\n",
    "n=27\n",
    "list(zip(tokenized_corpus[n],ftmodel_lang.predict(tokenized_corpus[n])[0],ftmodel_lang.predict(tokenized_corpus[n])[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Word Vector Without Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "feature_size = 15    # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "\n",
    "#window_length = 200 # The amount of words we look at per example. Experiment with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##THIS STEP TAKES VERY LONG TIME\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "ft_model = FastText(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count = min_word_count,\n",
    "                          sg=sg, sample=sample, iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "ft_model.save(modelfolder_ft_local + r'\\model_fasttext_local.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.519313</td>\n",
       "      <td>-0.459539</td>\n",
       "      <td>-0.185066</td>\n",
       "      <td>-0.091893</td>\n",
       "      <td>-0.072511</td>\n",
       "      <td>0.104139</td>\n",
       "      <td>1.010676</td>\n",
       "      <td>0.215087</td>\n",
       "      <td>0.304260</td>\n",
       "      <td>0.229366</td>\n",
       "      <td>-0.538796</td>\n",
       "      <td>0.310136</td>\n",
       "      <td>0.081293</td>\n",
       "      <td>-0.582336</td>\n",
       "      <td>-0.925841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.574375</td>\n",
       "      <td>-0.630665</td>\n",
       "      <td>0.596369</td>\n",
       "      <td>0.060031</td>\n",
       "      <td>-0.129327</td>\n",
       "      <td>-0.342102</td>\n",
       "      <td>0.216957</td>\n",
       "      <td>0.074616</td>\n",
       "      <td>0.059842</td>\n",
       "      <td>0.525639</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.501358</td>\n",
       "      <td>0.095393</td>\n",
       "      <td>-0.271787</td>\n",
       "      <td>-0.727843</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.777627</td>\n",
       "      <td>-0.219662</td>\n",
       "      <td>0.097365</td>\n",
       "      <td>0.023982</td>\n",
       "      <td>0.381467</td>\n",
       "      <td>0.716699</td>\n",
       "      <td>0.789555</td>\n",
       "      <td>0.038210</td>\n",
       "      <td>0.388016</td>\n",
       "      <td>0.544673</td>\n",
       "      <td>1.197809</td>\n",
       "      <td>-0.248336</td>\n",
       "      <td>0.052653</td>\n",
       "      <td>-0.342404</td>\n",
       "      <td>-0.735357</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.011255</td>\n",
       "      <td>-1.342419</td>\n",
       "      <td>-0.876766</td>\n",
       "      <td>-0.867125</td>\n",
       "      <td>-0.507696</td>\n",
       "      <td>0.127758</td>\n",
       "      <td>0.722860</td>\n",
       "      <td>-0.580572</td>\n",
       "      <td>0.518736</td>\n",
       "      <td>1.038196</td>\n",
       "      <td>-0.055575</td>\n",
       "      <td>0.899874</td>\n",
       "      <td>0.274856</td>\n",
       "      <td>-1.229804</td>\n",
       "      <td>-1.066004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.778493</td>\n",
       "      <td>-0.494959</td>\n",
       "      <td>0.170979</td>\n",
       "      <td>0.832334</td>\n",
       "      <td>0.467761</td>\n",
       "      <td>0.578545</td>\n",
       "      <td>0.875711</td>\n",
       "      <td>1.520114</td>\n",
       "      <td>0.190859</td>\n",
       "      <td>1.135316</td>\n",
       "      <td>0.025323</td>\n",
       "      <td>0.009384</td>\n",
       "      <td>0.875736</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>-1.662741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>0.983233</td>\n",
       "      <td>0.231945</td>\n",
       "      <td>-0.255089</td>\n",
       "      <td>-0.622469</td>\n",
       "      <td>0.352465</td>\n",
       "      <td>-0.222344</td>\n",
       "      <td>0.626213</td>\n",
       "      <td>0.343675</td>\n",
       "      <td>0.212641</td>\n",
       "      <td>0.593361</td>\n",
       "      <td>-0.027918</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>0.177415</td>\n",
       "      <td>-0.729206</td>\n",
       "      <td>-0.510179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>1.446260</td>\n",
       "      <td>-0.843464</td>\n",
       "      <td>-0.095849</td>\n",
       "      <td>0.620362</td>\n",
       "      <td>0.266516</td>\n",
       "      <td>-0.066601</td>\n",
       "      <td>1.194153</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.219292</td>\n",
       "      <td>-0.043578</td>\n",
       "      <td>-0.200063</td>\n",
       "      <td>-0.948163</td>\n",
       "      <td>0.223242</td>\n",
       "      <td>-1.490455</td>\n",
       "      <td>-0.503475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9349</th>\n",
       "      <td>0.732688</td>\n",
       "      <td>-0.808272</td>\n",
       "      <td>-0.517679</td>\n",
       "      <td>1.349131</td>\n",
       "      <td>0.413501</td>\n",
       "      <td>0.012112</td>\n",
       "      <td>1.095162</td>\n",
       "      <td>0.215480</td>\n",
       "      <td>0.958502</td>\n",
       "      <td>1.374768</td>\n",
       "      <td>-0.172372</td>\n",
       "      <td>-0.452244</td>\n",
       "      <td>0.126437</td>\n",
       "      <td>-0.583281</td>\n",
       "      <td>-0.950802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9350</th>\n",
       "      <td>0.806241</td>\n",
       "      <td>-0.042852</td>\n",
       "      <td>0.084847</td>\n",
       "      <td>-0.397776</td>\n",
       "      <td>0.111712</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>0.567875</td>\n",
       "      <td>0.360480</td>\n",
       "      <td>0.361108</td>\n",
       "      <td>0.589563</td>\n",
       "      <td>0.288895</td>\n",
       "      <td>0.126402</td>\n",
       "      <td>0.265174</td>\n",
       "      <td>-0.898443</td>\n",
       "      <td>-0.513258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9351</th>\n",
       "      <td>1.552345</td>\n",
       "      <td>-0.812791</td>\n",
       "      <td>0.165640</td>\n",
       "      <td>-0.895818</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>-0.305811</td>\n",
       "      <td>0.135906</td>\n",
       "      <td>0.582905</td>\n",
       "      <td>-0.891245</td>\n",
       "      <td>0.576537</td>\n",
       "      <td>1.092508</td>\n",
       "      <td>1.073591</td>\n",
       "      <td>0.049818</td>\n",
       "      <td>-1.371531</td>\n",
       "      <td>-1.048157</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "ID                                                                           \n",
       "1     0.519313 -0.459539 -0.185066 -0.091893 -0.072511  0.104139  1.010676   \n",
       "5     0.574375 -0.630665  0.596369  0.060031 -0.129327 -0.342102  0.216957   \n",
       "12    0.777627 -0.219662  0.097365  0.023982  0.381467  0.716699  0.789555   \n",
       "13    0.011255 -1.342419 -0.876766 -0.867125 -0.507696  0.127758  0.722860   \n",
       "26    0.778493 -0.494959  0.170979  0.832334  0.467761  0.578545  0.875711   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9347  0.983233  0.231945 -0.255089 -0.622469  0.352465 -0.222344  0.626213   \n",
       "9348  1.446260 -0.843464 -0.095849  0.620362  0.266516 -0.066601  1.194153   \n",
       "9349  0.732688 -0.808272 -0.517679  1.349131  0.413501  0.012112  1.095162   \n",
       "9350  0.806241 -0.042852  0.084847 -0.397776  0.111712  0.005265  0.567875   \n",
       "9351  1.552345 -0.812791  0.165640 -0.895818  0.011486 -0.305811  0.135906   \n",
       "\n",
       "             7         8         9        10        11        12        13  \\\n",
       "ID                                                                           \n",
       "1     0.215087  0.304260  0.229366 -0.538796  0.310136  0.081293 -0.582336   \n",
       "5     0.074616  0.059842  0.525639 -0.002986 -0.501358  0.095393 -0.271787   \n",
       "12    0.038210  0.388016  0.544673  1.197809 -0.248336  0.052653 -0.342404   \n",
       "13   -0.580572  0.518736  1.038196 -0.055575  0.899874  0.274856 -1.229804   \n",
       "26    1.520114  0.190859  1.135316  0.025323  0.009384  0.875736  0.001797   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9347  0.343675  0.212641  0.593361 -0.027918 -0.010977  0.177415 -0.729206   \n",
       "9348  0.330961  0.219292 -0.043578 -0.200063 -0.948163  0.223242 -1.490455   \n",
       "9349  0.215480  0.958502  1.374768 -0.172372 -0.452244  0.126437 -0.583281   \n",
       "9350  0.360480  0.361108  0.589563  0.288895  0.126402  0.265174 -0.898443   \n",
       "9351  0.582905 -0.891245  0.576537  1.092508  1.073591  0.049818 -1.371531   \n",
       "\n",
       "            14  label  \n",
       "ID                     \n",
       "1    -0.925841      1  \n",
       "5    -0.727843      1  \n",
       "12   -0.735357      1  \n",
       "13   -1.066004      1  \n",
       "26   -1.662741      1  \n",
       "...        ...    ...  \n",
       "9347 -0.510179      1  \n",
       "9348 -0.503475      1  \n",
       "9349 -0.950802      1  \n",
       "9350 -0.513258      1  \n",
       "9351 -1.048157      1  \n",
       "\n",
       "[2000 rows x 16 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ft_tnseEmbedded, ft_words, ft_wvs, ft_labels = getTNSE_Embedded(ft_model)\n",
    "\n",
    "# get document level embeddings\n",
    "ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus_full, model=ft_model,\n",
    "                                             num_features=feature_size)\n",
    "\n",
    "ft_features = pd.DataFrame(ft_doc_features, index=df_full.index)\n",
    "ft_features['label']=df_full.label\n",
    "ft_features.to_csv(datafolder_p + r\"\\embedding_ft_local.csv\")\n",
    "ft_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pretrained Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compress large size pretrained binary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cc.hi.300.bin \n",
    "ft300 = fasttext.load_model(modelfolder_ft+ r'\\cc.hi.300.bin')\n",
    "ft300.get_dimension()\n",
    "\n",
    "#reduce dimension from 300 to 100\n",
    "import fasttext.util\n",
    "fasttext.util.reduce_model(ft300, 100)\n",
    "ft300.get_dimension()\n",
    "\n",
    "#save the model with reduced dimension\n",
    "ft300.save_model(vectorfolder+'\\cc.hi.100.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Load a pretrained model in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftmodel_cc300 = fasttext.load_model(modelfolder_ft+cc300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model file name:'+modelfolder_ft+cc300)\n",
    "#print (ftmodel_cc300.get_dimension(), len(ftmodel_cc300.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ftmodel_cc100 = fasttext.load_model(modelfolder_ft+cc100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('file name:'+modelfolder_ft+cc100)\n",
    "#print (ftmodel_cc100.get_dimension(), len(ftmodel_cc100.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ftmodel_indnlp300 = fasttext.load_model(modelfolder_ft_ind+indicnlp300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model file name:'+modelfolder_ft_ind+indicnlp300)\n",
    "#print (ftmodel_indnlp300.get_dimension(), len(ftmodel_indnlp300.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Read details of Pretrained Vector of Text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttest wiki indicnlp vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158016\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "dict_ftvec=load_vectors(modelfolder_ft+wiki300_vec)\n",
    "print ( len(dict_ftvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: How to create model from pretrained-vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>wiki300_vec<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\18-DS\\github\\SDSHL\\data\\processed\\3-train_ft.csv\n"
     ]
    }
   ],
   "source": [
    "#used pretrained vector to get the word embedding. This model can also be used for prediction\n",
    "#df should be in fastText format\n",
    "print (train_file)\n",
    "ftmodel_wiki300_vec = fasttext.train_supervised(train_file,dim=300, \\\n",
    "                                     lr=0.5, epoch=25, wordNgrams=2, bucket=200000,\\\n",
    "                                     pretrainedVectors=modelfolder_ft + wiki300_vec)\n",
    "#loss='ova'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160445"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ftmodel_wiki300_vec.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki300_vec has 158016 words - Method 3\n",
      "ftmodel_wiki300_vec - a finetuned model from wiki300_vec vector has  160659 word - Method 4\n",
      "ftmodel_vec300 model is created from existing vector\n"
     ]
    }
   ],
   "source": [
    "print ( 'wiki300_vec has 158016 words - Method 3')\n",
    "print ( 'ftmodel_wiki300_vec - a finetuned model from wiki300_vec vector has  160659 word - Method 4')\n",
    "print ( 'ftmodel_vec300 model is created from existing vector')\n",
    "#It means 160659-158016 are the new words in the fine tuned model as done above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>indicnlp300_vec<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used pretrained vector to get the word embedding. This model can also be used for prediction\n",
    "#df should be in fastText format\n",
    "\n",
    "ftmodel_indicnlp300_vec = fasttext.train_supervised(train_file,dim=300, \\\n",
    "                                     lr=0.5, epoch=25, wordNgrams=2, bucket=200000,\\\n",
    "                                     pretrainedVectors=modelfolder_ft_ind + indicnlp300_vec)\n",
    "#loss='ova'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328394"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ftmodel_indicnlp300_vec.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \\****Indicnlp based model has more words than wiki based model\\****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of wiki_pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ftmodel_wiki300_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 0.9994444444444445, 0.9994444444444445)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftmodel_wiki300_vec.test(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_for_test(model):\n",
    "    y_pred=[]\n",
    "    for i in range(len(X_test)):\n",
    "        p=model.predict(X_test.iloc[i])\n",
    "        x1 = int(p[0][-1][-1])\n",
    "        x2 = p[1][0]\n",
    "        if x1==0: x2= 1-x2\n",
    "        y_pred.append( x2 )\n",
    "    return y_pred    \n",
    "\n",
    "\n",
    "def print_matrics(y_pred):\n",
    "    threshold=0.5\n",
    "    y_pred1=[]\n",
    "    for i in y_pred :\n",
    "        if i>threshold:\n",
    "            y_pred1.append(1)\n",
    "        else:\n",
    "            y_pred1.append(0)\n",
    "\n",
    "    acc = np.round( metrics.accuracy_score(y_test,y_pred1), 2)\n",
    "    recall = np.round( metrics.recall_score(y_test,y_pred1), 2)\n",
    "    precision = np.round( metrics.precision_score(y_test,y_pred1), 2)\n",
    "    f1 = np.round( metrics.f1_score(y_test,y_pred1), 2)\n",
    "    roc = np.round( metrics.roc_auc_score(y_test,y_pred1), 2)\n",
    "\n",
    "    print(\"Accuracy : \", acc )\n",
    "    print(\"Recall   : \", recall )\n",
    "    print(\"Precision: \", precision )\n",
    "    print(\"F1       : \", f1 )\n",
    "    print(\"ROC      : \", roc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.72\n",
      "Recall   :  0.66\n",
      "Precision:  0.75\n",
      "F1       :  0.7\n",
      "ROC      :  0.72\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_for_test(ftmodel_wiki300_vec)\n",
    "prediction['fasttext_wiki_pretrained'] = y_pred\n",
    "print_matrics(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ftmodel_indicnlp300_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 0.9994444444444445, 0.9994444444444445)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftmodel_indicnlp300_vec.test(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 0.7, 0.7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftmodel_indicnlp300_vec.test(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7\n",
      "Recall   :  0.63\n",
      "Precision:  0.73\n",
      "F1       :  0.68\n",
      "ROC      :  0.7\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_for_test(ftmodel_indicnlp300_vec)\n",
    "prediction['fasttext_indicnlp_pretrained'] = y_pred\n",
    "print_matrics(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fasttext_wiki_pretrained</th>\n",
       "      <th>fasttext_indicnlp_pretrained</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>0.109270</td>\n",
       "      <td>0.043942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>0.960893</td>\n",
       "      <td>0.968121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>0.032174</td>\n",
       "      <td>0.017442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.008743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8819</th>\n",
       "      <td>0.540725</td>\n",
       "      <td>0.451919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>0.023583</td>\n",
       "      <td>0.025390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>0.126112</td>\n",
       "      <td>0.111959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.999044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fasttext_wiki_pretrained  fasttext_indicnlp_pretrained\n",
       "ID                                                          \n",
       "5212                  0.109270                      0.043942\n",
       "8028                  0.960893                      0.968121\n",
       "2364                  0.032174                      0.017442\n",
       "5805                  0.000468                      0.000220\n",
       "5236                  0.004674                      0.008743\n",
       "...                        ...                           ...\n",
       "7171                 -0.000009                     -0.000010\n",
       "8819                  0.540725                      0.451919\n",
       "2686                  0.023583                      0.025390\n",
       "8692                  0.126112                      0.111959\n",
       "8533                  0.998601                      0.999044\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction= pd.DataFrame(prediction, index=df_test.index)\n",
    "df_prediction.to_csv(resultsfolder+ r'\\model_predictions_fasttext.csv')\n",
    "df_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Sentences to Vector & save Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use ftmodel_cc100 to generate senetence vector. This vector later on can be \\\n",
    "#used in classical ML models for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_vec(n_features,df,model ):\n",
    "\n",
    "    sent_all_vec = np.zeros((len(df), n_features), dtype='float32')\n",
    "    i=0\n",
    "    for row in np.array(df):\n",
    "        sent_vec=np.zeros(n_features)\n",
    "        j=0\n",
    "        for word in row:\n",
    "            sent_vec=sent_vec+ model.get_word_vector(word).astype('float32') \n",
    "            j+=1\n",
    "        sent_vec = sent_vec/j\n",
    "        sent_all_vec[i]=sent_vec\n",
    "        i+=1\n",
    "    return sent_all_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ftmodel_wiki300_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentence to vector using pretrained fasttext\n",
    "df1 = pd.DataFrame( df_to_vec(300, df_full['sentence_tkn'], ftmodel_wiki300_vec), index=df_full.index)\n",
    "df1['label'] = df_full['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(datafolder_p+r'\\embedding_ft300_wiki_pretrained.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ftmodel_indicnlp300_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentence to vector using pretrained fasttext\n",
    "df1 = pd.DataFrame( df_to_vec(300, df_full['sentence_tkn'], ftmodel_indicnlp300_vec), index=df_full.index)\n",
    "df1['label'] = df_full['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(datafolder_p+r'\\embedding_ft300_indicnlp_pretrained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047358</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.135721</td>\n",
       "      <td>0.088405</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.165502</td>\n",
       "      <td>-0.155682</td>\n",
       "      <td>-0.095986</td>\n",
       "      <td>0.154054</td>\n",
       "      <td>0.057132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114018</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.076288</td>\n",
       "      <td>-0.083610</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>-0.208232</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>-0.129891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.091848</td>\n",
       "      <td>0.030187</td>\n",
       "      <td>0.076497</td>\n",
       "      <td>0.099425</td>\n",
       "      <td>0.062724</td>\n",
       "      <td>0.128292</td>\n",
       "      <td>-0.106851</td>\n",
       "      <td>-0.120260</td>\n",
       "      <td>0.114149</td>\n",
       "      <td>0.073124</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052836</td>\n",
       "      <td>0.074878</td>\n",
       "      <td>0.056161</td>\n",
       "      <td>-0.067253</td>\n",
       "      <td>-0.036870</td>\n",
       "      <td>-0.174462</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.095432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.054877</td>\n",
       "      <td>-0.122250</td>\n",
       "      <td>0.005790</td>\n",
       "      <td>0.014169</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>0.108095</td>\n",
       "      <td>-0.175122</td>\n",
       "      <td>-0.036322</td>\n",
       "      <td>0.083358</td>\n",
       "      <td>0.054436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224565</td>\n",
       "      <td>0.113609</td>\n",
       "      <td>-0.028683</td>\n",
       "      <td>-0.058093</td>\n",
       "      <td>-0.024254</td>\n",
       "      <td>-0.043670</td>\n",
       "      <td>0.076704</td>\n",
       "      <td>0.026717</td>\n",
       "      <td>-0.104058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.086249</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.024039</td>\n",
       "      <td>0.072895</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>0.211339</td>\n",
       "      <td>-0.112617</td>\n",
       "      <td>-0.085320</td>\n",
       "      <td>0.079474</td>\n",
       "      <td>0.093073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076949</td>\n",
       "      <td>0.111832</td>\n",
       "      <td>-0.035515</td>\n",
       "      <td>0.039484</td>\n",
       "      <td>0.060837</td>\n",
       "      <td>-0.161844</td>\n",
       "      <td>0.072351</td>\n",
       "      <td>-0.024754</td>\n",
       "      <td>-0.071523</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.039130</td>\n",
       "      <td>-0.115447</td>\n",
       "      <td>0.105212</td>\n",
       "      <td>0.044516</td>\n",
       "      <td>0.024228</td>\n",
       "      <td>0.133685</td>\n",
       "      <td>-0.103824</td>\n",
       "      <td>-0.073412</td>\n",
       "      <td>0.072047</td>\n",
       "      <td>0.034901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076331</td>\n",
       "      <td>0.027883</td>\n",
       "      <td>0.007058</td>\n",
       "      <td>-0.046341</td>\n",
       "      <td>0.046777</td>\n",
       "      <td>-0.281825</td>\n",
       "      <td>-0.022362</td>\n",
       "      <td>0.033455</td>\n",
       "      <td>-0.082226</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>0.002272</td>\n",
       "      <td>-0.086058</td>\n",
       "      <td>-0.038732</td>\n",
       "      <td>0.153019</td>\n",
       "      <td>-0.074544</td>\n",
       "      <td>0.136648</td>\n",
       "      <td>-0.167548</td>\n",
       "      <td>-0.089112</td>\n",
       "      <td>-0.003029</td>\n",
       "      <td>0.065542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094834</td>\n",
       "      <td>0.119982</td>\n",
       "      <td>0.036304</td>\n",
       "      <td>0.022565</td>\n",
       "      <td>-0.061524</td>\n",
       "      <td>-0.109919</td>\n",
       "      <td>0.048014</td>\n",
       "      <td>0.096063</td>\n",
       "      <td>-0.084558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>0.122960</td>\n",
       "      <td>-0.072445</td>\n",
       "      <td>0.117342</td>\n",
       "      <td>0.054222</td>\n",
       "      <td>-0.055762</td>\n",
       "      <td>0.079046</td>\n",
       "      <td>-0.172321</td>\n",
       "      <td>-0.165351</td>\n",
       "      <td>0.070567</td>\n",
       "      <td>0.102010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149850</td>\n",
       "      <td>0.052122</td>\n",
       "      <td>0.129005</td>\n",
       "      <td>-0.117767</td>\n",
       "      <td>-0.056222</td>\n",
       "      <td>-0.093479</td>\n",
       "      <td>-0.070339</td>\n",
       "      <td>0.053621</td>\n",
       "      <td>-0.130905</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9349</th>\n",
       "      <td>0.043652</td>\n",
       "      <td>-0.038357</td>\n",
       "      <td>0.042601</td>\n",
       "      <td>0.095946</td>\n",
       "      <td>-0.007229</td>\n",
       "      <td>0.077493</td>\n",
       "      <td>-0.083869</td>\n",
       "      <td>-0.072260</td>\n",
       "      <td>0.015455</td>\n",
       "      <td>0.051727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169764</td>\n",
       "      <td>0.075684</td>\n",
       "      <td>0.048517</td>\n",
       "      <td>-0.089417</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>-0.072376</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>0.018361</td>\n",
       "      <td>-0.046572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9350</th>\n",
       "      <td>-0.084335</td>\n",
       "      <td>-0.060846</td>\n",
       "      <td>0.017173</td>\n",
       "      <td>0.098216</td>\n",
       "      <td>-0.003965</td>\n",
       "      <td>0.121374</td>\n",
       "      <td>-0.274284</td>\n",
       "      <td>-0.039122</td>\n",
       "      <td>0.081010</td>\n",
       "      <td>0.032615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146666</td>\n",
       "      <td>0.095312</td>\n",
       "      <td>0.042335</td>\n",
       "      <td>0.042473</td>\n",
       "      <td>-0.057779</td>\n",
       "      <td>-0.100751</td>\n",
       "      <td>-0.053520</td>\n",
       "      <td>0.048866</td>\n",
       "      <td>-0.157083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9351</th>\n",
       "      <td>0.123998</td>\n",
       "      <td>-0.134069</td>\n",
       "      <td>-0.034343</td>\n",
       "      <td>0.025313</td>\n",
       "      <td>0.008232</td>\n",
       "      <td>0.075021</td>\n",
       "      <td>-0.077383</td>\n",
       "      <td>-0.095655</td>\n",
       "      <td>-0.002435</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101376</td>\n",
       "      <td>0.116997</td>\n",
       "      <td>-0.085256</td>\n",
       "      <td>0.026599</td>\n",
       "      <td>0.035413</td>\n",
       "      <td>-0.073230</td>\n",
       "      <td>-0.004765</td>\n",
       "      <td>-0.012492</td>\n",
       "      <td>-0.121084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "ID                                                                           \n",
       "1     0.047358 -0.065001  0.135721  0.088405  0.040134  0.165502 -0.155682   \n",
       "5     0.091848  0.030187  0.076497  0.099425  0.062724  0.128292 -0.106851   \n",
       "12    0.054877 -0.122250  0.005790  0.014169  0.007954  0.108095 -0.175122   \n",
       "13    0.086249  0.014710  0.024039  0.072895 -0.000357  0.211339 -0.112617   \n",
       "26    0.039130 -0.115447  0.105212  0.044516  0.024228  0.133685 -0.103824   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9347  0.002272 -0.086058 -0.038732  0.153019 -0.074544  0.136648 -0.167548   \n",
       "9348  0.122960 -0.072445  0.117342  0.054222 -0.055762  0.079046 -0.172321   \n",
       "9349  0.043652 -0.038357  0.042601  0.095946 -0.007229  0.077493 -0.083869   \n",
       "9350 -0.084335 -0.060846  0.017173  0.098216 -0.003965  0.121374 -0.274284   \n",
       "9351  0.123998 -0.134069 -0.034343  0.025313  0.008232  0.075021 -0.077383   \n",
       "\n",
       "             7         8         9  ...       291       292       293  \\\n",
       "ID                                  ...                                 \n",
       "1    -0.095986  0.154054  0.057132  ... -0.114018  0.002591  0.076288   \n",
       "5    -0.120260  0.114149  0.073124  ... -0.052836  0.074878  0.056161   \n",
       "12   -0.036322  0.083358  0.054436  ... -0.224565  0.113609 -0.028683   \n",
       "13   -0.085320  0.079474  0.093073  ... -0.076949  0.111832 -0.035515   \n",
       "26   -0.073412  0.072047  0.034901  ... -0.076331  0.027883  0.007058   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9347 -0.089112 -0.003029  0.065542  ... -0.094834  0.119982  0.036304   \n",
       "9348 -0.165351  0.070567  0.102010  ... -0.149850  0.052122  0.129005   \n",
       "9349 -0.072260  0.015455  0.051727  ... -0.169764  0.075684  0.048517   \n",
       "9350 -0.039122  0.081010  0.032615  ... -0.146666  0.095312  0.042335   \n",
       "9351 -0.095655 -0.002435  0.017213  ... -0.101376  0.116997 -0.085256   \n",
       "\n",
       "           294       295       296       297       298       299  label  \n",
       "ID                                                                       \n",
       "1    -0.083610  0.009277 -0.208232  0.033195  0.017050 -0.129891      1  \n",
       "5    -0.067253 -0.036870 -0.174462  0.000053 -0.006892 -0.095432      1  \n",
       "12   -0.058093 -0.024254 -0.043670  0.076704  0.026717 -0.104058      1  \n",
       "13    0.039484  0.060837 -0.161844  0.072351 -0.024754 -0.071523      1  \n",
       "26   -0.046341  0.046777 -0.281825 -0.022362  0.033455 -0.082226      1  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "9347  0.022565 -0.061524 -0.109919  0.048014  0.096063 -0.084558      1  \n",
       "9348 -0.117767 -0.056222 -0.093479 -0.070339  0.053621 -0.130905      1  \n",
       "9349 -0.089417  0.019201 -0.072376  0.007109  0.018361 -0.046572      1  \n",
       "9350  0.042473 -0.057779 -0.100751 -0.053520  0.048866 -0.157083      1  \n",
       "9351  0.026599  0.035413 -0.073230 -0.004765 -0.012492 -0.121084      1  \n",
       "\n",
       "[2000 rows x 301 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: How to cerate a new model using our corpus. This can be used as a pretrained model for other NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "# Skipgram model :\n",
    "ftmodel_usup = fasttext.train_unsupervised(train_datafile, model='skipgram', dim=300, minn=2, maxn=5,\\\n",
    "                                     lr=0.5, epoch=5, wordNgrams=2, bucket=200000)\n",
    "\n",
    "# or, cbow model :\n",
    "#model = fasttext.train_unsupervised('data.txt', model='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(model.words)) \n",
    "#print (model.words)\n",
    "ftmodel_usup.save_model(\"sdsms.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved & trained model.\n",
    "ftmodel_usup = fasttext.load_model(\"sdsms.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ftmodel_usup.get_dimension())\n",
    "print(ftmodel_usup.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus, model=ftmodel_usup,\n",
    "                                             num_features=feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftmodel_vec300.predict('हिंसा प्रति हिंसा को रोकना दुरुह कार्य है.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# vecfile_name= vectorfolder + '\\wiki.hi.vec'\n",
    "\n",
    "# fin = io.open(vecfile_name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "# data = {}\n",
    "# for line in fin:\n",
    "#     tokens = line.rstrip().split(' ')\n",
    "#     data[tokens[0]] = map(float, tokens[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in data[\"माता\"]:\n",
    "#     print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "#help(fasttext.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.get_line('I am hari')\n",
    "\n",
    "# test.get_sentence_vector('Hari Thapliyal')\n",
    "\n",
    "#test.get_input_vector(1000000)\n",
    "\n",
    "#test.get_subwords('Hari Thapliyal')\n",
    "\n",
    "test.get_label_id('orange')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n',len(test.words) )\n",
    "# print('\\n',test.get_dimension() )\n",
    "# #print(test.get_labels())\n",
    "# print('\\n',test.get_line(\"hari\"))\n",
    "# print('\\n',test.get_analogies('apple','orange','juice',10) )\n",
    "\n",
    "# print ('\\n', test.get_subwords('Hari') )\n",
    "# #print( test.get_words())\n",
    "# print ('\\n',test.get_sentence_vector('Hari Thapliyal'))\n",
    "# print ('\\n',test.get_subword_id('apple') )\n",
    "# print ('\\n',test.get_nearest_neighbors('apple') )\n",
    "# test.predict(['Thapliyal', 'हर'])\n",
    "n=1000\n",
    "list(zip(tokenized_corpus[n],test.predict(tokenized_corpus[n])[0],test.predict(tokenized_corpus[n])[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array(tokenized_corpus)\n",
    "a.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for c  in a:\n",
    "    b.append( \" \".join(c))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=set(\"\".join(b).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_corpus)\n",
    "tokenized_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.array(a)\n",
    "b.shape\n",
    "c= set( list(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext.FastText(tokenized_corpus)\n",
    "\n",
    "test= supervised(tokenized_corpus, size=feature_size, \n",
    "                              window=window_context, min_count = min_word_count,\n",
    "                              sg=sg, sample=sample, iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.sys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
