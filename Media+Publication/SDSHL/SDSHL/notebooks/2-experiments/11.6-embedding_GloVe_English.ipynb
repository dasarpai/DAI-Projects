{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> I have only GloVe vector for English Lanuage so cannot be used for Hinglish </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "\n",
    "vectorfolder_glove = r'D:\\18-DS\\data\\Datasets_nlp\\glove6b50dtxt\\glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(datafolder_p+ r'\\train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1852,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3692722371967655 0.36934503713706957\n"
     ]
    }
   ],
   "source": [
    "sentences = df['sentence']\n",
    "y = df['label']\n",
    "\n",
    "sentences_train, sentences_val, y_train,y_val = train_test_split(sentences, y, \n",
    "                                                                 stratify=y, test_size=0.2, random_state=1000)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=1)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_val = tokenizer.texts_to_sequences(sentences_val)\n",
    "# Adding 1 because of  reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1                          \n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
    "\n",
    "print( pd.Series(y_val).sum()/len(pd.Series(y_val)), pd.Series(y_train).sum()/len(pd.Series(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7355"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vectorfile, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(vectorfile, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "            else:\n",
    "                idx=0\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix( vectorfolder_glove,\n",
    "                                            tokenizer.word_index,  \n",
    "                                            embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.50370002, -0.19799   , -0.10062   , ...,  0.92774999,\n",
       "         0.41187   ,  0.85675001],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
