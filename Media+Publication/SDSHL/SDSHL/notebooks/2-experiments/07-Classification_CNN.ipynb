{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split   \n",
    "import sklearn.metrics as metrics\n",
    "from keras.preprocessing.text import Tokenizer                    \n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models'\n",
    "\n",
    "file_train = datafolder_p + r'\\2-train.csv'\n",
    "file_test  = datafolder_p + r'\\2-test.csv'\n",
    "file_data  = datafolder_p + r'\\2-Hinglish_Sarcasm_Clean.csv'\n",
    "file_FE  = datafolder_p + r'\\4-Hinglish_Sarcasm_Clean_FE.csv'\n",
    "\n",
    "prediction={}\n",
    "sent_size     = 100 #Max number of the words. If some sentence is more than this then that will be ignored.\n",
    "embedding_dim = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> CNN from Embedding Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding data cannot work on CNN models. Because CNN & RNN need tokens. \n",
    "#CNN generates weights and biases. Which are same as embedding, so embedding cannot be input\n",
    "#In CNN input we must specify sentence length and dictionary length and token for each word.\n",
    "#in Embedding we do don't have this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> CNN from Lexical FE File </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train  = pd.read_csv(file_train, sep='\\t', index_col=\"ID\")\n",
    "idx_train = df_train.index\n",
    "\n",
    "df_val   = pd.read_csv(file_test, sep='\\t', index_col=\"ID\")\n",
    "idx_val  = df_val.index\n",
    "\n",
    "df  = pd.read_csv(file_FE, sep='\\t', index_col=\"ID\")\n",
    "df_train = df.loc[idx_train]\n",
    "df_val   = df.loc[idx_val]\n",
    "\n",
    "X_train = df_train.drop('label', axis=1)\n",
    "y_train = df_train['label']\n",
    "\n",
    "X_val = df_val.drop('label', axis=1)\n",
    "y_val = df_val['label']\n",
    "\n",
    "# X_train = tokenizer.texts_to_sequences(df_train['sentence'])\n",
    "# X_train = pad_sequences(X_train, padding='post', maxlen=sent_size)\n",
    "# X_train = pd.DataFrame(X_train, index=idx_train)\n",
    "# y_train = df_train['label']\n",
    "\n",
    "# X_val = tokenizer.texts_to_sequences(df_val['sentence'])\n",
    "# X_val = pad_sequences(X_val, padding='post', maxlen=sent_size)\n",
    "# X_val = pd.DataFrame(X_val, index=idx_val)\n",
    "# y_val = df_val['label']\n",
    "\n",
    "vocab_size=20\n",
    "embedding_dim=20\n",
    "sent_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> CNN from Original File </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train  = pd.read_csv(file_train, sep='\\t', index_col=\"ID\")\n",
    "df_train = df_train[['sentence','label']]\n",
    "idx_train = df_train.index\n",
    "\n",
    "df_val   = pd.read_csv(file_test, sep='\\t', index_col=\"ID\")\n",
    "df_val   = df_val[['sentence','label']]\n",
    "idx_val  = df_val.index\n",
    "\n",
    "df  = pd.read_csv(file_data, sep='\\t', index_col=\"ID\")\n",
    "full_text = df['sentence']\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000) #pickup only 5000 top words\n",
    "tokenizer.fit_on_texts(full_text)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(df_train['sentence'])\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=sent_size)\n",
    "X_train = pd.DataFrame(X_train, index=idx_train)\n",
    "y_train = df_train['label']\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(df_val['sentence'])\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=sent_size)\n",
    "X_val = pd.DataFrame(X_val, index=idx_val)\n",
    "y_val = df_val['label']\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=5000) #pickup only 5000 top words\n",
    "# tokenizer.fit_on_texts(df['sentence'])\n",
    "\n",
    "# df_train = tokenizer.texts_to_sequences(df_train['sentence'])\n",
    "# df_train = pad_sequences(df_train, padding='post', maxlen=sent_size)\n",
    "# df_train = pd.DataFrame(df_train, index=idx_train)\n",
    "\n",
    "# df_val = tokenizer.texts_to_sequences(df_val['sentence'])\n",
    "# df_val = pad_sequences(df_val, padding='post', maxlen=sent_size)\n",
    "# df_val = pd.DataFrame(df_val, index=idx_val)\n",
    "\n",
    "vocab_size=len(tokenizer.word_index) + 1 #+1 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1800, 20), (1800,), (200, 20), (200,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token to Sequence (embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit( X_train, y_train,\n",
    "#                     epochs=10,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(df, batch_size):\n",
    "    print( 'batch size =', batch_size)\n",
    "    \n",
    "    while True:  \n",
    "        df_size = len(df)\n",
    "        \n",
    "        num_batches = df_size//batch_size # calculate the number of batches\n",
    "        remaining_records= df_size %batch_size\n",
    "        i=0\n",
    "        for j in range(num_batches):\n",
    "            tempdf = df.iloc[i*batch_size: (i+1)*batch_size,:]\n",
    "            print (j,'\\n',tempdf.shape)\n",
    "            yield tempdf.iloc[:,:100], tempdf['label']\n",
    " \n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if remaining_records!=0:\n",
    "            tempdf = df.iloc[(i+1)*batch_size:,:]\n",
    "            print ('here',j,'\\n',tempdf.shape)\n",
    "            yield tempdf.iloc[:,:100], tempdf['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture : CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 9115, 20)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim, vocab_size, sent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 20, 20)            182300    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16, 128)           12928     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 196,529\n",
      "Trainable params: 196,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=sent_size))\n",
    "cnnmodel.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(layers.GlobalMaxPooling1D())\n",
    "cnnmodel.add(layers.Dense(10, activation='relu'))\n",
    "cnnmodel.add(layers.Dense(1, activation='sigmoid'))\n",
    "cnnmodel.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Print summary of model\n",
    "print(cnnmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.6922 - accuracy: 0.5400 - val_loss: 0.6898 - val_accuracy: 0.5600\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.6798 - accuracy: 0.6650 - val_loss: 0.6821 - val_accuracy: 0.5950\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.6582 - accuracy: 0.7322 - val_loss: 0.6700 - val_accuracy: 0.6100\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6163 - accuracy: 0.7578 - val_loss: 0.6463 - val_accuracy: 0.6150\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.5327 - accuracy: 0.8439 - val_loss: 0.6123 - val_accuracy: 0.6450\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3959 - accuracy: 0.9106 - val_loss: 0.5867 - val_accuracy: 0.6600\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.2535 - accuracy: 0.9517 - val_loss: 0.6276 - val_accuracy: 0.6750\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1465 - accuracy: 0.9761 - val_loss: 0.6865 - val_accuracy: 0.6950\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0828 - accuracy: 0.9911 - val_loss: 0.7605 - val_accuracy: 0.6900\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.0497 - accuracy: 0.9944 - val_loss: 0.8589 - val_accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "# history = model.fit_generator( train_generator,\n",
    "#                               steps_per_epoch=training_steps_per_epoch,\n",
    "#                               epochs=10,\n",
    "#                               validation_data=val_generator )\n",
    "\n",
    "\n",
    "history = cnnmodel.fit( X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture : RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 20, 100)           911500    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 20, 128)           117248    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,037,069\n",
      "Trainable params: 1,037,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#deep learning library\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "rnnmodel=Sequential()\n",
    "\n",
    "#embedding layer\n",
    "rnnmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=sent_size))\n",
    "\n",
    "#lstm layer\n",
    "rnnmodel.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
    "\n",
    "#Global Maxpooling\n",
    "rnnmodel.add(GlobalMaxPooling1D())\n",
    "\n",
    "#Dense Layer\n",
    "rnnmodel.add(Dense(64,activation='relu')) \n",
    "rnnmodel.add(Dense(1,activation='sigmoid')) \n",
    "\n",
    "#Add loss function, metrics, optimizer\n",
    "rnnmodel.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n",
    "\n",
    "#Adding callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
    "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
    "\n",
    "#Print summary of model\n",
    "print(rnnmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get train & test data generator. Only for large dataset size, which can not be loaded in memeory.\n",
    "# batch_size = 200\n",
    "# training_steps_per_epoch = round(len(X_train) / batch_size)\n",
    "\n",
    "# train_generator = generator(df_train, batch_size)\n",
    "# val_generator   = generator(df_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 2s 103ms/step - loss: 0.6838 - acc: 0.5811 - val_loss: 0.6697 - val_acc: 0.5450\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 1s 54ms/step - loss: 0.5743 - acc: 0.7033 - val_loss: 0.6500 - val_acc: 0.6400\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.3511 - acc: 0.8544 - val_loss: 0.7064 - val_acc: 0.6450\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.1884 - acc: 0.9344 - val_loss: 0.7979 - val_acc: 0.6400\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 1s 68ms/step - loss: 0.1100 - acc: 0.9694 - val_loss: 1.2121 - val_acc: 0.6000\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 1s 64ms/step - loss: 0.0563 - acc: 0.9861 - val_loss: 1.8174 - val_acc: 0.6200\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 1s 64ms/step - loss: 0.0319 - acc: 0.9911 - val_loss: 1.8116 - val_acc: 0.6150\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 1s 60ms/step - loss: 0.0195 - acc: 0.9956 - val_loss: 1.9871 - val_acc: 0.6100\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.0130 - acc: 0.9961 - val_loss: 2.0975 - val_acc: 0.6250\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 1s 62ms/step - loss: 0.0137 - acc: 0.9978 - val_loss: 2.2931 - val_acc: 0.6200\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "# history = model.fit_generator( train_generator,\n",
    "#                               steps_per_epoch=training_steps_per_epoch,\n",
    "#                               epochs=10,\n",
    "#                               validation_data=val_generator )\n",
    "\n",
    "history = rnnmodel.fit( X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of model on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred):\n",
    "    threshold=0.5\n",
    "    roc = np.round( metrics.roc_auc_score(y_val,y_pred), 2)\n",
    "    \n",
    "    y_pred1=[]\n",
    "    for i in y_pred :  \n",
    "\n",
    "        if i>threshold:\n",
    "            y_pred1.append(1)\n",
    "        else:\n",
    "            y_pred1.append(0)\n",
    "\n",
    "    acc = np.round( metrics.accuracy_score(y_val,y_pred1), 2)\n",
    "    recall = np.round( metrics.recall_score(y_val,y_pred1), 2)\n",
    "    precision = np.round( metrics.precision_score(y_val,y_pred1), 2)\n",
    "    f1 = np.round( metrics.f1_score(y_val,y_pred1), 2)\n",
    "\n",
    "    print(\"Accuracy : \", acc )\n",
    "    print(\"Recall   : \", recall )\n",
    "    print(\"Precision: \", precision )\n",
    "    print(\"F1       : \", f1 )\n",
    "    print(\"ROC      : \", roc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction={}\n",
    "prediction[\"CNN\"] = list(np.reshape( cnnmodel.predict(X_val),-1))\n",
    "prediction[\"RNN\"] = list(np.reshape( rnnmodel.predict(X_val),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.68\n",
      "Recall   :  0.62\n",
      "Precision:  0.7\n",
      "F1       :  0.66\n",
      "ROC      :  0.73\n"
     ]
    }
   ],
   "source": [
    "print_metrics(prediction['CNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.62\n",
      "Recall   :  0.63\n",
      "Precision:  0.62\n",
      "F1       :  0.62\n",
      "ROC      :  0.65\n"
     ]
    }
   ],
   "source": [
    "print_metrics(prediction['RNN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save CNN, RNN Predictions Results to compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = pd.DataFrame(prediction, columns=[\"CNN\",\"RNN\"], index=idx_val)\n",
    "\n",
    "df_prediction.to_csv(datafolder_p + r'\\model_predictions_NN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNN</th>\n",
       "      <th>RNN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>0.569488</td>\n",
       "      <td>0.865231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>0.059986</td>\n",
       "      <td>0.749970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>0.019504</td>\n",
       "      <td>0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>0.008435</td>\n",
       "      <td>0.396357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>0.011073</td>\n",
       "      <td>0.955523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8819</th>\n",
       "      <td>0.265164</td>\n",
       "      <td>0.999915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>0.129757</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>0.607230</td>\n",
       "      <td>0.015694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>0.717929</td>\n",
       "      <td>0.999913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           CNN       RNN\n",
       "ID                      \n",
       "5212  0.569488  0.865231\n",
       "8028  0.059986  0.749970\n",
       "2364  0.019504  0.000362\n",
       "5805  0.008435  0.396357\n",
       "5236  0.011073  0.955523\n",
       "...        ...       ...\n",
       "7171  0.000628  0.000176\n",
       "8819  0.265164  0.999915\n",
       "2686  0.129757  0.000343\n",
       "8692  0.607230  0.015694\n",
       "8533  0.717929  0.999913\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.68\n",
      "Recall   :  0.62\n",
      "Precision:  0.7\n",
      "F1       :  0.66\n",
      "ROC      :  0.73\n"
     ]
    }
   ],
   "source": [
    "print_metrics(prediction['CNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
