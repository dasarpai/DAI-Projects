{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  7 12:29:12 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   44C    P0    60W / 149W |      0MiB / 11441MiB |    100%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#htop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C99cln9hZNbj"
   },
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed' #Change this when working on Nimblebox\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models\\BERT'\n",
    "resultsfolder = r'D:\\18-DS\\github\\SDSHL\\data\\results'\n",
    "\n",
    "# vectorfolder_bert = modelfolder + r'\\BERT'\n",
    "# vectorfolder_ft   = modelfolder + r'\\fasttext_wiki.hi'\n",
    "# vectorfolder_standford = modelfolder + r'\\POS-Tagger-Hindi'\n",
    "\n",
    "filepath_train =datafolder_p+ r'\\2-train.csv' #Change this when working on Nimblebox\n",
    "filepath_test  =datafolder_p+ r'\\2-test.csv'  #Change this when working on Nimblebox\n",
    "filepath_fulldata = datafolder_p+ r'\\2-Hinglish_Sarcasm_Clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_config' from 'tensorflow.python.eager.context' (d:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\context.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15016/2727426868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_config' from 'tensorflow.python.eager.context' (d:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\eager\\context.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import random as rn\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#import sklearn.svm as svm\n",
    "from sklearn.linear_model import LogisticRegression             \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "#from torchnlp.datasets import imdb_dataset\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4sH2gw6XZNbs"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(filepath_train, sep=\"\\t\")\n",
    "df_test  = pd.read_csv(filepath_test, sep=\"\\t\")\n",
    "df_full  = pd.read_csv(filepath_fulldata, sep=\"\\t\", index_col=\"ID\")\n",
    "\n",
    "X_train =  df_train['sentence'] \n",
    "y_train = df_train['label']\n",
    "\n",
    "X_test =  df_test['sentence'] \n",
    "y_test = df_test['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fWjHk9FLZNbz"
   },
   "outputs": [],
   "source": [
    "prediction={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tensor Dataset of Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1800, 200, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = list(X_train)\n",
    "y_train = list(y_train)\n",
    "\n",
    "X_test = list(X_test)\n",
    "y_test = list(y_test)\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuZYyHX2ZNcF"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJZlF8PzZNcJ"
   },
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NEpz2L4uZNcK"
   },
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import torch\n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            bert_tokenizer,\n",
    "            bert_model,\n",
    "            max_length: int = 60,\n",
    "            embedding_func: Optional[Callable[[torch.tensor], torch.tensor]] = None,\n",
    "    ):\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.model = bert_model\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_func = embedding_func\n",
    "\n",
    "        if self.embedding_func is None:\n",
    "            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "    def _tokenize(self, text: str) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        # Tokenize the text with the provided tokenizer\n",
    "        tokenized_text = self.tokenizer.encode_plus(text,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    max_length=self.max_length,\n",
    "                                                    truncation=True\n",
    "                                                    )[\"input_ids\"]\n",
    "\n",
    "        # Create an attention mask telling BERT to use all words\n",
    "        attention_mask = [1] * len(tokenized_text)\n",
    "\n",
    "        # bert takes in a batch so we need to unsqueeze the rows\n",
    "        return (\n",
    "            torch.tensor(tokenized_text).unsqueeze(0),\n",
    "            torch.tensor(attention_mask).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def _tokenize_and_predict(self, text: str) -> torch.tensor:\n",
    "        tokenized, attention_mask = self._tokenize(text)\n",
    "\n",
    "        embeddings = self.model(tokenized, attention_mask)\n",
    "        return self.embedding_func(embeddings)\n",
    "\n",
    "    def transform(self, text: List[str]):\n",
    "        if isinstance(text, pd.Series):\n",
    "            text = text.tolist()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return torch.stack([self._tokenize_and_predict(string) for string in text])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wuSA0DScZNcQ"
   },
   "outputs": [],
   "source": [
    "def pred_for_test(model):\n",
    "    y_pred=[]\n",
    "    for i in range(len(X_test)):\n",
    "        p=model.predict(X_test.iloc[i])\n",
    "        x1 = int(p[0][-1][-1])\n",
    "        x2 = p[1][0]\n",
    "        if x1==0: x2= 1-x2\n",
    "        y_pred.append( x2 )\n",
    "    return y_pred    \n",
    "\n",
    "\n",
    "def print_metrics(y_pred):\n",
    "    threshold=0.5\n",
    "    y_pred1=[]\n",
    "    for i in y_pred :\n",
    "        if i>threshold:\n",
    "            y_pred1.append(1)\n",
    "        else:\n",
    "            y_pred1.append(0)\n",
    "\n",
    "    acc = np.round( metrics.accuracy_score(y_test,y_pred1), 2)\n",
    "    recall = np.round( metrics.recall_score(y_test,y_pred1), 2)\n",
    "    precision = np.round( metrics.precision_score(y_test,y_pred1), 2)\n",
    "    f1 = np.round( metrics.f1_score(y_test,y_pred1), 2)\n",
    "    roc = np.round( metrics.roc_auc_score(y_test,y_pred1), 2)\n",
    "\n",
    "    print(\"Accuracy : \", acc )\n",
    "    print(\"Recall   : \", recall )\n",
    "    print(\"Precision: \", precision )\n",
    "    print(\"F1       : \", f1 )\n",
    "    print(\"ROC      : \", roc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> BERT Multilanguage - Task Transfer - Pytorch</font>\n",
    "<font color=red> pytoch works only on Nimblebox. Because it need GPU and my machine does not have that. So DON'T RUN it Here</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspiration from\n",
    "#https://towardsdatascience.com/bert-to-the-rescue-17671379687f\n",
    "#https://github.com/shudima/notebooks/blob/master/BERT_to_the_rescue.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_tokens():\n",
    "    #This will download Pretrained Multilingual Model and create a Model in Memory\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "    train_tokens = list(map(lambda t: ['[CLS]'] + bert_tokenizer.tokenize(t)[:255] + ['[SEP]'], X_train))\n",
    "    test_tokens = list(map(lambda t: ['[CLS]'] + bert_tokenizer.tokenize(t)[:255] + ['[SEP]'], X_test))\n",
    "\n",
    "    print (\"Tokens:\", len(train_tokens), len(test_tokens) )\n",
    "\n",
    "    train_tokens_ids = pad_sequences(list(map(bert_tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=256, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "    test_tokens_ids = pad_sequences(list(map(bert_tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=256, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "\n",
    "    print (\"Token Ids:\",train_tokens_ids.shape, test_tokens_ids.shape)\n",
    "\n",
    "    train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
    "    test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
    "\n",
    "    print (\"Masks: \", len(train_masks), len(test_masks))\n",
    "    return train_tokens, test_tokens, train_tokens_ids, test_tokens_ids, train_masks, test_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "EPOCHS = 10\n",
    "def get_tensor_dataset(train_tokens_ids,test_tokens_ids,y_train,y_test, train_masks, test_masks):\n",
    "    train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
    "    train_y_tensor = torch.tensor(y_train).float()\n",
    "\n",
    "    test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
    "    test_y_tensor = torch.tensor(y_test).float()\n",
    "\n",
    "    train_masks_tensor = torch.tensor(train_masks)\n",
    "    test_masks_tensor = torch.tensor(test_masks)\n",
    "\n",
    "    print ( str(torch.cuda.memory_allocated(torch.cuda.current_device())/1000000 ) + 'M')\n",
    "\n",
    "    train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
    "    #train_sampler = RandomSampler(train_dataset)\n",
    "    #train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "    train_dataloader = DataLoader(train_dataset,  batch_size=BATCH_SIZE)\n",
    "\n",
    "    test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
    "    #test_sampler = SequentialSampler(test_dataset)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 1800 200\n",
      "Token Ids: (1800, 256) (200, 256)\n",
      "Masks:  1800 200\n"
     ]
    }
   ],
   "source": [
    "train_tokens, test_tokens, train_tokens_ids, test_tokens_ids, train_masks, test_masks = get_bert_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014848M\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader  = \\\n",
    "get_tensor_dataset(train_tokens_ids,test_tokens_ids,y_train,y_test, train_masks, test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop BERT Multilanguage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is required for multilingual bert\n",
    "\n",
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained( 'bert-base-multilingual-uncased')  \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        #_, pooled_output = self.bert(tokens, attention_mask=masks)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf = BertBinaryClassifier()\n",
    "bert_clf = bert_clf.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
    "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = Adam(bert_clf.parameters(), lr=3e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert():\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        bert_clf.train()\n",
    "        train_loss = 0\n",
    "        for step_num, batch_data in enumerate(train_dataloader):\n",
    "            token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "            #print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
    "            logits = bert_clf(token_ids, masks)\n",
    "\n",
    "            loss_func = nn.BCELoss()\n",
    "\n",
    "            batch_loss = loss_func(logits, labels)\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "\n",
    "            bert_clf.zero_grad()\n",
    "            batch_loss.backward()\n",
    "\n",
    "\n",
    "            clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print('Epoch: ', epoch_num + 1)\n",
    "            print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(token_ids) / BATCH_SIZE, train_loss / (step_num + 1)))\n",
    "            \n",
    "        return bert_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "\r\n",
      "899/1.0 loss: 0.6341746076279217 \n"
     ]
    }
   ],
   "source": [
    "bert_clf = train_bert()\n",
    "bert_clf = bert_clf.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Multilanguage - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    bert_clf.eval()\n",
    "    bert_predicted = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for step_num, batch_data in enumerate(test_dataloader):\n",
    "\n",
    "            token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "            logits = bert_clf(token_ids, masks)\n",
    "            loss_func = nn.BCELoss()\n",
    "            loss = loss_func(logits, labels)\n",
    "            numpy_logits = logits.cpu().detach().numpy()\n",
    "\n",
    "            #bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
    "            bert_predicted += list(numpy_logits[:, 0])\n",
    "            \n",
    "    return bert_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    }
   ],
   "source": [
    "bert_predicted_multi = evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "prediction['mBERT_PytorchTT'] = bert_predicted_multi\n",
    "\n",
    "print(classification_report(y_test, np.array(bert_predicted_multi)>.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> mBERT / Multilanguage - Task Transfer - Transformer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer as BertTokenizer2, BertModel as BertModel2\n",
    "bert_model_multi     = BertModel2.from_pretrained(modelfolder + r'\\bert-base-multilingual-uncased')\n",
    "bert_tokenizer_multi = BertTokenizer2.from_pretrained(modelfolder + r'\\bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 BertTransformer(bert_model=None, bert_tokenizer=None,\n",
       "                                 embedding_func=<function BertTransformer.__init__.<locals>.<lambda> at 0x000001553D5AED30>,\n",
       "                                 max_length=200)),\n",
       "                ('classifier', GaussianNB())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_transformer_multi = BertTransformer(bert_tokenizer_multi, bert_model_multi, max_length=200)\n",
    "#classifier = svm.LinearSVC(C=1.0, class_weight=\"balanced\")\n",
    "#classifier = LogisticRegression()\n",
    "classifier = GaussianNB()\n",
    "\n",
    "model_pipeline_multi = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", bert_transformer_multi),\n",
    "        (\"classifier\", classifier),\n",
    "    ]\n",
    ")\n",
    "model_pipeline_multi.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mBERT  /Multilanguage - Transformer - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_multi = model_pipeline_multi.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.58\n",
      "Recall   :  0.65\n",
      "Precision:  0.57\n",
      "F1       :  0.61\n",
      "ROC      :  0.58\n"
     ]
    }
   ],
   "source": [
    "prediction['mBERT_TransformerTT'] = y_pred_multi[:,1]\n",
    "print_metrics(y_pred_multi[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> mBERT (Multilanguage) - Embedding Transfer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This steps takes approx 5 min.\n",
    "\n",
    "import torch\n",
    "\n",
    "mbert_embedding=[]\n",
    "for i in range(len(df_full)):\n",
    "    txt = df_full.iloc[i,:]['sentence']\n",
    "    token_dict = bert_tokenizer_multi.encode_plus(txt, add_special_tokens=False)\n",
    "    embed = bert_model_multi((torch.tensor(token_dict[\"input_ids\"]).unsqueeze(0)))\n",
    "    embed = embed[1][0].detach().numpy()\n",
    "    #print (i, sep='\\t')\n",
    "    mbert_embedding.append(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.094827</td>\n",
       "      <td>-0.215950</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.234672</td>\n",
       "      <td>0.196442</td>\n",
       "      <td>0.032061</td>\n",
       "      <td>0.441763</td>\n",
       "      <td>-0.226298</td>\n",
       "      <td>-0.364357</td>\n",
       "      <td>0.092771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.241572</td>\n",
       "      <td>-0.193821</td>\n",
       "      <td>-0.136545</td>\n",
       "      <td>0.077711</td>\n",
       "      <td>0.382187</td>\n",
       "      <td>-0.238862</td>\n",
       "      <td>0.214421</td>\n",
       "      <td>-0.165181</td>\n",
       "      <td>-0.177810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.038516</td>\n",
       "      <td>-0.242589</td>\n",
       "      <td>-0.016740</td>\n",
       "      <td>0.177583</td>\n",
       "      <td>0.259062</td>\n",
       "      <td>-0.056287</td>\n",
       "      <td>0.393562</td>\n",
       "      <td>-0.124342</td>\n",
       "      <td>-0.305216</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242798</td>\n",
       "      <td>-0.161236</td>\n",
       "      <td>-0.138323</td>\n",
       "      <td>0.054489</td>\n",
       "      <td>0.389266</td>\n",
       "      <td>-0.065651</td>\n",
       "      <td>0.266223</td>\n",
       "      <td>-0.181780</td>\n",
       "      <td>-0.205100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.049239</td>\n",
       "      <td>-0.187880</td>\n",
       "      <td>-0.150780</td>\n",
       "      <td>0.173796</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>-0.029632</td>\n",
       "      <td>0.253110</td>\n",
       "      <td>-0.011287</td>\n",
       "      <td>-0.269391</td>\n",
       "      <td>0.044454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098685</td>\n",
       "      <td>-0.133191</td>\n",
       "      <td>-0.070416</td>\n",
       "      <td>-0.030003</td>\n",
       "      <td>0.250877</td>\n",
       "      <td>-0.089087</td>\n",
       "      <td>0.122501</td>\n",
       "      <td>-0.123521</td>\n",
       "      <td>-0.141871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.091381</td>\n",
       "      <td>-0.213047</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.234689</td>\n",
       "      <td>0.212581</td>\n",
       "      <td>0.039017</td>\n",
       "      <td>0.450925</td>\n",
       "      <td>-0.237699</td>\n",
       "      <td>-0.366408</td>\n",
       "      <td>0.108367</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252267</td>\n",
       "      <td>-0.209274</td>\n",
       "      <td>-0.139032</td>\n",
       "      <td>0.090818</td>\n",
       "      <td>0.392109</td>\n",
       "      <td>-0.244524</td>\n",
       "      <td>0.209654</td>\n",
       "      <td>-0.160693</td>\n",
       "      <td>-0.187955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.103511</td>\n",
       "      <td>-0.217064</td>\n",
       "      <td>0.008468</td>\n",
       "      <td>0.232815</td>\n",
       "      <td>0.178900</td>\n",
       "      <td>0.027574</td>\n",
       "      <td>0.430879</td>\n",
       "      <td>-0.205967</td>\n",
       "      <td>-0.356071</td>\n",
       "      <td>0.073888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231867</td>\n",
       "      <td>-0.176149</td>\n",
       "      <td>-0.130337</td>\n",
       "      <td>0.062329</td>\n",
       "      <td>0.371234</td>\n",
       "      <td>-0.230683</td>\n",
       "      <td>0.218179</td>\n",
       "      <td>-0.172345</td>\n",
       "      <td>-0.170212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "ID                                                                         \n",
       "1  -0.094827 -0.215950  0.002612  0.234672  0.196442  0.032061  0.441763   \n",
       "5  -0.038516 -0.242589 -0.016740  0.177583  0.259062 -0.056287  0.393562   \n",
       "12 -0.049239 -0.187880 -0.150780  0.173796  0.074700 -0.029632  0.253110   \n",
       "13 -0.091381 -0.213047 -0.000264  0.234689  0.212581  0.039017  0.450925   \n",
       "26 -0.103511 -0.217064  0.008468  0.232815  0.178900  0.027574  0.430879   \n",
       "\n",
       "           7         8         9  ...       759       760       761       762  \\\n",
       "ID                                ...                                           \n",
       "1  -0.226298 -0.364357  0.092771  ... -0.241572 -0.193821 -0.136545  0.077711   \n",
       "5  -0.124342 -0.305216 -0.001444  ... -0.242798 -0.161236 -0.138323  0.054489   \n",
       "12 -0.011287 -0.269391  0.044454  ... -0.098685 -0.133191 -0.070416 -0.030003   \n",
       "13 -0.237699 -0.366408  0.108367  ... -0.252267 -0.209274 -0.139032  0.090818   \n",
       "26 -0.205967 -0.356071  0.073888  ... -0.231867 -0.176149 -0.130337  0.062329   \n",
       "\n",
       "         763       764       765       766       767  label  \n",
       "ID                                                           \n",
       "1   0.382187 -0.238862  0.214421 -0.165181 -0.177810      1  \n",
       "5   0.389266 -0.065651  0.266223 -0.181780 -0.205100      1  \n",
       "12  0.250877 -0.089087  0.122501 -0.123521 -0.141871      1  \n",
       "13  0.392109 -0.244524  0.209654 -0.160693 -0.187955      1  \n",
       "26  0.371234 -0.230683  0.218179 -0.172345 -0.170212      1  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mbert = pd.DataFrame(mbert_embedding, index=df_full.index)\n",
    "df_mbert['label'] = df_full['label']\n",
    "df_mbert.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save BERT Multilangue Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbert.to_csv(datafolder_p+r'\\embedding_bert768_mbert_pretrained.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> IndicBERT -Task Transfer - Transformer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "bert_tokenizer_indic = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "bert_model_indic = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 BertTransformer(bert_model=None, bert_tokenizer=None,\n",
       "                                 embedding_func=<function BertTransformer.__init__.<locals>.<lambda> at 0x0000015534EBDA60>,\n",
       "                                 max_length=200)),\n",
       "                ('classifier', GaussianNB())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bert_transformer_indic = BertTransformer(bert_tokenizer_indic, bert_model_indic, max_length=200)\n",
    "#classifier = svm.LinearSVC(C=1.0, class_weight=\"balanced\")\n",
    "#classifier = LogisticRegression()\n",
    "classifier = GaussianNB()\n",
    "\n",
    "model_pipeline_indic = Pipeline(\n",
    "    [\n",
    "        (\"vectorizer\", bert_transformer_indic),\n",
    "        (\"classifier\", classifier),\n",
    "    ]\n",
    ")\n",
    "model_pipeline_indic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IndicBERT  - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_indic = model_pipeline_indic.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.58\n",
      "Recall   :  0.63\n",
      "Precision:  0.57\n",
      "F1       :  0.6\n",
      "ROC      :  0.58\n"
     ]
    }
   ],
   "source": [
    "prediction['IndicBERT_TT'] = y_pred_indic[:,1]\n",
    "print_metrics(y_pred_indic[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> IndicBERT - Embedding Transfer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This steps takes approx 5 min.\n",
    "\n",
    "indicbert_embedding=[]\n",
    "for i in range(len(df_full)):\n",
    "    txt = df_full.iloc[i,:]['sentence']\n",
    "    token_dict = bert_tokenizer_indic.encode_plus(txt, add_special_tokens=False, max_length=200, truncation=True)\n",
    "    embed = bert_model_indic((torch.tensor(token_dict[\"input_ids\"]).unsqueeze(0)))\n",
    "    embed = embed[1][0].detach().numpy()\n",
    "    #print (i, sep='\\t')\n",
    "    indicbert_embedding.append(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.053525</td>\n",
       "      <td>0.054395</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>-0.003695</td>\n",
       "      <td>0.040563</td>\n",
       "      <td>0.080847</td>\n",
       "      <td>0.035940</td>\n",
       "      <td>-0.005091</td>\n",
       "      <td>-0.018056</td>\n",
       "      <td>0.050841</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031083</td>\n",
       "      <td>0.057919</td>\n",
       "      <td>0.128374</td>\n",
       "      <td>-0.057468</td>\n",
       "      <td>-0.054164</td>\n",
       "      <td>-0.063550</td>\n",
       "      <td>-0.030812</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.015657</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.054423</td>\n",
       "      <td>0.050057</td>\n",
       "      <td>0.026642</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.040388</td>\n",
       "      <td>0.084593</td>\n",
       "      <td>0.040480</td>\n",
       "      <td>-0.006151</td>\n",
       "      <td>-0.018039</td>\n",
       "      <td>0.049458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027824</td>\n",
       "      <td>0.066910</td>\n",
       "      <td>0.133481</td>\n",
       "      <td>-0.054165</td>\n",
       "      <td>-0.059787</td>\n",
       "      <td>-0.067824</td>\n",
       "      <td>-0.032006</td>\n",
       "      <td>0.032855</td>\n",
       "      <td>0.019553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.052677</td>\n",
       "      <td>0.054255</td>\n",
       "      <td>0.020152</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.038358</td>\n",
       "      <td>0.079170</td>\n",
       "      <td>0.039753</td>\n",
       "      <td>-0.004033</td>\n",
       "      <td>-0.014087</td>\n",
       "      <td>0.051323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030897</td>\n",
       "      <td>0.061226</td>\n",
       "      <td>0.132689</td>\n",
       "      <td>-0.061606</td>\n",
       "      <td>-0.064817</td>\n",
       "      <td>-0.065052</td>\n",
       "      <td>-0.039030</td>\n",
       "      <td>0.036289</td>\n",
       "      <td>0.016919</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.056458</td>\n",
       "      <td>0.047421</td>\n",
       "      <td>0.008888</td>\n",
       "      <td>-0.004903</td>\n",
       "      <td>0.043382</td>\n",
       "      <td>0.084137</td>\n",
       "      <td>0.030783</td>\n",
       "      <td>-0.006145</td>\n",
       "      <td>-0.014157</td>\n",
       "      <td>0.050950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025582</td>\n",
       "      <td>0.060028</td>\n",
       "      <td>0.126639</td>\n",
       "      <td>-0.054046</td>\n",
       "      <td>-0.056553</td>\n",
       "      <td>-0.067852</td>\n",
       "      <td>-0.035591</td>\n",
       "      <td>0.026892</td>\n",
       "      <td>0.020892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.055969</td>\n",
       "      <td>0.045073</td>\n",
       "      <td>0.028485</td>\n",
       "      <td>-0.003376</td>\n",
       "      <td>0.040905</td>\n",
       "      <td>0.086122</td>\n",
       "      <td>0.036461</td>\n",
       "      <td>-0.003613</td>\n",
       "      <td>-0.019812</td>\n",
       "      <td>0.051705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036923</td>\n",
       "      <td>0.058726</td>\n",
       "      <td>0.134128</td>\n",
       "      <td>-0.059550</td>\n",
       "      <td>-0.056367</td>\n",
       "      <td>-0.062027</td>\n",
       "      <td>-0.031240</td>\n",
       "      <td>0.032978</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "ID                                                                         \n",
       "1  -0.053525  0.054395  0.017980 -0.003695  0.040563  0.080847  0.035940   \n",
       "5  -0.054423  0.050057  0.026642  0.001021  0.040388  0.084593  0.040480   \n",
       "12 -0.052677  0.054255  0.020152  0.000721  0.038358  0.079170  0.039753   \n",
       "13 -0.056458  0.047421  0.008888 -0.004903  0.043382  0.084137  0.030783   \n",
       "26 -0.055969  0.045073  0.028485 -0.003376  0.040905  0.086122  0.036461   \n",
       "\n",
       "           7         8         9  ...       759       760       761       762  \\\n",
       "ID                                ...                                           \n",
       "1  -0.005091 -0.018056  0.050841  ... -0.031083  0.057919  0.128374 -0.057468   \n",
       "5  -0.006151 -0.018039  0.049458  ... -0.027824  0.066910  0.133481 -0.054165   \n",
       "12 -0.004033 -0.014087  0.051323  ... -0.030897  0.061226  0.132689 -0.061606   \n",
       "13 -0.006145 -0.014157  0.050950  ... -0.025582  0.060028  0.126639 -0.054046   \n",
       "26 -0.003613 -0.019812  0.051705  ... -0.036923  0.058726  0.134128 -0.059550   \n",
       "\n",
       "         763       764       765       766       767  label  \n",
       "ID                                                           \n",
       "1  -0.054164 -0.063550 -0.030812  0.034091  0.015657      1  \n",
       "5  -0.059787 -0.067824 -0.032006  0.032855  0.019553      1  \n",
       "12 -0.064817 -0.065052 -0.039030  0.036289  0.016919      1  \n",
       "13 -0.056553 -0.067852 -0.035591  0.026892  0.020892      1  \n",
       "26 -0.056367 -0.062027 -0.031240  0.032978  0.009224      1  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indicbert = pd.DataFrame(indicbert_embedding, index=df_full.index)\n",
    "df_indicbert['label'] = df_full['label']\n",
    "df_indicbert.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save BERT indicnlp Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indicbert.to_csv(datafolder_p+r'\\embedding_bert768_indicbert_pretrained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> Save Task Transfer Prediction Results of mBERT and indicBERT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mBERT_TransformerTT</th>\n",
       "      <th>IndicBERT_TransformerTT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>3.900909e-01</td>\n",
       "      <td>4.119921e-40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>2.755473e-04</td>\n",
       "      <td>2.135327e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.874681e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>3.214948e-50</td>\n",
       "      <td>5.854190e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8819</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>1.259107e-38</td>\n",
       "      <td>2.715738e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>1.208542e-03</td>\n",
       "      <td>7.116999e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>9.994519e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mBERT_TransformerTT  IndicBERT_TransformerTT\n",
       "ID                                                \n",
       "5212         3.900909e-01             4.119921e-40\n",
       "8028         1.000000e+00             1.000000e+00\n",
       "2364         1.000000e+00             1.000000e+00\n",
       "5805         2.755473e-04             2.135327e-19\n",
       "5236         1.000000e+00             5.874681e-03\n",
       "...                   ...                      ...\n",
       "7171         3.214948e-50             5.854190e-09\n",
       "8819         1.000000e+00             1.000000e+00\n",
       "2686         1.259107e-38             2.715738e-32\n",
       "8692         1.208542e-03             7.116999e-21\n",
       "8533         9.994519e-01             1.000000e+00\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction= pd.DataFrame(prediction, index=df_test.ID)\n",
    "df_prediction.to_csv(resultsfolder+ r'\\model_predictions_BERT.csv')\n",
    "df_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AZBUEHxZNin"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "11.4-embedding_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
