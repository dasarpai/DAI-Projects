{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-3.9.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.9)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Installing collected packages: tweepy\n",
      "Successfully installed tweepy-3.9.0\n",
      "Collecting jsonpickle\n",
      "  Using cached jsonpickle-1.4.1-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: importlib-metadata in d:\\users\\admin\\anaconda3\\lib\\site-packages (from jsonpickle) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from importlib-metadata->jsonpickle) (3.1.0)\n",
      "Installing collected packages: jsonpickle\n",
      "Successfully installed jsonpickle-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import jsonpickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save files in this folder\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create convert json to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCSV_fromJson(csv_fname, txt_fname):\n",
    "    f = open(csv_fname,'a',encoding='utf8')\n",
    "    csvWriter = csv.writer(f)\n",
    "    headers=['full_text','retweet_count','user_followers_count','favorite_count','place','coordinates','geo','created_at','id_str']\n",
    "    #headers=['full_text']\n",
    "    csvWriter.writerow(headers)\n",
    "\n",
    "    for inputFile in [txt_fname]:#all the text-file names you want to convert to Csv in the sae folder as this code\n",
    "        tweets = []\n",
    "        for line in open(inputFile, 'r'):\n",
    "            tweets.append(json.loads(line))\n",
    "\n",
    "        \n",
    "        count_lines=0\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                csvWriter.writerow([tweet['full_text'],tweet['retweet_count'],tweet['user']['followers_count'],tweet['favorite_count'],tweet['place'],tweet['coordinates'],tweet['geo'],tweet['created_at'],str(tweet['id_str'])])\n",
    "                #csvWriter.writerow(tweet['full_text'])\n",
    "                count_lines+=1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        print(count_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Twitter using Tweepy api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that contains the credentials to access Twitter API\n",
    "ACCESS_TOKEN = '1683653820-p2Ehkyl2OQUmaY7Xf7H9pc4ErNvyvovBVwxy9GC'\n",
    "ACCESS_SECRET = 'TbtPT96BPIc31FK21b1iX9hcLfg4XOqiRZWrq5gAERoPc'\n",
    "\n",
    "#CONSUMER_KEY / CONSUMER_SECRET\n",
    "API_KEY = 'kUHB8y0fHK5WiGapp4CffpP2g'\n",
    "API_SECRET ='Gd614dFtCjGWwgN9M4t6jVT83q5kntkyDFDRmpZE8D7JfGBLI7'\n",
    "\n",
    "# Setup access to API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "    return api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create API object\n",
    "api = connect_to_twitter_OAuth()\n",
    "status = \"Testing!\"\n",
    "#api.update_status(status=status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweets from my stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CreateJsonFile_ofTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateJsonFile_ofTweets(fname, hashtag, max_tweets=1000000):\n",
    "    #auth = tweepy.AppAuthHandler(API_KEY,API_SECRET)\n",
    "    #api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    api = connect_to_twitter_OAuth()\n",
    "    #wait_on_rate_limit ‚Äì Whether or not to automatically wait for rate limits to replenish\n",
    "    #wait_on_rate_limit_notify ‚Äì Whether or not to print a notification when Tweepy is waiting for rate limits to replenish\n",
    "    # the sleep mode is automatically enabled with above 2 args\n",
    "\n",
    "    tweetsPerQuery = 100#this is the maximum provided by API\n",
    "    #max_tweets just for the sake of While loop\n",
    "\n",
    "    # No sinceId and max_id ..Get whathever you have exhaustively\n",
    "    since_id = None\n",
    "    max_id = -1\n",
    "    tweet_count = 0\n",
    "    print(\"Downloading the tweeets..takes some time..\")\n",
    "\n",
    "    search_query=hashtag \n",
    "    x=0\n",
    "    with open(fname,'w') as f:\n",
    "        print(\"Downloading hashtag\" + search_query)\n",
    "        while(tweet_count<max_tweets):\n",
    "            try:\n",
    "                if(max_id<=0):\n",
    "                    if(not since_id):\n",
    "                        new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"hi\",\n",
    "                                                tweet_mode='extended', include_rts=False)\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"hi\",\n",
    "                                                tweet_mode='extended',since_id=since_id, include_rts=False)\n",
    "                else:\n",
    "                    if(not since_id):\n",
    "                        new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"hi\",\n",
    "                                                tweet_mode='extended',max_id=str(max_id-1), include_rts=False)\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"hi\",tweet_mode='extended',\n",
    "                                                max_id=str(max_id-1),since_id=since_id, include_rts=False)\n",
    "                \n",
    "                # Tweets Exhausted\n",
    "                if(not new_tweets):\n",
    "                    print(\"No more tweets found!!\")\n",
    "                    break\n",
    "                    \n",
    "                # write all the new_tweets to a json file\n",
    "                for tweet in new_tweets:\n",
    "                    f.write(jsonpickle.encode(tweet._json,unpicklable=False)+'\\n')\n",
    "                    tweet_count+=len(new_tweets)\n",
    "                    #print(\"Successfully downloaded {0} tweets\".format(tweet_count))\n",
    "                    max_id=new_tweets[-1].id\n",
    "                    \n",
    "            # in case of any error\n",
    "            except tweepy.TweepError as e:\n",
    "                    print(\"Some error!!:\"+str(e))\n",
    "                    break\n",
    "    return tweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweets from a specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = api.user_timeline(account, count=20000, lang=\"hi\", tweet_mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "199\n",
      "200\n",
      "200\n",
      "200\n",
      "197\n",
      "200\n",
      "198\n",
      "197\n",
      "200\n",
      "200\n",
      "192\n",
      "111\n",
      "200\n",
      "162\n",
      "197\n",
      "191\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "199\n",
      "197\n",
      "200\n",
      "198\n",
      "200\n",
      "200\n",
      "198\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "199\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "199\n",
      "Saving file D:\\18-DS\\github\\SDSHL\\data\\external\\\\ridhi_bose.csv\n"
     ]
    }
   ],
   "source": [
    "#disabled accounts = 'Real_Netan', 'sandeep353055' yatisharma111, Arnab5222, 'PbSwain_IND', DeshbhaktRosha1, ShrishtySharma_\n",
    "\n",
    "accounts = ['acs_rkpandey','Bittu_Tufani','Gulmoha99443364','maliksarah5gma1','mjaved819',\n",
    "            'Rajeshkm_RJS','pallavinaharre1','raviag07','samratkikavitae','Sourav1717',\n",
    "            'Sujitkumar_mau','theBookwalla','gareeb_bhartiya','AkshayM90903668','vptvns',\n",
    "            'Vyang_Kumar','HindiSatire','Shanu44339200','SwetaSinghAT','VinodRajotiya82',\n",
    "            'gautam007','Republic_Bharat','VickyAarya007','KapilMishra_IND','VyangyaVahini',\n",
    "            'DChaurasia2312','WiskyWala','ManojTiwariMP', 'chitraaum','RealPushpendra',\n",
    "            'RajatSharmaLive','RubikaLiyaquat','TheAbhishek_IND','Real_Sweta','SushantBSinha',\n",
    "            'VickyAarya007','badri_dk','Abhasin89009555','BhartiyRudr','Jainritesh_rj','TeriJogan','ridhi_bose']\n",
    "\n",
    "\n",
    "dfs=[]\n",
    "for account in accounts:\n",
    "    tweets = api.user_timeline(account, count=20000, lang=\"hi\", tweet_mode=\"extended\")\n",
    "    tweets = [[account,tweet.full_text] for tweet in tweets]\n",
    "    #tweets = (tweet for tweet in tweets)\n",
    "    df = pd.DataFrame( tweets , columns=['account',\"Text\"])\n",
    "    dfs.append(df)\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    print (len(dfs[i]))\n",
    "    if i==0:\n",
    "        df=dfs[0]\n",
    "        i=1\n",
    "    else:\n",
    "        df=pd.concat( [df, dfs[i] ], ignore_index=True)\n",
    "        i+=1\n",
    "    \n",
    "df.to_csv(datafolder_e + r'\\\\' + account+\".csv\")\n",
    "print (\"Saving file \"+ datafolder_e + r'\\twitter_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweets from a specific hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the tweeets..takes some time..\n",
      "Downloading hashtag#‡§ï‡§ü‡§æ‡§ï‡•ç‡§∑\n",
      "100\n",
      "A total of 10000 tweets are downloaded and saved to ‡§ï‡§ü‡§æ‡§ï‡•ç‡§∑.txt\n",
      "Total time taken is  2.0661940574645996 seconds.\n",
      "_‡§ï‡§ü‡§æ‡§ï‡•ç‡§∑.csv file created\n",
      "=====================>\n",
      "Downloading the tweeets..takes some time..\n",
      "Downloading hashtag##‡§µ‡•ç‡§Ø‡§Ç‡§ó\n",
      "60\n",
      "A total of 3600 tweets are downloaded and saved to #‡§µ‡•ç‡§Ø‡§Ç‡§ó.txt\n",
      "Total time taken is  4.286127328872681 seconds.\n",
      "_#‡§µ‡•ç‡§Ø‡§Ç‡§ó.csv file created\n",
      "=====================>\n",
      "Downloading the tweeets..takes some time..\n",
      "Downloading hashtag#‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø\n",
      "100\n",
      "A total of 10000 tweets are downloaded and saved to ‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø.txt\n",
      "Total time taken is  4.182105779647827 seconds.\n",
      "_‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø.csv file created\n",
      "=====================>\n",
      "Downloading the tweeets..takes some time..\n",
      "Downloading hashtag#‡§Æ‡•ã‡§¶‡•Ä_‡§´‡§ø‡§∞‡§ï‡•Ä_‡§≤‡•á_‡§∞‡§π‡§æ_‡§π‡•à\n",
      "No more tweets found!!\n",
      "0\n",
      "A total of 0 tweets are downloaded and saved to ‡§Æ‡•ã‡§¶‡•Ä_‡§´‡§ø‡§∞‡§ï‡•Ä_‡§≤‡•á_‡§∞‡§π‡§æ_‡§π‡•à.txt\n",
      "Total time taken is  1.3484437465667725 seconds.\n",
      "_‡§Æ‡•ã‡§¶‡•Ä_‡§´‡§ø‡§∞‡§ï‡•Ä_‡§≤‡•á_‡§∞‡§π‡§æ_‡§π‡•à.csv file created\n",
      "=====================>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hashtags= [\"‡§ï‡§ü‡§æ‡§ï‡•ç‡§∑\", \"#‡§µ‡•ç‡§Ø‡§Ç‡§ó\", \"‡§µ‡•ç‡§Ø‡§Ç‡§ó‡•ç‡§Ø\",\"‡§Æ‡•ã‡§¶‡•Ä_‡§´‡§ø‡§∞‡§ï‡•Ä_‡§≤‡•á_‡§∞‡§π‡§æ_‡§π‡•à\"]\n",
    "#hashtags= [\"‡§Æ‡•ã‡§¶‡•Ä_‡§´‡§ø‡§∞‡§ï‡•Ä_‡§≤‡•á_‡§∞‡§π‡§æ_‡§π‡•à\"]\n",
    "\n",
    "for hashtag in hashtags:\n",
    "    st = time.time()\n",
    "    fname= hashtag+\".txt\"\n",
    "    tweet_count = CreateJsonFile_ofTweets(fname, \"#\"+hashtag, 1000)\n",
    "    createCSV_fromJson(hashtag+'.csv', hashtag+'.txt')\n",
    "    end = time.time()\n",
    "    print(\"A total of {0} tweets are downloaded and saved to {1}\".format(tweet_count,fname))\n",
    "    print(\"Total time taken is \",end-st,\"seconds.\")\n",
    "    #df = pd.read_csv(hashtag+'.csv',  usecols=['created_at','full_text'], encoding = 'utf8')\n",
    "    df = pd.read_csv(hashtag+'.csv',  usecols=['full_text'], encoding = 'utf8')\n",
    "    \n",
    "    #df= df.rename(columns={'created_at':'Time','full_text':'Text'})\n",
    "    df= df.rename(columns={'full_text':'Text'})\n",
    "    #df=df[ ['Time','Text'] ]\n",
    "    df=df[ ['Text'] ]\n",
    "    df.to_csv( datafolder_e + r'\\\\' + \"_\"+hashtag+'.csv')\n",
    "    print(datafolder_e + r\"\\_\"+ hashtag+'.csv'+ \" file created\")\n",
    "    print (\"=====================>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @saurv365: ‡§≠‡•à‡§Ç‡§∏ - ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞, ‡§ï‡•ã‡§µ‡§ø‡§° ‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä, ‡§Ö‡§∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>‡§≠‡•à‡§Ç‡§∏ - ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞, ‡§ï‡•ã‡§µ‡§ø‡§° ‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä, ‡§Ö‡§∞‡•ç‡§•‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ, ‡§ï‡§ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @0007tilak: ‡§Ü‡§ú ‡§µ‡•ã ‡§≠‡•Ä #DaughtersDay2020 ‡§ï‡•Ä ‡§∂...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @AjjuBhaiyaa: ‡§Æ‡•á‡§∞‡•Ä ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§∏‡•á ‡§¨‡§®‡§§‡•Ä ‡§®‡§π‡•Ä‡§Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>‡§Æ‡•á‡§∞‡•Ä ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§∏‡•á ‡§¨‡§®‡§§‡•Ä ‡§®‡§π‡•Ä‡§Ç \\r\\n‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§§‡§®‡§æ ‡§≠‡•Ä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>‡§Ü‡§ú ‡§µ‡•ã ‡§≠‡•Ä #DaughtersDay2020 ‡§ï‡•Ä ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§ì‡§Ç ‡§∏‡§π‡§ø‡§§ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>‡§¨‡§°‡§º‡•Ä ‡§ñ‡§¨‡§∞- ‡§¶‡•Ä‡§™‡§ø‡§ï‡§æ ‡§®‡•á ‡§∞‡§£‡§µ‡•Ä‡§∞ ‡§∏‡•á ‡§¨‡•ã‡§≤‡§æ ‡§Ü‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§®‡•Ä‡§ö‡•á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>RT @AsifRazakhan_: #‡§ï‡§ü‡§æ‡§ï‡•ç‡§∑\\r\\n500-500 ‡§∏‡•å ‡§∞‡•Ç‡§™‡§Ø‡•á...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>300</td>\n",
       "      <td>#‡§ï‡§ü‡§æ‡§ï‡•ç‡§∑\\r\\n500-500 ‡§∏‡•å ‡§∞‡•Ç‡§™‡§Ø‡•á\\r\\n‡§î‡§∞ ‡§è‡§ï ‡§™‡•ç‡§≤‡•á‡§ü ‡§¨‡§ø‡§∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>301</td>\n",
       "      <td>RT @aditiraval: My mom is watching #Anupamaa o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               Text\n",
       "0             0  RT @saurv365: ‡§≠‡•à‡§Ç‡§∏ - ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞, ‡§ï‡•ã‡§µ‡§ø‡§° ‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä, ‡§Ö‡§∞...\n",
       "1             1  ‡§≠‡•à‡§Ç‡§∏ - ‡§∞‡•ã‡§ú‡§ó‡§æ‡§∞, ‡§ï‡•ã‡§µ‡§ø‡§° ‡§Æ‡§π‡§æ‡§Æ‡§æ‡§∞‡•Ä, ‡§Ö‡§∞‡•ç‡§•‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ, ‡§ï‡§ø...\n",
       "2             2  RT @0007tilak: ‡§Ü‡§ú ‡§µ‡•ã ‡§≠‡•Ä #DaughtersDay2020 ‡§ï‡•Ä ‡§∂...\n",
       "3             3  RT @AjjuBhaiyaa: ‡§Æ‡•á‡§∞‡•Ä ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§∏‡•á ‡§¨‡§®‡§§‡•Ä ‡§®‡§π‡•Ä‡§Ç...\n",
       "4             4  ‡§Æ‡•á‡§∞‡•Ä ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§∏‡•á ‡§¨‡§®‡§§‡•Ä ‡§®‡§π‡•Ä‡§Ç \\r\\n‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§§‡§®‡§æ ‡§≠‡•Ä...\n",
       "..          ...                                                ...\n",
       "297         297  ‡§Ü‡§ú ‡§µ‡•ã ‡§≠‡•Ä #DaughtersDay2020 ‡§ï‡•Ä ‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§ì‡§Ç ‡§∏‡§π‡§ø‡§§ ...\n",
       "298         298  ‡§¨‡§°‡§º‡•Ä ‡§ñ‡§¨‡§∞- ‡§¶‡•Ä‡§™‡§ø‡§ï‡§æ ‡§®‡•á ‡§∞‡§£‡§µ‡•Ä‡§∞ ‡§∏‡•á ‡§¨‡•ã‡§≤‡§æ ‡§Ü‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§®‡•Ä‡§ö‡•á...\n",
       "299         299  RT @AsifRazakhan_: #‡§ï‡§ü‡§æ‡§ï‡•ç‡§∑\\r\\n500-500 ‡§∏‡•å ‡§∞‡•Ç‡§™‡§Ø‡•á...\n",
       "300         300  #‡§ï‡§ü‡§æ‡§ï‡•ç‡§∑\\r\\n500-500 ‡§∏‡•å ‡§∞‡•Ç‡§™‡§Ø‡•á\\r\\n‡§î‡§∞ ‡§è‡§ï ‡§™‡•ç‡§≤‡•á‡§ü ‡§¨‡§ø‡§∞...\n",
       "301         301  RT @aditiraval: My mom is watching #Anupamaa o...\n",
       "\n",
       "[302 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(datafolder_e + r'\\\\' + \"_\"+hashtags[0]+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(hashtags[0]+'.csv',  encoding = 'utf8')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all @ read text from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Text, dtype: object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ManojTiwariMP'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>account</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>‡§∞‡§æ‡§ó‡§æ- ‡§™‡§§‡§æ ‡§π‡•à ‡§Æ‡•á‡§∞‡•á ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§™‡•á\\n‡§ï‡•å‡§® ‡§™‡•à‡§¶‡§æ ‡§π‡•Å‡§Ü ‡§•‡§æ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>‡§ú‡§ø‡§∏ ‡§§‡§∞‡§π ‡§∞‡§æ‡§Æ ‡§∞‡§π‡•Ä‡§Æ ‡§ï‡•á 36 ‡§Ö‡§∂‡•ç‡§∞‡§Æ\\n‡§∏‡•Ä‡§≤ ‡§π‡•à ‡§â‡§∏‡•Ä ‡§§‡§∞‡§π ‡§Æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>@mechitraaa @Abhasin89009555 ye to kuchh nhi h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>‡§∏‡•Å‡§∂‡§æ‡§Ç‡§§ ‡§∏‡§ø‡§Ç‡§π ‡§∞‡§æ‡§ú‡§™‡•Ç‡§§\\n‡§Ü‡§§‡•ç‡§Æ‡§π‡§§‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§ï‡•Ä ‡§∏‡•Å‡§™‡•ç‡§∞‡•Ä‡§Æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>RT @Abhasin89009555: ‡§á‡§® ‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§∏‡§Æ‡§ù‡§ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @RajTiwa56444261: ‡§Æ‡§ø‡§§‡•ç‡§∞‡•ã ‡§Ü‡§ú ‡§π‡§Æ‡§æ‡§∞‡•á ‡§µ‡§ø‡§µ‡§æ‡§π ‡§ï‡•á ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @its_mahii01: ‡§Æ‡•à ‡§î‡§∞ ‡§Æ‡•á‡§∞‡•Ä ‡§§‡§®‡•ç‡§π‡§æ‡§à ‡§Ö‡§ï‡•ç‡§∏‡§∞\\n‡•§\\n‡•§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @universe__guru: ‡§¶‡•á‡§∂ ‡§¨‡§ö‡§æ ‡§≤‡•ã ‡§∏‡§æ‡§π‡§¨ ,\\n‡§Æ‡•ã‡§π‡§¨‡•ç‡§¨‡§§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @Chatterj1Asking: ‡§≤‡§ó‡§§‡§æ ‡§π‡•à 1962 ‡§ï‡§æ \\n‡§π‡§ø‡§∏‡§æ‡§¨ ‡§¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @shuklaji250: ‡§∏‡•Å‡§™‡•ç‡§∞‡§≠‡§æ‡§§, ‡§π‡§∞ ‡§π‡§∞ ‡§Æ‡§π‡§æ‡§¶‡•á‡§µ..!!üö©üôèüèª...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0          account  \\\n",
       "0              0    VickyAarya007   \n",
       "1              1    VickyAarya007   \n",
       "2              2    VickyAarya007   \n",
       "3              3    VickyAarya007   \n",
       "4              4    VickyAarya007   \n",
       "...          ...              ...   \n",
       "1995        1995  ShrishtySharma_   \n",
       "1996        1996  ShrishtySharma_   \n",
       "1997        1997  ShrishtySharma_   \n",
       "1998        1998  ShrishtySharma_   \n",
       "1999        1999  ShrishtySharma_   \n",
       "\n",
       "                                                   Text  \n",
       "0     ‡§∞‡§æ‡§ó‡§æ- ‡§™‡§§‡§æ ‡§π‡•à ‡§Æ‡•á‡§∞‡•á ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§™‡•á\\n‡§ï‡•å‡§® ‡§™‡•à‡§¶‡§æ ‡§π‡•Å‡§Ü ‡§•‡§æ ...  \n",
       "1     ‡§ú‡§ø‡§∏ ‡§§‡§∞‡§π ‡§∞‡§æ‡§Æ ‡§∞‡§π‡•Ä‡§Æ ‡§ï‡•á 36 ‡§Ö‡§∂‡•ç‡§∞‡§Æ\\n‡§∏‡•Ä‡§≤ ‡§π‡•à ‡§â‡§∏‡•Ä ‡§§‡§∞‡§π ‡§Æ...  \n",
       "2     @mechitraaa @Abhasin89009555 ye to kuchh nhi h...  \n",
       "3     ‡§∏‡•Å‡§∂‡§æ‡§Ç‡§§ ‡§∏‡§ø‡§Ç‡§π ‡§∞‡§æ‡§ú‡§™‡•Ç‡§§\\n‡§Ü‡§§‡•ç‡§Æ‡§π‡§§‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§ï‡•Ä ‡§∏‡•Å‡§™‡•ç‡§∞‡•Ä‡§Æ...  \n",
       "4     RT @Abhasin89009555: ‡§á‡§® ‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§∏‡§Æ‡§ù‡§ø...  \n",
       "...                                                 ...  \n",
       "1995  RT @RajTiwa56444261: ‡§Æ‡§ø‡§§‡•ç‡§∞‡•ã ‡§Ü‡§ú ‡§π‡§Æ‡§æ‡§∞‡•á ‡§µ‡§ø‡§µ‡§æ‡§π ‡§ï‡•á ...  \n",
       "1996  RT @its_mahii01: ‡§Æ‡•à ‡§î‡§∞ ‡§Æ‡•á‡§∞‡•Ä ‡§§‡§®‡•ç‡§π‡§æ‡§à ‡§Ö‡§ï‡•ç‡§∏‡§∞\\n‡•§\\n‡•§...  \n",
       "1997  RT @universe__guru: ‡§¶‡•á‡§∂ ‡§¨‡§ö‡§æ ‡§≤‡•ã ‡§∏‡§æ‡§π‡§¨ ,\\n‡§Æ‡•ã‡§π‡§¨‡•ç‡§¨‡§§...  \n",
       "1998  RT @Chatterj1Asking: ‡§≤‡§ó‡§§‡§æ ‡§π‡•à 1962 ‡§ï‡§æ \\n‡§π‡§ø‡§∏‡§æ‡§¨ ‡§¨...  \n",
       "1999  RT @shuklaji250: ‡§∏‡•Å‡§™‡•ç‡§∞‡§≠‡§æ‡§§, ‡§π‡§∞ ‡§π‡§∞ ‡§Æ‡§π‡§æ‡§¶‡•á‡§µ..!!üö©üôèüèª...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclean_text(x):\n",
    "    text_list= x.split(\" \")\n",
    "    clean_text = []\n",
    "    #don't include account\n",
    "    pattern = \"^[^@]\"\n",
    "    for token in text_list:\n",
    "        if re.search(pattern, token):\n",
    "            clean_text.append(token)\n",
    "    \n",
    "    #don't include http links\n",
    "    clean_text1 = []\n",
    "    pattern = \"^[^http]*[^https]\"\n",
    "    for token in clean_text:\n",
    "        if re.search(pattern, token):\n",
    "            clean_text1.append(token)\n",
    "    \n",
    "    #print (clean_text1)\n",
    "    clean_text1=\" \".join(clean_text1)\n",
    "    \n",
    "    clean_text1= clean_text1.replace(\"\\n\",\" \")\n",
    "    \n",
    "    return clean_text1\n",
    "\n",
    "df1 = df[ ['account','Text'] ]\n",
    "df1= pd.DataFrame(df1['Text'].apply(lambda x: getclean_text(x)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['account']=df.account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>account</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡§∞‡§æ‡§ó‡§æ- ‡§™‡§§‡§æ ‡§π‡•à ‡§Æ‡•á‡§∞‡•á ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§™‡•á ‡§ï‡•å‡§® ‡§™‡•à‡§¶‡§æ ‡§π‡•Å‡§Ü ‡§•‡§æ ?...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§ú‡§ø‡§∏ ‡§§‡§∞‡§π ‡§∞‡§æ‡§Æ ‡§∞‡§π‡•Ä‡§Æ ‡§ï‡•á 36 ‡§Ö‡§∂‡•ç‡§∞‡§Æ ‡§∏‡•Ä‡§≤ ‡§π‡•à ‡§â‡§∏‡•Ä ‡§§‡§∞‡§π ‡§Æ‡•å...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ye kuchh nhi me Jane ke liye isse aage bhi bah...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡§∏‡•Å‡§∂‡§æ‡§Ç‡§§ ‡§∏‡§ø‡§Ç‡§π ‡§∞‡§æ‡§ú‡§™‡•Ç‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§π‡§§‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§ï‡•Ä ‡§∏‡•Å‡§™‡•ç‡§∞‡•Ä‡§Æ ...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT ‡§á‡§® ‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§∏‡§Æ‡§ù‡§ø‡§è ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§™‡§°‡§º‡•Ä...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>RT ‡§Æ‡§ø‡§§‡•ç‡§∞‡•ã ‡§Ü‡§ú ‡§π‡§Æ‡§æ‡§∞‡•á ‡§µ‡§ø‡§µ‡§æ‡§π ‡§ï‡•á 18 ‡§µ‡§∞‡•ç‡§∑ ‡§™‡•Ç‡§∞‡•á ‡§π‡•Å‡§è.....</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>RT ‡§Æ‡•à ‡§î‡§∞ ‡§Æ‡•á‡§∞‡•Ä ‡§§‡§®‡•ç‡§π‡§æ‡§à ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡•§ ‡•§ ‡•§ ‡•§  ‡§ö‡§æ‡§á‡§®‡§æ ‡§µ‡§æ‡§≤‡•ã...</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>RT ‡§¶‡•á‡§∂ ‡§¨‡§ö‡§æ ‡§≤‡•ã ‡§∏‡§æ‡§π‡§¨ , ‡§Æ‡•ã‡§π‡§¨‡•ç‡§¨‡§§ ‡§§‡•ã ‡§ï‡§≠‡•Ä ‡§Æ‡§ø‡§≤‡§®‡•á ‡§∏‡•á ‡§∞...</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>RT ‡§≤‡§ó‡§§‡§æ ‡§π‡•à 1962 ‡§ï‡§æ  ‡§π‡§ø‡§∏‡§æ‡§¨ ‡§¨‡§∞‡§æ‡§¨‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§≠‡•Ä ‡§Æ‡•ã‡§¶‡•Ä‡§ú‡•Ä...</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>RT ‡§∏‡•Å‡§™‡•ç‡§∞‡§≠‡§æ‡§§, ‡§π‡§∞ ‡§π‡§∞ ‡§Æ‡§π‡§æ‡§¶‡•á‡§µ..!!üö©üôèüèª</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text          account\n",
       "0     ‡§∞‡§æ‡§ó‡§æ- ‡§™‡§§‡§æ ‡§π‡•à ‡§Æ‡•á‡§∞‡•á ‡§ú‡§®‡•ç‡§Æ‡§¶‡§ø‡§® ‡§™‡•á ‡§ï‡•å‡§® ‡§™‡•à‡§¶‡§æ ‡§π‡•Å‡§Ü ‡§•‡§æ ?...    VickyAarya007\n",
       "1     ‡§ú‡§ø‡§∏ ‡§§‡§∞‡§π ‡§∞‡§æ‡§Æ ‡§∞‡§π‡•Ä‡§Æ ‡§ï‡•á 36 ‡§Ö‡§∂‡•ç‡§∞‡§Æ ‡§∏‡•Ä‡§≤ ‡§π‡•à ‡§â‡§∏‡•Ä ‡§§‡§∞‡§π ‡§Æ‡•å...    VickyAarya007\n",
       "2     ye kuchh nhi me Jane ke liye isse aage bhi bah...    VickyAarya007\n",
       "3     ‡§∏‡•Å‡§∂‡§æ‡§Ç‡§§ ‡§∏‡§ø‡§Ç‡§π ‡§∞‡§æ‡§ú‡§™‡•Ç‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§π‡§§‡•ç‡§Ø‡§æ ‡§Æ‡§æ‡§Æ‡§≤‡•á ‡§ï‡•Ä ‡§∏‡•Å‡§™‡•ç‡§∞‡•Ä‡§Æ ...    VickyAarya007\n",
       "4     RT ‡§á‡§® ‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§¶‡•á‡§ñ‡§ï‡§∞ ‡§∏‡§Æ‡§ù‡§ø‡§è ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§™‡§°‡§º‡•Ä...    VickyAarya007\n",
       "...                                                 ...              ...\n",
       "1995  RT ‡§Æ‡§ø‡§§‡•ç‡§∞‡•ã ‡§Ü‡§ú ‡§π‡§Æ‡§æ‡§∞‡•á ‡§µ‡§ø‡§µ‡§æ‡§π ‡§ï‡•á 18 ‡§µ‡§∞‡•ç‡§∑ ‡§™‡•Ç‡§∞‡•á ‡§π‡•Å‡§è.....  ShrishtySharma_\n",
       "1996  RT ‡§Æ‡•à ‡§î‡§∞ ‡§Æ‡•á‡§∞‡•Ä ‡§§‡§®‡•ç‡§π‡§æ‡§à ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡•§ ‡•§ ‡•§ ‡•§  ‡§ö‡§æ‡§á‡§®‡§æ ‡§µ‡§æ‡§≤‡•ã...  ShrishtySharma_\n",
       "1997  RT ‡§¶‡•á‡§∂ ‡§¨‡§ö‡§æ ‡§≤‡•ã ‡§∏‡§æ‡§π‡§¨ , ‡§Æ‡•ã‡§π‡§¨‡•ç‡§¨‡§§ ‡§§‡•ã ‡§ï‡§≠‡•Ä ‡§Æ‡§ø‡§≤‡§®‡•á ‡§∏‡•á ‡§∞...  ShrishtySharma_\n",
       "1998  RT ‡§≤‡§ó‡§§‡§æ ‡§π‡•à 1962 ‡§ï‡§æ  ‡§π‡§ø‡§∏‡§æ‡§¨ ‡§¨‡§∞‡§æ‡§¨‡§∞ ‡§ï‡§∞‡§®‡§æ ‡§≠‡•Ä ‡§Æ‡•ã‡§¶‡•Ä‡§ú‡•Ä...  ShrishtySharma_\n",
       "1999                   RT ‡§∏‡•Å‡§™‡•ç‡§∞‡§≠‡§æ‡§§, ‡§π‡§∞ ‡§π‡§∞ ‡§Æ‡§π‡§æ‡§¶‡•á‡§µ..!!üö©üôèüèª  ShrishtySharma_\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[ ['account','Text'] ].to_csv( datafolder_e + r'\\\\' + \"_\"+accounts[0]+\".csv\", index=False )\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may want to save the timeline ids over there, to not to lose them during execution\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# Fetch tweet ids from a timeline \n",
    "# Use these block if this is there is no saved tweets from \n",
    "# this account or you dont plan to use since_id functionality \n",
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "counter = 0\n",
    "timeline_ids = list()\n",
    "start_time = datetime.now()\n",
    "\n",
    "user_id='Shanu44339200'\n",
    "\n",
    "try:\n",
    "    for status in tweepy.Cursor(api.user_timeline, user_id=user_id).items():\n",
    "        # process status here\n",
    "        timeline_ids.append(status.id)\n",
    "        end_time = datetime.now()\n",
    "        if np.ceil((end_time - start_time).seconds / 60) >= 12:\n",
    "            print(\"Worked for 12 minutes, waiting for 15 minutes now\")\n",
    "            print(datetime.now())\n",
    "            time.sleep(60 * 15)\n",
    "            start_time = datetime.now()\n",
    "            end_time = datetime.now()\n",
    "\n",
    "        counter += 1\n",
    "except tweepy.RateLimitError as e:\n",
    "    print(\"Rate limit error exceed waiting for 15 secs\")\n",
    "    print(datetime.now())\n",
    "    print(\"You may want to save the timeline ids over there, to not to lose them during execution\")\n",
    "    time.sleep(60 * 15)\n",
    "    start_time = datetime.now()\n",
    "    end_time = datetime.now()\n",
    "\n",
    "print(\"You may want to save the timeline ids over there, to not to lose them during execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 07:52:45 Testing!\n",
      "2020-05-23 07:41:16 Testing!\n",
      "2020-05-22 13:26:20 Testing!\n",
      "2019-09-07 18:09:31 RT @PMBhutan: We are proud of India and its scientists today. Chandrayaan-2 saw some challenges last minute but the courage and hard work y‚Ä¶\n",
      "2019-09-07 18:09:23 https://t.co/zIUnVdwrXu\n",
      "\n",
      "#nonviolence #foodchain #survival\n",
      "2019-09-07 07:26:16 @anandmahindra A true nationalist, patriotic, ethical business professional. India will always remember the way you‚Ä¶ https://t.co/RoL5t0aVVs\n",
      "2019-09-07 07:25:55 @anandmahindra every time I read your tweets. You occupy more space in my my mind and heart. Along with business, you are leader of masses.\n",
      "2019-09-07 07:20:45 RT @anandmahindra: The communication isn‚Äôt lost. Every single person in India can feel the heartbeat of #chandrayaan2 We can hear it whispe‚Ä¶\n",
      "2019-09-07 07:20:01 RT @RajivMessage: Chidambaram says his \"only worry is about the economy.\" Does he mean his family economy?\n",
      "https://t.co/R636ZVqwv1\n",
      "2019-09-05 14:41:02 Causing #superintelligence, one submission at a time #kaggle - https://t.co/iD1f96IuPJ\n",
      "2019-09-04 16:45:30 https://t.co/rDvkdiENxu\n",
      "\n",
      "#dalailama #forgiveness #wisdom\n",
      "2019-09-02 11:45:59 Meditation Perspective of a Practitioner https://t.co/0VtR7yIASO\n",
      "2019-09-01 14:37:12 Are you Atheist https://t.co/Ay8B0MjO8D\n",
      "2019-08-29 17:52:26 https://t.co/tXbVmyqT1R\n",
      "\n",
      "#workmanagement #happyworkplace #happiness\n",
      "2019-08-28 11:57:29 https://t.co/pq2mxFaok4\n",
      "\n",
      "#csrreport  #csr  #corporatesocialresponsibility\n",
      "2019-08-17 07:00:40 https://t.co/27iL3fIXt1\n",
      "\n",
      "#Secularism #hinduism\n",
      "2019-08-14 18:04:18 https://t.co/B7R5IXSH7h\n",
      "\n",
      "#humanity #morality #religion #technology\n",
      "2019-08-08 08:39:03 @SushmaSwaraj @narendramodi In the memory of my leader and inspiration. @SushmaSwaraj \n",
      "https://t.co/POu43VD9Ky\n",
      "2019-08-06 12:28:50 Principal Component Analysis explained visually https://t.co/AcyKKr9oKX via @setosaio\n",
      "2019-08-02 08:04:21 https://t.co/3gKA7g6sPz\n",
      "#bhagwatgita #divinequalities #psychology\n",
      "2019-08-01 09:28:51 https://t.co/5Dj5Ea8a9t #CCDFounder\n",
      "2019-06-21 07:12:12 RT @myogiadityanath: ‚Äú‡§® ‡§§‡§∏‡•ç‡§Ø ‡§∞‡•ã‡§ó‡•ã ‡§® ‡§ú‡§∞‡§æ ‡§® ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å‡§É ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§‡§∏‡•ç‡§Ø ‡§Ø‡•ã‡§ó‡§æ‡§ó‡•ç‡§∞‡§ø‡§Æ‡§Ø‡§Ç ‡§∂‡§∞‡•Ä‡§∞‡§Æ‡•ç‚Äù\n",
      "‡§Ö‡§∞‡•ç‡§•‡§æ‡§§‡•ç ‡§Ø‡•ã‡§ó‡§æ‡§≠‡•ç‡§Ø‡§æ‡§∏ ‡§∏‡•á ‡§§‡§™‡§æ ‡§π‡•Å‡§Ü ‡§∂‡§∞‡•Ä‡§∞ ‡§∞‡•ã‡§ó , ‡§ú‡§∞‡§æ ‡§è‡§µ‡§Ç ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å ‡§∏‡•á‚Ä¶\n",
      "2019-06-21 07:07:46 RT @Swamy39: Income Tax Appellate Tribunal today impose a fine of Rs 91 crores for a dubious criminal tax fraud by Prannoy Roy and wife. No‚Ä¶\n",
      "2019-06-21 06:58:40 Which hosting site is good and economic? Any idea please? I am considering to switch. At present using https://t.co/ngD1NGY5pL\n",
      "2019-06-02 17:26:24 Testing!\n",
      "2019-05-29 08:46:04 https://t.co/lS2XlQHNzJ\n",
      "2018-12-31 15:02:58 We suffer ONLY in present. But this suffering happens either because of repentance of past or fear of future. Past‚Ä¶ https://t.co/UhIx3iAfAG\n",
      "2018-01-23 09:28:45 2 Project Managers for an industrial engineering firm are urgently required at Chennai location. Please call me at‚Ä¶ https://t.co/lDDY4utKDR\n",
      "2017-06-01 17:07:34 Project Professional 2016 Reporting https://t.co/hvSEEpmiis https://t.co/pQhi0QXkDh\n",
      "2017-06-01 15:22:02 PowerBI &amp; Project Online\n",
      "https://t.co/FtGJlTEoHy https://t.co/8a1BKwMO2x\n",
      "2017-06-01 13:50:07 Power BI Reporting for Project Online https://t.co/tfvQmozwcM https://t.co/uq1KHNRB7p\n",
      "2017-06-01 13:39:58 How to connect to Project Online from PowerBI and generate beautiful project dashboards. Watching this presentation‚Ä¶https://t.co/XssvFOsXMf\n",
      "2017-06-01 10:35:57 Journey of Sanskrit in the West https://t.co/hlnDom8wuY\n",
      "2017-06-01 08:44:03 Topics of PMI-ACP Workshop https://t.co/HbaGOtkPpy https://t.co/EGnZwLct3F\n",
      "2017-06-01 02:29:02 A clear-cut different between champion and under-performing organization. #pmlogymetrics Source: PMI https://t.co/IqeulJGoKb\n",
      "2017-05-31 16:15:04 Is there any relationship between project success/fail and strategic alignment of the project? #pmlogymetrics Sourc‚Ä¶ https://t.co/my7e1D9YAn\n",
      "2017-05-31 15:16:27 Project Professional 2016 - reports https://t.co/MXzIkLMtB3\n",
      "2017-05-31 13:36:33 Nasadiya Sukta from RigVeda https://t.co/nZj5q31r6A\n",
      "2017-05-31 10:23:06 Primary Reasons of Project Failures? https://t.co/QI8ahXHiud\n",
      "2017-05-31 08:44:04 Do you know what Executives and PMO directors like? https://t.co/Bts5TDWONa\n",
      "2017-05-31 02:29:01 Projects are 2.5 times more successful when proven Project Management Practices are used. #pmlogymetrics Source: PMI https://t.co/cT0sR0juyC\n",
      "2017-05-30 16:15:19 This is really eye opener. Why it is overall negative of last 5 years! https://t.co/oCl28CUYuo\n",
      "2017-05-30 10:23:01 Organizations with aligned report 27 percent more projects completed successfully and 42 percent fewer projects with scope creep\n",
      "2017-05-30 08:44:00 Recognize the role of Enterprise-Wide-Project-Management (EPMO) in your organization. It helps in aligning IT to Strategy.\n",
      "2017-05-28 09:18:33 Switch off/on Windows desktop cons in Windows 10 https://t.co/3eGSPc5drm\n",
      "2017-05-28 09:16:35 Change desktop background in Windows 10 https://t.co/i1I9g9pWga\n",
      "2017-05-27 10:23:00 A significant uptick in demand for project talent, especially in rapidly developing economies - as China and India. https://t.co/Jtnlpvuq3S\n",
      "2017-05-27 08:44:01 Project Management Skill Gap and GDP at Risk \n",
      "https://t.co/N019cg42YW https://t.co/dPOjjIgoWR\n",
      "2017-05-27 02:29:01 Project Management Job Growth. https://t.co/C4tvndjGQt\n",
      "2017-05-26 16:15:14 Project Management Employment Outlook. https://t.co/YTgOC4D8EQ\n",
      "https://t.co/Iik1iHD4el https://t.co/NRsQLNNuow\n",
      "2017-05-26 11:30:28 Demand is high for those Project Management practitioners who have the necessary mix of competencies. https://t.co/0jduHpwEhy\n",
      "2017-05-23 09:06:57 App stores: number of apps in leading app stores 2017 | Statista https://t.co/2QeiZLdPiJ\n",
      "2017-05-23 07:56:00 Defining target(audience, platform, geography, industry), goal (performance, business, security) is the key in app development\n",
      "2017-05-23 07:53:05 Who is building that app which is fast, secure, which supports all platform/human languages/abilities people and useful for all users!\n",
      "2017-05-23 07:46:15 50% of organizations have fewer than five developers in-house, barely enough to field a single mobile team.- #Forrester\n",
      "2017-05-23 07:44:54 81% of global CEOs see mobile technologies as the most strategically important for their organization https://t.co/L6GsT9Rjiz\n",
      "2017-05-23 07:38:17 Apps in google app store 2.8mn, apple app store 2.2mn, windows .67mn, amazon .6mn, blackberry .23mn. It does not mean only many options! But\n",
      "2017-05-23 07:31:06 Have you heard of term \"Digital Darwinism\"? Absolutely relevant and useful term today.\n",
      "2017-05-23 07:29:16 Fourth Industrial Revolution. Cloud, data, intelligence, mobile, robotics, IoT etc\n",
      "fundamentally changing the way we work, live, and play\n",
      "2017-05-09 02:29:01 https://t.co/xCMG59ewzP Use canonical URLs - Search Console Help https://t.co/pTg0ROOyYL\n",
      "2017-05-08 16:15:09 https://t.co/4rWEVTWkYn Content Syndication: How to Get Started | Search Engine Watch https://t.co/Ap0VSYmFt6\n",
      "2017-05-07 16:15:09 Metrics: Concern of organizations about Agile Adoption! https://t.co/jZ5zP4ZR6x\n",
      "2017-05-07 10:23:17 Metrics: Agile-What should be my iteration length? https://t.co/wRejLwvqyc\n",
      "2017-05-07 08:44:20 Metrics: Leading Causes of Failed Agile Project https://t.co/zfffcIzlcl\n",
      "2017-05-07 02:29:02 Metrics: Bigdata - PMNetwork-2014-Jun https://t.co/hxnBlUH3oJ\n",
      "2017-05-06 17:14:46 Startups picking up in India but still very slow. https://t.co/l0PBhtlA71\n",
      "2017-05-03 08:44:05 44% of pm use no software, even though PWC found that the use of commercially available PM software increases performance and satisfaction.\n",
      "2017-05-03 02:29:04 Infosys planning to hire 10,000 Americans in US. Let us see how cost effective it is for them.https://t.co/oEtsuVCNSv\n",
      "2017-05-02 16:15:03 Railways to invite bids for manufacturing modern coaches https://t.co/NScjhc18o6 via @economictimes\n",
      "2017-05-02 11:46:50 RT @PiyushGoyal: Night-time map testifies the success of rural electrification &amp; implementation of clean energy projects in India.\n",
      "https://‚Ä¶\n",
      "2017-05-02 08:44:03 Shape up or ship out: BRO, other laggards warned https://t.co/XPWlvAxwbw via @economictimes\n",
      "2017-05-02 02:29:00 There is no independent world from d observer! Think for a second what is d meaning of all events in nature if there is no observer. #pmlogy\n",
      "2017-05-01 16:22:01 Project Procurement Management - PMLOGY https://t.co/B0b8HVDC5X https://t.co/g8ewMD9KHP\n",
      "2017-04-30 13:26:39 Yoga is Religious or Not? https://t.co/Mf4CVtxlTF https://t.co/na9EKaqvbi\n",
      "2017-04-30 12:26:55 Project Human Resource Management https://t.co/2IRIC8Gsvx https://t.co/12VbZy2uvC\n",
      "2017-04-30 06:30:14 Project Cost Management - PMLOGY https://t.co/SXakk4g1NG https://t.co/zR61KQwI1k\n",
      "2017-04-29 17:01:18 Project Communication Management https://t.co/JNdtybeW6M https://t.co/Ll2yWK1zk8\n",
      "2017-04-29 13:50:52 Project Integration Management https://t.co/1qvdNusYD2 https://t.co/VtvJqbqiQJ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may want to save the timeline ids over there, to not to lose them during execution\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "# Get the max_id that already  resides in db\n",
    "max_id_in_town = 1000 #some_max_id\n",
    "counter = 0\n",
    "timeline_ids = list()\n",
    "start_time = datetime.now()\n",
    "try:\n",
    "    for status in tweepy.Cursor(api.user_timeline, user_id=user_id, since_id=max_id_in_town).items():\n",
    "        # process status here\n",
    "        timeline_ids.append(status.id)\n",
    "        print (status.created_at, status.text)\n",
    "        end_time = datetime.now()\n",
    "        if np.ceil((end_time - start_time).seconds / 60) >= 12:\n",
    "            print(\"Worked for 12 mins, waiting for 15 mins now\")\n",
    "            print(datetime.now())\n",
    "            print(\"You may want to save the timeline ids over there, to not to lose them during execution\")\n",
    "            time.sleep(60 * 15)\n",
    "            start_time = datetime.now()\n",
    "            end_time = datetime.now()\n",
    "\n",
    "        counter += 1\n",
    "except tweepy.RateLimitError as e:\n",
    "    print(\"Rate limit error exceed waiting for 15 secs\")\n",
    "    print(datetime.now())\n",
    "    print(\"You may want to save the timeline ids over there, to not to lose them during execution\")\n",
    "    time.sleep(60 * 15)\n",
    "    start_time = datetime.now()\n",
    "    end_time = datetime.now()\n",
    "print(\"You may want to save the timeline ids over there, to not to lose them during execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set looping params\n",
    "start_time = datetime.now()\n",
    "raw_tweets = list()\n",
    "\n",
    "for id_ in timeline_ids:\n",
    "    try:\n",
    "        res = api.get_status(id=id_, tweet_mode=\"extended\")\n",
    "        raw_tweets.append({\"accident_id\": id_,\n",
    "                           \"created_at\": res.created_at,\n",
    "                           \"text\": res.full_text})\n",
    "        end_time = datetime.now()\n",
    "   \n",
    "        if np.ceil((end_time - start_time).seconds / 60) >= 12:\n",
    "            print(\"Worked for 12 mins, waiting for 15 mins now\")\n",
    "            print(datetime.now())\n",
    "            print(\"Save tweets somewhere\")\n",
    "            time.sleep(60 * 15)\n",
    "            start_time = datetime.now()\n",
    "            end_time = datetime.now()\n",
    "        del res\n",
    "\n",
    "    except tweepy.RateLimitError as e:\n",
    "        print(\"Rate limit error exceed waiting for 15 secs\")\n",
    "        print(datetime.now())\n",
    "        print(\"Save tweets somewhere\")\n",
    "\n",
    "        time.sleep(60 * 15)\n",
    "        start_time = datetime.now()\n",
    "        end_time = datetime.now()\n",
    "print(\"Save tweets somewhere\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
