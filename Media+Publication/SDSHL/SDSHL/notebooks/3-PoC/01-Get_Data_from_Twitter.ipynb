{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-3.9.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from tweepy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.9)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Installing collected packages: tweepy\n",
      "Successfully installed tweepy-3.9.0\n",
      "Collecting jsonpickle\n",
      "  Using cached jsonpickle-1.4.1-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: importlib-metadata in d:\\users\\admin\\anaconda3\\lib\\site-packages (from jsonpickle) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from importlib-metadata->jsonpickle) (3.1.0)\n",
      "Installing collected packages: jsonpickle\n",
      "Successfully installed jsonpickle-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install jsonpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import jsonpickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save files in this folder\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create convert json to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCSV_fromJson(csv_fname, txt_fname):\n",
    "    f = open(csv_fname,'a',encoding='utf8')\n",
    "    csvWriter = csv.writer(f)\n",
    "    headers=['full_text','retweet_count','user_followers_count','favorite_count','place','coordinates','geo','created_at','id_str']\n",
    "    #headers=['full_text']\n",
    "    csvWriter.writerow(headers)\n",
    "\n",
    "    for inputFile in [txt_fname]:#all the text-file names you want to convert to Csv in the sae folder as this code\n",
    "        tweets = []\n",
    "        for line in open(inputFile, 'r'):\n",
    "            tweets.append(json.loads(line))\n",
    "\n",
    "        \n",
    "        count_lines=0\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                csvWriter.writerow([tweet['full_text'],tweet['retweet_count'],tweet['user']['followers_count'],tweet['favorite_count'],tweet['place'],tweet['coordinates'],tweet['geo'],tweet['created_at'],str(tweet['id_str'])])\n",
    "                #csvWriter.writerow(tweet['full_text'])\n",
    "                count_lines+=1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        print(count_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Twitter using Tweepy api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that contains the credentials to access Twitter API\n",
    "ACCESS_TOKEN = '1683653820-p2Ehkyl2OQUmaY7Xf7H9pc4ErNvyvovBVwxy9GC'\n",
    "ACCESS_SECRET = 'TbtPT96BPIc31FK21b1iX9hcLfg4XOqiRZWrq5gAERoPc'\n",
    "\n",
    "#CONSUMER_KEY / CONSUMER_SECRET\n",
    "API_KEY = 'kUHB8y0fHK5WiGapp4CffpP2g'\n",
    "API_SECRET ='Gd614dFtCjGWwgN9M4t6jVT83q5kntkyDFDRmpZE8D7JfGBLI7'\n",
    "\n",
    "# Setup access to API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "    return api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create API object\n",
    "api = connect_to_twitter_OAuth()\n",
    "status = \"Testing!\"\n",
    "#api.update_status(status=status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweets from my stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CreateJsonFile_ofTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateJsonFile_ofTweets(fname, hashtag, max_tweets=1000000):\n",
    "    #auth = tweepy.AppAuthHandler(API_KEY,API_SECRET)\n",
    "    #api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    api = connect_to_twitter_OAuth()\n",
    "    #wait_on_rate_limit – Whether or not to automatically wait for rate limits to replenish\n",
    "    #wait_on_rate_limit_notify – Whether or not to print a notification when Tweepy is waiting for rate limits to replenish\n",
    "    # the sleep mode is automatically enabled with above 2 args\n",
    "\n",
    "    tweetsPerQuery = 100#this is the maximum provided by API\n",
    "    #max_tweets just for the sake of While loop\n",
    "\n",
    "    # No sinceId and max_id ..Get whathever you have exhaustively\n",
    "    since_id = None\n",
    "    max_id = -1\n",
    "    tweet_count = 0\n",
    "    print(\"Downloading the tweeets..takes some time..\")\n",
    "\n",
    "    search_query=hashtag \n",
    "    x=0\n",
    "    with open(fname,'w') as f:\n",
    "        print(\"Downloading hashtag\" + search_query)\n",
    "        while(tweet_count<max_tweets):\n",
    "            try:\n",
    "                if(max_id<=0):\n",
    "                    if(not since_id):\n",
    "                        new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"hi\",\n",
    "                                                tweet_mode='extended', include_rts=False)\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"hi\",\n",
    "                                                tweet_mode='extended',since_id=since_id, include_rts=False)\n",
    "                else:\n",
    "                    if(not since_id):\n",
    "                        new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"hi\",\n",
    "                                                tweet_mode='extended',max_id=str(max_id-1), include_rts=False)\n",
    "                    else:\n",
    "                        new_tweets = api.search(q=search_query,count=tweetsPerQuery,lang=\"hi\",tweet_mode='extended',\n",
    "                                                max_id=str(max_id-1),since_id=since_id, include_rts=False)\n",
    "                \n",
    "                # Tweets Exhausted\n",
    "                if(not new_tweets):\n",
    "                    print(\"No more tweets found!!\")\n",
    "                    break\n",
    "                    \n",
    "                # write all the new_tweets to a json file\n",
    "                for tweet in new_tweets:\n",
    "                    f.write(jsonpickle.encode(tweet._json,unpicklable=False)+'\\n')\n",
    "                    tweet_count+=len(new_tweets)\n",
    "                    #print(\"Successfully downloaded {0} tweets\".format(tweet_count))\n",
    "                    max_id=new_tweets[-1].id\n",
    "                    \n",
    "            # in case of any error\n",
    "            except tweepy.TweepError as e:\n",
    "                    print(\"Some error!!:\"+str(e))\n",
    "                    break\n",
    "    return tweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweets from a specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = api.user_timeline(account, count=20000, lang=\"hi\", tweet_mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "199\n",
      "200\n",
      "200\n",
      "200\n",
      "197\n",
      "200\n",
      "198\n",
      "197\n",
      "200\n",
      "200\n",
      "192\n",
      "111\n",
      "200\n",
      "162\n",
      "197\n",
      "191\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "199\n",
      "197\n",
      "200\n",
      "198\n",
      "200\n",
      "200\n",
      "198\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "199\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "199\n",
      "Saving file D:\\18-DS\\github\\SDSHL\\data\\external\\\\ridhi_bose.csv\n"
     ]
    }
   ],
   "source": [
    "#disabled accounts = 'Real_Netan', 'sandeep353055' yatisharma111, Arnab5222, 'PbSwain_IND', DeshbhaktRosha1, ShrishtySharma_\n",
    "\n",
    "accounts = ['acs_rkpandey','Bittu_Tufani','Gulmoha99443364','maliksarah5gma1','mjaved819',\n",
    "            'Rajeshkm_RJS','pallavinaharre1','raviag07','samratkikavitae','Sourav1717',\n",
    "            'Sujitkumar_mau','theBookwalla','gareeb_bhartiya','AkshayM90903668','vptvns',\n",
    "            'Vyang_Kumar','HindiSatire','Shanu44339200','SwetaSinghAT','VinodRajotiya82',\n",
    "            'gautam007','Republic_Bharat','VickyAarya007','KapilMishra_IND','VyangyaVahini',\n",
    "            'DChaurasia2312','WiskyWala','ManojTiwariMP', 'chitraaum','RealPushpendra',\n",
    "            'RajatSharmaLive','RubikaLiyaquat','TheAbhishek_IND','Real_Sweta','SushantBSinha',\n",
    "            'VickyAarya007','badri_dk','Abhasin89009555','BhartiyRudr','Jainritesh_rj','TeriJogan','ridhi_bose']\n",
    "\n",
    "\n",
    "dfs=[]\n",
    "for account in accounts:\n",
    "    tweets = api.user_timeline(account, count=20000, lang=\"hi\", tweet_mode=\"extended\")\n",
    "    tweets = [[account,tweet.full_text] for tweet in tweets]\n",
    "    #tweets = (tweet for tweet in tweets)\n",
    "    df = pd.DataFrame( tweets , columns=['account',\"Text\"])\n",
    "    dfs.append(df)\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    print (len(dfs[i]))\n",
    "    if i==0:\n",
    "        df=dfs[0]\n",
    "        i=1\n",
    "    else:\n",
    "        df=pd.concat( [df, dfs[i] ], ignore_index=True)\n",
    "        i+=1\n",
    "    \n",
    "df.to_csv(datafolder_e + r'\\\\' + account+\".csv\")\n",
    "print (\"Saving file \"+ datafolder_e + r'\\twitter_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweets from a specific hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the tweeets..takes some time..\n",
      "Downloading hashtag#कटाक्ष\n",
      "100\n",
      "A total of 10000 tweets are downloaded and saved to कटाक्ष.txt\n",
      "Total time taken is  2.0661940574645996 seconds.\n",
      "_कटाक्ष.csv file created\n",
      "=====================>\n",
      "Downloading the tweeets..takes some time..\n",
      "Downloading hashtag##व्यंग\n",
      "60\n",
      "A total of 3600 tweets are downloaded and saved to #व्यंग.txt\n",
      "Total time taken is  4.286127328872681 seconds.\n",
      "_#व्यंग.csv file created\n",
      "=====================>\n",
      "Downloading the tweeets..takes some time..\n",
      "Downloading hashtag#व्यंग्य\n",
      "100\n",
      "A total of 10000 tweets are downloaded and saved to व्यंग्य.txt\n",
      "Total time taken is  4.182105779647827 seconds.\n",
      "_व्यंग्य.csv file created\n",
      "=====================>\n",
      "Downloading the tweeets..takes some time..\n",
      "Downloading hashtag#मोदी_फिरकी_ले_रहा_है\n",
      "No more tweets found!!\n",
      "0\n",
      "A total of 0 tweets are downloaded and saved to मोदी_फिरकी_ले_रहा_है.txt\n",
      "Total time taken is  1.3484437465667725 seconds.\n",
      "_मोदी_फिरकी_ले_रहा_है.csv file created\n",
      "=====================>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hashtags= [\"कटाक्ष\", \"#व्यंग\", \"व्यंग्य\",\"मोदी_फिरकी_ले_रहा_है\"]\n",
    "#hashtags= [\"मोदी_फिरकी_ले_रहा_है\"]\n",
    "\n",
    "for hashtag in hashtags:\n",
    "    st = time.time()\n",
    "    fname= hashtag+\".txt\"\n",
    "    tweet_count = CreateJsonFile_ofTweets(fname, \"#\"+hashtag, 1000)\n",
    "    createCSV_fromJson(hashtag+'.csv', hashtag+'.txt')\n",
    "    end = time.time()\n",
    "    print(\"A total of {0} tweets are downloaded and saved to {1}\".format(tweet_count,fname))\n",
    "    print(\"Total time taken is \",end-st,\"seconds.\")\n",
    "    #df = pd.read_csv(hashtag+'.csv',  usecols=['created_at','full_text'], encoding = 'utf8')\n",
    "    df = pd.read_csv(hashtag+'.csv',  usecols=['full_text'], encoding = 'utf8')\n",
    "    \n",
    "    #df= df.rename(columns={'created_at':'Time','full_text':'Text'})\n",
    "    df= df.rename(columns={'full_text':'Text'})\n",
    "    #df=df[ ['Time','Text'] ]\n",
    "    df=df[ ['Text'] ]\n",
    "    df.to_csv( datafolder_e + r'\\\\' + \"_\"+hashtag+'.csv')\n",
    "    print(datafolder_e + r\"\\_\"+ hashtag+'.csv'+ \" file created\")\n",
    "    print (\"=====================>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @saurv365: भैंस - रोजगार, कोविड महामारी, अर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>भैंस - रोजगार, कोविड महामारी, अर्थव्यवस्था, कि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @0007tilak: आज वो भी #DaughtersDay2020 की श...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @AjjuBhaiyaa: मेरी अक्सर लोगों से बनती नहीं...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>मेरी अक्सर लोगों से बनती नहीं \\r\\nमें कितना भी...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>आज वो भी #DaughtersDay2020 की शुभकामनाओं सहित ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>बड़ी खबर- दीपिका ने रणवीर से बोला आते समय नीचे...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>RT @AsifRazakhan_: #कटाक्ष\\r\\n500-500 सौ रूपये...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>300</td>\n",
       "      <td>#कटाक्ष\\r\\n500-500 सौ रूपये\\r\\nऔर एक प्लेट बिर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>301</td>\n",
       "      <td>RT @aditiraval: My mom is watching #Anupamaa o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               Text\n",
       "0             0  RT @saurv365: भैंस - रोजगार, कोविड महामारी, अर...\n",
       "1             1  भैंस - रोजगार, कोविड महामारी, अर्थव्यवस्था, कि...\n",
       "2             2  RT @0007tilak: आज वो भी #DaughtersDay2020 की श...\n",
       "3             3  RT @AjjuBhaiyaa: मेरी अक्सर लोगों से बनती नहीं...\n",
       "4             4  मेरी अक्सर लोगों से बनती नहीं \\r\\nमें कितना भी...\n",
       "..          ...                                                ...\n",
       "297         297  आज वो भी #DaughtersDay2020 की शुभकामनाओं सहित ...\n",
       "298         298  बड़ी खबर- दीपिका ने रणवीर से बोला आते समय नीचे...\n",
       "299         299  RT @AsifRazakhan_: #कटाक्ष\\r\\n500-500 सौ रूपये...\n",
       "300         300  #कटाक्ष\\r\\n500-500 सौ रूपये\\r\\nऔर एक प्लेट बिर...\n",
       "301         301  RT @aditiraval: My mom is watching #Anupamaa o...\n",
       "\n",
       "[302 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(datafolder_e + r'\\\\' + \"_\"+hashtags[0]+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_csv(hashtags[0]+'.csv',  encoding = 'utf8')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all @ read text from Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Text, dtype: object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ManojTiwariMP'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>account</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>रागा- पता है मेरे जन्मदिन पे\\nकौन पैदा हुआ था ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>जिस तरह राम रहीम के 36 अश्रम\\nसील है उसी तरह म...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>@mechitraaa @Abhasin89009555 ye to kuchh nhi h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>सुशांत सिंह राजपूत\\nआत्महत्या मामले की सुप्रीम...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>VickyAarya007</td>\n",
       "      <td>RT @Abhasin89009555: इन तस्वीरों को देखकर समझि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @RajTiwa56444261: मित्रो आज हमारे विवाह के ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @its_mahii01: मै और मेरी तन्हाई अक्सर\\n।\\n।...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @universe__guru: देश बचा लो साहब ,\\nमोहब्बत...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @Chatterj1Asking: लगता है 1962 का \\nहिसाब ब...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "      <td>RT @shuklaji250: सुप्रभात, हर हर महादेव..!!🚩🙏🏻...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0          account  \\\n",
       "0              0    VickyAarya007   \n",
       "1              1    VickyAarya007   \n",
       "2              2    VickyAarya007   \n",
       "3              3    VickyAarya007   \n",
       "4              4    VickyAarya007   \n",
       "...          ...              ...   \n",
       "1995        1995  ShrishtySharma_   \n",
       "1996        1996  ShrishtySharma_   \n",
       "1997        1997  ShrishtySharma_   \n",
       "1998        1998  ShrishtySharma_   \n",
       "1999        1999  ShrishtySharma_   \n",
       "\n",
       "                                                   Text  \n",
       "0     रागा- पता है मेरे जन्मदिन पे\\nकौन पैदा हुआ था ...  \n",
       "1     जिस तरह राम रहीम के 36 अश्रम\\nसील है उसी तरह म...  \n",
       "2     @mechitraaa @Abhasin89009555 ye to kuchh nhi h...  \n",
       "3     सुशांत सिंह राजपूत\\nआत्महत्या मामले की सुप्रीम...  \n",
       "4     RT @Abhasin89009555: इन तस्वीरों को देखकर समझि...  \n",
       "...                                                 ...  \n",
       "1995  RT @RajTiwa56444261: मित्रो आज हमारे विवाह के ...  \n",
       "1996  RT @its_mahii01: मै और मेरी तन्हाई अक्सर\\n।\\n।...  \n",
       "1997  RT @universe__guru: देश बचा लो साहब ,\\nमोहब्बत...  \n",
       "1998  RT @Chatterj1Asking: लगता है 1962 का \\nहिसाब ब...  \n",
       "1999  RT @shuklaji250: सुप्रभात, हर हर महादेव..!!🚩🙏🏻...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getclean_text(x):\n",
    "    text_list= x.split(\" \")\n",
    "    clean_text = []\n",
    "    #don't include account\n",
    "    pattern = \"^[^@]\"\n",
    "    for token in text_list:\n",
    "        if re.search(pattern, token):\n",
    "            clean_text.append(token)\n",
    "    \n",
    "    #don't include http links\n",
    "    clean_text1 = []\n",
    "    pattern = \"^[^http]*[^https]\"\n",
    "    for token in clean_text:\n",
    "        if re.search(pattern, token):\n",
    "            clean_text1.append(token)\n",
    "    \n",
    "    #print (clean_text1)\n",
    "    clean_text1=\" \".join(clean_text1)\n",
    "    \n",
    "    clean_text1= clean_text1.replace(\"\\n\",\" \")\n",
    "    \n",
    "    return clean_text1\n",
    "\n",
    "df1 = df[ ['account','Text'] ]\n",
    "df1= pd.DataFrame(df1['Text'].apply(lambda x: getclean_text(x)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['account']=df.account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>account</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>रागा- पता है मेरे जन्मदिन पे कौन पैदा हुआ था ?...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>जिस तरह राम रहीम के 36 अश्रम सील है उसी तरह मौ...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ye kuchh nhi me Jane ke liye isse aage bhi bah...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>सुशांत सिंह राजपूत आत्महत्या मामले की सुप्रीम ...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT इन तस्वीरों को देखकर समझिए क्यों जरूरत पड़ी...</td>\n",
       "      <td>VickyAarya007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>RT मित्रो आज हमारे विवाह के 18 वर्ष पूरे हुए.....</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>RT मै और मेरी तन्हाई अक्सर । । । ।  चाइना वालो...</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>RT देश बचा लो साहब , मोहब्बत तो कभी मिलने से र...</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>RT लगता है 1962 का  हिसाब बराबर करना भी मोदीजी...</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>RT सुप्रभात, हर हर महादेव..!!🚩🙏🏻</td>\n",
       "      <td>ShrishtySharma_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text          account\n",
       "0     रागा- पता है मेरे जन्मदिन पे कौन पैदा हुआ था ?...    VickyAarya007\n",
       "1     जिस तरह राम रहीम के 36 अश्रम सील है उसी तरह मौ...    VickyAarya007\n",
       "2     ye kuchh nhi me Jane ke liye isse aage bhi bah...    VickyAarya007\n",
       "3     सुशांत सिंह राजपूत आत्महत्या मामले की सुप्रीम ...    VickyAarya007\n",
       "4     RT इन तस्वीरों को देखकर समझिए क्यों जरूरत पड़ी...    VickyAarya007\n",
       "...                                                 ...              ...\n",
       "1995  RT मित्रो आज हमारे विवाह के 18 वर्ष पूरे हुए.....  ShrishtySharma_\n",
       "1996  RT मै और मेरी तन्हाई अक्सर । । । ।  चाइना वालो...  ShrishtySharma_\n",
       "1997  RT देश बचा लो साहब , मोहब्बत तो कभी मिलने से र...  ShrishtySharma_\n",
       "1998  RT लगता है 1962 का  हिसाब बराबर करना भी मोदीजी...  ShrishtySharma_\n",
       "1999                   RT सुप्रभात, हर हर महादेव..!!🚩🙏🏻  ShrishtySharma_\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[ ['account','Text'] ].to_csv( datafolder_e + r'\\\\' + \"_\"+accounts[0]+\".csv\", index=False )\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may want to save the timeline ids over there, to not to lose them during execution\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "# Fetch tweet ids from a timeline \n",
    "# Use these block if this is there is no saved tweets from \n",
    "# this account or you dont plan to use since_id functionality \n",
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "counter = 0\n",
    "timeline_ids = list()\n",
    "start_time = datetime.now()\n",
    "\n",
    "user_id='Shanu44339200'\n",
    "\n",
    "try:\n",
    "    for status in tweepy.Cursor(api.user_timeline, user_id=user_id).items():\n",
    "        # process status here\n",
    "        timeline_ids.append(status.id)\n",
    "        end_time = datetime.now()\n",
    "        if np.ceil((end_time - start_time).seconds / 60) >= 12:\n",
    "            print(\"Worked for 12 minutes, waiting for 15 minutes now\")\n",
    "            print(datetime.now())\n",
    "            time.sleep(60 * 15)\n",
    "            start_time = datetime.now()\n",
    "            end_time = datetime.now()\n",
    "\n",
    "        counter += 1\n",
    "except tweepy.RateLimitError as e:\n",
    "    print(\"Rate limit error exceed waiting for 15 secs\")\n",
    "    print(datetime.now())\n",
    "    print(\"You may want to save the timeline ids over there, to not to lose them during execution\")\n",
    "    time.sleep(60 * 15)\n",
    "    start_time = datetime.now()\n",
    "    end_time = datetime.now()\n",
    "\n",
    "print(\"You may want to save the timeline ids over there, to not to lose them during execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 07:52:45 Testing!\n",
      "2020-05-23 07:41:16 Testing!\n",
      "2020-05-22 13:26:20 Testing!\n",
      "2019-09-07 18:09:31 RT @PMBhutan: We are proud of India and its scientists today. Chandrayaan-2 saw some challenges last minute but the courage and hard work y…\n",
      "2019-09-07 18:09:23 https://t.co/zIUnVdwrXu\n",
      "\n",
      "#nonviolence #foodchain #survival\n",
      "2019-09-07 07:26:16 @anandmahindra A true nationalist, patriotic, ethical business professional. India will always remember the way you… https://t.co/RoL5t0aVVs\n",
      "2019-09-07 07:25:55 @anandmahindra every time I read your tweets. You occupy more space in my my mind and heart. Along with business, you are leader of masses.\n",
      "2019-09-07 07:20:45 RT @anandmahindra: The communication isn’t lost. Every single person in India can feel the heartbeat of #chandrayaan2 We can hear it whispe…\n",
      "2019-09-07 07:20:01 RT @RajivMessage: Chidambaram says his \"only worry is about the economy.\" Does he mean his family economy?\n",
      "https://t.co/R636ZVqwv1\n",
      "2019-09-05 14:41:02 Causing #superintelligence, one submission at a time #kaggle - https://t.co/iD1f96IuPJ\n",
      "2019-09-04 16:45:30 https://t.co/rDvkdiENxu\n",
      "\n",
      "#dalailama #forgiveness #wisdom\n",
      "2019-09-02 11:45:59 Meditation Perspective of a Practitioner https://t.co/0VtR7yIASO\n",
      "2019-09-01 14:37:12 Are you Atheist https://t.co/Ay8B0MjO8D\n",
      "2019-08-29 17:52:26 https://t.co/tXbVmyqT1R\n",
      "\n",
      "#workmanagement #happyworkplace #happiness\n",
      "2019-08-28 11:57:29 https://t.co/pq2mxFaok4\n",
      "\n",
      "#csrreport  #csr  #corporatesocialresponsibility\n",
      "2019-08-17 07:00:40 https://t.co/27iL3fIXt1\n",
      "\n",
      "#Secularism #hinduism\n",
      "2019-08-14 18:04:18 https://t.co/B7R5IXSH7h\n",
      "\n",
      "#humanity #morality #religion #technology\n",
      "2019-08-08 08:39:03 @SushmaSwaraj @narendramodi In the memory of my leader and inspiration. @SushmaSwaraj \n",
      "https://t.co/POu43VD9Ky\n",
      "2019-08-06 12:28:50 Principal Component Analysis explained visually https://t.co/AcyKKr9oKX via @setosaio\n",
      "2019-08-02 08:04:21 https://t.co/3gKA7g6sPz\n",
      "#bhagwatgita #divinequalities #psychology\n",
      "2019-08-01 09:28:51 https://t.co/5Dj5Ea8a9t #CCDFounder\n",
      "2019-06-21 07:12:12 RT @myogiadityanath: “न तस्य रोगो न जरा न मृत्युः प्राप्तस्य योगाग्रिमयं शरीरम्”\n",
      "अर्थात् योगाभ्यास से तपा हुआ शरीर रोग , जरा एवं मृत्यु से…\n",
      "2019-06-21 07:07:46 RT @Swamy39: Income Tax Appellate Tribunal today impose a fine of Rs 91 crores for a dubious criminal tax fraud by Prannoy Roy and wife. No…\n",
      "2019-06-21 06:58:40 Which hosting site is good and economic? Any idea please? I am considering to switch. At present using https://t.co/ngD1NGY5pL\n",
      "2019-06-02 17:26:24 Testing!\n",
      "2019-05-29 08:46:04 https://t.co/lS2XlQHNzJ\n",
      "2018-12-31 15:02:58 We suffer ONLY in present. But this suffering happens either because of repentance of past or fear of future. Past… https://t.co/UhIx3iAfAG\n",
      "2018-01-23 09:28:45 2 Project Managers for an industrial engineering firm are urgently required at Chennai location. Please call me at… https://t.co/lDDY4utKDR\n",
      "2017-06-01 17:07:34 Project Professional 2016 Reporting https://t.co/hvSEEpmiis https://t.co/pQhi0QXkDh\n",
      "2017-06-01 15:22:02 PowerBI &amp; Project Online\n",
      "https://t.co/FtGJlTEoHy https://t.co/8a1BKwMO2x\n",
      "2017-06-01 13:50:07 Power BI Reporting for Project Online https://t.co/tfvQmozwcM https://t.co/uq1KHNRB7p\n",
      "2017-06-01 13:39:58 How to connect to Project Online from PowerBI and generate beautiful project dashboards. Watching this presentation…https://t.co/XssvFOsXMf\n",
      "2017-06-01 10:35:57 Journey of Sanskrit in the West https://t.co/hlnDom8wuY\n",
      "2017-06-01 08:44:03 Topics of PMI-ACP Workshop https://t.co/HbaGOtkPpy https://t.co/EGnZwLct3F\n",
      "2017-06-01 02:29:02 A clear-cut different between champion and under-performing organization. #pmlogymetrics Source: PMI https://t.co/IqeulJGoKb\n",
      "2017-05-31 16:15:04 Is there any relationship between project success/fail and strategic alignment of the project? #pmlogymetrics Sourc… https://t.co/my7e1D9YAn\n",
      "2017-05-31 15:16:27 Project Professional 2016 - reports https://t.co/MXzIkLMtB3\n",
      "2017-05-31 13:36:33 Nasadiya Sukta from RigVeda https://t.co/nZj5q31r6A\n",
      "2017-05-31 10:23:06 Primary Reasons of Project Failures? https://t.co/QI8ahXHiud\n",
      "2017-05-31 08:44:04 Do you know what Executives and PMO directors like? https://t.co/Bts5TDWONa\n",
      "2017-05-31 02:29:01 Projects are 2.5 times more successful when proven Project Management Practices are used. #pmlogymetrics Source: PMI https://t.co/cT0sR0juyC\n",
      "2017-05-30 16:15:19 This is really eye opener. Why it is overall negative of last 5 years! https://t.co/oCl28CUYuo\n",
      "2017-05-30 10:23:01 Organizations with aligned report 27 percent more projects completed successfully and 42 percent fewer projects with scope creep\n",
      "2017-05-30 08:44:00 Recognize the role of Enterprise-Wide-Project-Management (EPMO) in your organization. It helps in aligning IT to Strategy.\n",
      "2017-05-28 09:18:33 Switch off/on Windows desktop cons in Windows 10 https://t.co/3eGSPc5drm\n",
      "2017-05-28 09:16:35 Change desktop background in Windows 10 https://t.co/i1I9g9pWga\n",
      "2017-05-27 10:23:00 A significant uptick in demand for project talent, especially in rapidly developing economies - as China and India. https://t.co/Jtnlpvuq3S\n",
      "2017-05-27 08:44:01 Project Management Skill Gap and GDP at Risk \n",
      "https://t.co/N019cg42YW https://t.co/dPOjjIgoWR\n",
      "2017-05-27 02:29:01 Project Management Job Growth. https://t.co/C4tvndjGQt\n",
      "2017-05-26 16:15:14 Project Management Employment Outlook. https://t.co/YTgOC4D8EQ\n",
      "https://t.co/Iik1iHD4el https://t.co/NRsQLNNuow\n",
      "2017-05-26 11:30:28 Demand is high for those Project Management practitioners who have the necessary mix of competencies. https://t.co/0jduHpwEhy\n",
      "2017-05-23 09:06:57 App stores: number of apps in leading app stores 2017 | Statista https://t.co/2QeiZLdPiJ\n",
      "2017-05-23 07:56:00 Defining target(audience, platform, geography, industry), goal (performance, business, security) is the key in app development\n",
      "2017-05-23 07:53:05 Who is building that app which is fast, secure, which supports all platform/human languages/abilities people and useful for all users!\n",
      "2017-05-23 07:46:15 50% of organizations have fewer than five developers in-house, barely enough to field a single mobile team.- #Forrester\n",
      "2017-05-23 07:44:54 81% of global CEOs see mobile technologies as the most strategically important for their organization https://t.co/L6GsT9Rjiz\n",
      "2017-05-23 07:38:17 Apps in google app store 2.8mn, apple app store 2.2mn, windows .67mn, amazon .6mn, blackberry .23mn. It does not mean only many options! But\n",
      "2017-05-23 07:31:06 Have you heard of term \"Digital Darwinism\"? Absolutely relevant and useful term today.\n",
      "2017-05-23 07:29:16 Fourth Industrial Revolution. Cloud, data, intelligence, mobile, robotics, IoT etc\n",
      "fundamentally changing the way we work, live, and play\n",
      "2017-05-09 02:29:01 https://t.co/xCMG59ewzP Use canonical URLs - Search Console Help https://t.co/pTg0ROOyYL\n",
      "2017-05-08 16:15:09 https://t.co/4rWEVTWkYn Content Syndication: How to Get Started | Search Engine Watch https://t.co/Ap0VSYmFt6\n",
      "2017-05-07 16:15:09 Metrics: Concern of organizations about Agile Adoption! https://t.co/jZ5zP4ZR6x\n",
      "2017-05-07 10:23:17 Metrics: Agile-What should be my iteration length? https://t.co/wRejLwvqyc\n",
      "2017-05-07 08:44:20 Metrics: Leading Causes of Failed Agile Project https://t.co/zfffcIzlcl\n",
      "2017-05-07 02:29:02 Metrics: Bigdata - PMNetwork-2014-Jun https://t.co/hxnBlUH3oJ\n",
      "2017-05-06 17:14:46 Startups picking up in India but still very slow. https://t.co/l0PBhtlA71\n",
      "2017-05-03 08:44:05 44% of pm use no software, even though PWC found that the use of commercially available PM software increases performance and satisfaction.\n",
      "2017-05-03 02:29:04 Infosys planning to hire 10,000 Americans in US. Let us see how cost effective it is for them.https://t.co/oEtsuVCNSv\n",
      "2017-05-02 16:15:03 Railways to invite bids for manufacturing modern coaches https://t.co/NScjhc18o6 via @economictimes\n",
      "2017-05-02 11:46:50 RT @PiyushGoyal: Night-time map testifies the success of rural electrification &amp; implementation of clean energy projects in India.\n",
      "https://…\n",
      "2017-05-02 08:44:03 Shape up or ship out: BRO, other laggards warned https://t.co/XPWlvAxwbw via @economictimes\n",
      "2017-05-02 02:29:00 There is no independent world from d observer! Think for a second what is d meaning of all events in nature if there is no observer. #pmlogy\n",
      "2017-05-01 16:22:01 Project Procurement Management - PMLOGY https://t.co/B0b8HVDC5X https://t.co/g8ewMD9KHP\n",
      "2017-04-30 13:26:39 Yoga is Religious or Not? https://t.co/Mf4CVtxlTF https://t.co/na9EKaqvbi\n",
      "2017-04-30 12:26:55 Project Human Resource Management https://t.co/2IRIC8Gsvx https://t.co/12VbZy2uvC\n",
      "2017-04-30 06:30:14 Project Cost Management - PMLOGY https://t.co/SXakk4g1NG https://t.co/zR61KQwI1k\n",
      "2017-04-29 17:01:18 Project Communication Management https://t.co/JNdtybeW6M https://t.co/Ll2yWK1zk8\n",
      "2017-04-29 13:50:52 Project Integration Management https://t.co/1qvdNusYD2 https://t.co/VtvJqbqiQJ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may want to save the timeline ids over there, to not to lose them during execution\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "# Get the max_id that already  resides in db\n",
    "max_id_in_town = 1000 #some_max_id\n",
    "counter = 0\n",
    "timeline_ids = list()\n",
    "start_time = datetime.now()\n",
    "try:\n",
    "    for status in tweepy.Cursor(api.user_timeline, user_id=user_id, since_id=max_id_in_town).items():\n",
    "        # process status here\n",
    "        timeline_ids.append(status.id)\n",
    "        print (status.created_at, status.text)\n",
    "        end_time = datetime.now()\n",
    "        if np.ceil((end_time - start_time).seconds / 60) >= 12:\n",
    "            print(\"Worked for 12 mins, waiting for 15 mins now\")\n",
    "            print(datetime.now())\n",
    "            print(\"You may want to save the timeline ids over there, to not to lose them during execution\")\n",
    "            time.sleep(60 * 15)\n",
    "            start_time = datetime.now()\n",
    "            end_time = datetime.now()\n",
    "\n",
    "        counter += 1\n",
    "except tweepy.RateLimitError as e:\n",
    "    print(\"Rate limit error exceed waiting for 15 secs\")\n",
    "    print(datetime.now())\n",
    "    print(\"You may want to save the timeline ids over there, to not to lose them during execution\")\n",
    "    time.sleep(60 * 15)\n",
    "    start_time = datetime.now()\n",
    "    end_time = datetime.now()\n",
    "print(\"You may want to save the timeline ids over there, to not to lose them during execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set looping params\n",
    "start_time = datetime.now()\n",
    "raw_tweets = list()\n",
    "\n",
    "for id_ in timeline_ids:\n",
    "    try:\n",
    "        res = api.get_status(id=id_, tweet_mode=\"extended\")\n",
    "        raw_tweets.append({\"accident_id\": id_,\n",
    "                           \"created_at\": res.created_at,\n",
    "                           \"text\": res.full_text})\n",
    "        end_time = datetime.now()\n",
    "   \n",
    "        if np.ceil((end_time - start_time).seconds / 60) >= 12:\n",
    "            print(\"Worked for 12 mins, waiting for 15 mins now\")\n",
    "            print(datetime.now())\n",
    "            print(\"Save tweets somewhere\")\n",
    "            time.sleep(60 * 15)\n",
    "            start_time = datetime.now()\n",
    "            end_time = datetime.now()\n",
    "        del res\n",
    "\n",
    "    except tweepy.RateLimitError as e:\n",
    "        print(\"Rate limit error exceed waiting for 15 secs\")\n",
    "        print(datetime.now())\n",
    "        print(\"Save tweets somewhere\")\n",
    "\n",
    "        time.sleep(60 * 15)\n",
    "        start_time = datetime.now()\n",
    "        end_time = datetime.now()\n",
    "print(\"Save tweets somewhere\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
