{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models\\POS-Tagger-Hindi\\hi_hdtb_models'\n",
    "\n",
    "# filepath_train =datafolder_p+ r'\\2-train.csv'\n",
    "# filepath_test  =datafolder_p+ r'\\2-test.csv'\n",
    "\n",
    "filepath_data = datafolder_p + r'\\2-Hinglish_Sarcasm_Clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stanfordnlp\n",
    "import stanfordnlp\n",
    "#stanfordnlp.download('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(filepath_data, sep=\"\\t\", index_col=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df['sentence_wo_emo'])\n",
    "#use sentence_wo_emo to perform pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\admin\\\\stanfordnlp_resources\\\\hi_hdtb_models\\\\hi_hdtb_tokenizer.pt', 'pretokenized': True, 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'D:\\\\18-DS\\\\data\\\\models\\\\POS-Tagger-Hindi\\\\hi_hdtb_models\\\\hi_hdtb_tagger.pt', 'pretrain_path': 'D:\\\\18-DS\\\\data\\\\models\\\\POS-Tagger-Hindi\\\\hi_hdtb_models\\\\hi_hdtb.pretrain.pt', 'batch_size': 1000, 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "tag = modelfolder+ r'\\hi_hdtb_tagger.pt'\n",
    "pret = modelfolder+ r'\\hi_hdtb.pretrain.pt'\n",
    "\n",
    "config = {\n",
    "        'processors': 'tokenize,pos',\n",
    "        'tokenize_pretokenized': True,\n",
    "        'pos_model_path': tag,\n",
    "        'pos_pretrain_path': pret,\n",
    "        'pos_batch_size': 1000,\n",
    "        'lang':'hi'\n",
    "         }\n",
    "nlp = stanfordnlp.Pipeline(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create POS for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step is TIME Consuming\n",
    "\n",
    "doc_pos=[]\n",
    "for i in range( len( docs ) ):\n",
    "    doc_nlp = nlp( docs[i] ) #get nlp of ith sentence\n",
    "    no_of_words = len(doc_nlp.conll_file.sents[0])\n",
    "    pos_list =[]\n",
    "    for j in range( no_of_words ):\n",
    "        pos = doc_nlp.conll_file.sents[0][j][3] # 3rd is POS name\n",
    "        pos_list.append(pos)\n",
    "    pos_list = \", \".join(pos_list)\n",
    "    doc_pos.append(pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a dataset of pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.DataFrame(doc_pos, columns=['pos'], index=df.index)\n",
    "df_pos['sentence_wo_emo']=list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unique pos in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'ADP', 'VERB', 'AUX', 'PRON', 'PROPN', 'PART', 'DET', 'ADJ', 'SCONJ', 'PUNCT', 'CCONJ', 'NUM', 'ADV', 'INTJ', 'X']\n"
     ]
    }
   ],
   "source": [
    "pos_types=[]\n",
    "pos_sents= df_pos.pos.str.replace(\" \",\"\").str.split(\",\")\n",
    "for pos_sent in pos_sents:  \n",
    "    for pos_word in pos_sent:\n",
    "        if pos_word not in pos_types:\n",
    "            pos_types.append (pos_word)\n",
    "\n",
    "print((pos_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get frequencies of pos from words in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rows = len(df_pos)\n",
    "cols = len(pos_types) + 1 # one more colmn for number of words in sentence\n",
    "pos_vec= np.zeros((rows, cols), dtype=int)\n",
    "\n",
    "i=0\n",
    "for i in range(len(df_pos)):\n",
    "    sent    = df_pos.iloc[i,]['sentence_wo_emo']\n",
    "    pos_list= df_pos.iloc[i,]['pos'].replace(\" \",\"\").split(\",\")\n",
    "    for j in range(len(pos_types)):\n",
    "        pos1=pos_types[j]\n",
    "        find_pos= [pos1]*len(pos_list)       \n",
    "        pos_vec[i][j]= sum(np.array(pos_list)==np.array(find_pos))\n",
    "    \n",
    "    pos_vec[i][j+1]=len(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pos frequencies in a dataset\n",
    "col_names = pos_types.copy()\n",
    "col_names.append('words')\n",
    "df_pos1 = pd.DataFrame(pos_vec, columns=col_names, index=df_pos.index)\n",
    "\n",
    "#merge the pos dataset with original dataset\n",
    "df_pos1 = pd.concat( [df_pos1, df[['label','sentence_org','sentence','sentence_wo_emo']], df_pos['pos'] ], axis=1, join=\"inner\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 22)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pos file\n",
    "df_pos1.to_csv(datafolder_p+ r'\\4-features_pos.csv',sep='\\t', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Experiment Area </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['sentence']\n",
    "#use regex and remove emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "folder=r'D:\\18-DS\\data\\models\\POS-Tagger-Hindi'\n",
    "\n",
    "tag = folder+ r'\\hi_hdtb_models\\hi_hdtb_tagger.pt'\n",
    "pret = folder+ r'\\hi_hdtb_models\\hi_hdtb.pretrain.pt'\n",
    "\n",
    "\n",
    "#'pos_model_path': './en_ewt_models/en_ewt_tagger.pt',\n",
    "#'pos_pretrain_path': './en_ewt_models/en_ewt.pretrain.pt',\n",
    "\n",
    "config = {\n",
    "        'processors': 'tokenize,pos',\n",
    "        'tokenize_pretokenized': True,\n",
    "        'pos_model_path': tag,\n",
    "        'pos_pretrain_path': pret,\n",
    "        'pos_batch_size': 1000,\n",
    "        'lang':'hi'\n",
    "         }\n",
    "nlp = stanfordnlp.Pipeline(**config)\n",
    "# doc = nlp(\"\"\"केंद्र की मोदी सरकार ने शुक्रवार को अपना अंतरिम बजट पेश किया.\"\"\")\n",
    "\n",
    "# # doc = nlp(\"\"\"केंद्र की मोदी सरकार ने शुक्रवार को अपना अंतरिम बजट पेश किया. कार्यवाहक वित्त मंत्री पीयूष गोयल \\\n",
    "# # ने अपने बजट में किसान, मजदूर, करदाता, महिला वर्ग समेत हर किसी के लिए बंपर ऐलान किए. हालांकि, \\\n",
    "# # बजट के बाद भी टैक्स को लेकर काफी कन्फ्यूजन बना रहा. \\\n",
    "# # केंद्र सरकार के इस अंतरिम बजट क्या खास रहा और किसको क्या मिला, आसान भाषा में यहां समझें\"\"\")\n",
    "# print(doc.conll_file.conll_as_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_pos=[]\n",
    "for i in range( len( docs ) ):\n",
    "    doc = nlp( docs[i] )\n",
    "    pos_list =[]\n",
    "    for j in range( len(doc.conll_file.sents[0]) ):\n",
    "        pos = doc.conll_file.sents[0][j][3]\n",
    "        #print (doc.conll_file.sents[0][j][1] , pos)\n",
    "        pos_list.append(pos)\n",
    "    pos_list = \", \".join(pos_list)\n",
    "    doc_pos.append(pos_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = pd.DataFrame(doc_pos, columns=['pos'], index=df['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train_pos.iloc[1,].pos.split(\",\"))\n",
    "\n",
    "pos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos1=(train_pos.iloc[1,].pos.split(\",\"))\n",
    "x = filter('NOUN', pos1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pos1)\n",
    "for i in x:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_pos( pos_list): \n",
    "    if (pos2_name in pos_list ): \n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "  \n",
    "  \n",
    "# using filter function \n",
    "pos2_name='NOUN'\n",
    "filtered = filter(find_pos,  pos1) \n",
    "#print (list(filtered))  \n",
    "#print('The filtered letters are:') \n",
    "\n",
    "for s in filtered: \n",
    "    print(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array(pos1)==np.array(['NOUN']*len(pos1))\n",
    "\n",
    "pos1=(train_pos.iloc[1,].pos.split(\",\"))\n",
    "print (pos1)\n",
    "print (np.array(['NOUN']*len(pos1)))\n",
    "print (np.array(pos1))\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFun(**kwargs): \n",
    "    for key, value in kwargs.items():\n",
    "        print (\"%s == %s\" %(key, value))\n",
    " \n",
    "# Driver code\n",
    "myFun(first ='Geeks', mid ='for', last='Geeks')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array( ['NOUN']*len(pos1) ) == np.array( pos1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array( pos1, dtype=\"<U6\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array( ['NOUN']*len(pos1), dtype='<U6' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array( pos1, dtype=\"<U6\" ) ==np.array( ['NOUN']*len(pos1), dtype='<U6' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list=(train_pos.iloc[1,].pos.split(\",\"))\n",
    "for i in range(len(pos_list)):\n",
    "    print ( pos_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=str(train_pos.iloc[1,]).split(\",\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
