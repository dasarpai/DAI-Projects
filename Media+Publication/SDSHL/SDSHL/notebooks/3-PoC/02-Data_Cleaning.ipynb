{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models'\n",
    "\n",
    "vectorfolder_bert = modelfolder + r'\\BERT'\n",
    "vectorfolder_ft   = modelfolder + r'\\fasttext_wiki.hi'\n",
    "vectorfolder_standford = modelfolder + r'\\POS-Tagger-Hindi'\n",
    "\n",
    "file           = datafolder_p + r'\\1-Hinglish_SarcasmCSV.csv'\n",
    "file_clean     = datafolder_p + r\"\\2-Hinglish_Sarcasm_Clean.csv\"\n",
    "file_FE        = datafolder_p + r\"\\3-features_pos.csv\"\n",
    "\n",
    "file_ft        = datafolder_p + r\"\\2-Hinglish_Sarcasm_Clean-fasttext.csv\"\n",
    "train_datafile = datafolder_p + r'\\2-train.csv'\n",
    "test_datafile  = datafolder_p + r'\\2-test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries required\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loadfiles in memory\n",
    "\n",
    "#Load Hindi stopwords\n",
    "df_stopwords = pd.read_csv(datafolder_e + \"\\Stopword_Hindi.csv\")\n",
    "stop_words= list(df_stopwords['words'])\n",
    "stop_words=[] #don't do anything with stopwords\n",
    "\n",
    "#Load 1-Hinglish_SarcasmCSV file\n",
    "df= pd.read_csv(file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove following characters from text '[‚Äî\\-`\":‚Äú‚Äù~]'\n",
    "def normalize_document(doc):\n",
    "    \n",
    "    #pattern = r'[.,;?‡•§:-‚Äî‚Äù`~‚Äú]@#‡•§'\n",
    "    #pattern = r'[!.,?#‡•§)()@:\"]'\n",
    "    pattern_sc = r'[,;‚Äò‚Äô‚Äî\\-`\":‚Äú‚Äù~)(}{*/]' #remplace these special characters with single space\n",
    "    pattern_url = r'^(http|href|ftp|file)s?:\\/\\/.*[\\r\\n]*' #remove any url with null space\n",
    "    pattern_at = r'@*' #remove @\n",
    "    pattern_hash = '#[\\s]*' #remove space after hashtag\n",
    "    pattern_emo = '[\\u210d-\\U0001F9FF]' #emoticon identification\n",
    "    pattern_amp =  '[\\s]+&[\\s]+' #'[\\s][&][\\s]'\n",
    "    \n",
    "    # lower case and remove special characters\\whitespaces.\n",
    "    #doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A) #not required for Hindi script\n",
    "    \n",
    "    #replace any character which has pattern with blank\n",
    "    doc = re.sub( pattern_sc,' ',doc)\n",
    "    doc = re.sub( pattern_url,'',doc)\n",
    "    doc = re.sub( pattern_at,'',doc)\n",
    "\n",
    "    doc = re.sub( '|', \"\",doc)         #sometimes | is used as sentence marker\n",
    "    doc = re.sub( '‡•§‡•§', \"\", doc)       #this is fullstop marker in Devanagari\n",
    "    doc = re.sub( '‡•§', \"\", doc)        #this is fullstop marker in Devanagari\n",
    "    doc = re.sub ('[.]*', \"\", doc)       #sometimes . is fullstop marker by people\n",
    "    doc = re.sub ('[?]+', \"?\", doc)\n",
    "    doc = re.sub ('[!]+', \"!\", doc)\n",
    "    \n",
    "    \n",
    "    #doc = re.sub (' +',' ', doc) #any extra space - No required here. Doing it later.\n",
    "    doc = re.sub ('‚Ä¶','', doc)   #special character\n",
    "    doc = re.sub ('_','', doc)   #Arvind_Kejriwal/ ‡§ß‡§®‡•ç‡§Ø_‡§π‡•à_‡§≠‡§æ‡§∞‡§§_‡§≠‡•Ç‡§Æ‡§ø _ should be removed \n",
    "\n",
    "    #Loop not working for this for some unknown reasons\n",
    "    doc = re.sub ('‡•¶','0', doc)\n",
    "    doc = re.sub ('‡•ß','1', doc)\n",
    "    doc = re.sub ('‡•®','2', doc)\n",
    "    doc = re.sub ('‡•©','3', doc)\n",
    "    doc = re.sub ('‡•™','4', doc)\n",
    "    doc = re.sub ('‡•´','5', doc)\n",
    "    doc = re.sub ('‡•¨','6', doc)\n",
    "    doc = re.sub ('‡•≠','7', doc)\n",
    "    doc = re.sub ('‡•Æ','8', doc)\n",
    "    doc = re.sub ('‡•Ø','9', doc)\n",
    "    \n",
    "    \n",
    "    doc = doc.lower() #convert text to lower case.\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    #sometmes emoji and normal text is without space. To create a space between normal word and emoji. \n",
    "    #Otherwise it will tokenization problem.\n",
    "    emo_pos=re.finditer(pattern_emo , doc)\n",
    "\n",
    "    i=0\n",
    "    j=0\n",
    "    doc1=[]\n",
    "    for pos in emo_pos:\n",
    "        j=pos.end()\n",
    "        if j<0:\n",
    "            j=0\n",
    "        doc1.append(doc[i:j-1]+\" \"+doc[j-1]+\" \" )\n",
    "\n",
    "        i=pos.end()\n",
    "\n",
    "    doc1.append( doc[j:]+\" \" )\n",
    "  \n",
    "    if len(doc1)>0: doc=\"\".join(doc1) #use emoticon process text only if text contained any emoticon else leave.\n",
    "\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    \n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    doc = re.sub( pattern_hash,'#',doc)\n",
    "    doc = re.sub( pattern_amp,'&',doc) #remove spaces around &. J & K, A & N, Modi & Shah\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the dataset\n",
    "corpus = list(df.sentence)\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "norm_corpus = normalize_corpus(corpus)\n",
    "\n",
    "df.sentence=norm_corpus\n",
    "\n",
    "df.sentence=df.sentence.str.replace('\\u200d','') #This is special character comes in Hindi words.\n",
    "\n",
    "df.sentence=df.sentence.str.replace(' üáÆ üá≥ ',' IN ') #In is indian flag while creating space \n",
    "#between emoticon and normal word it is getting split, we need to unite this back.\n",
    "\n",
    "df['sentence'] = df['sentence'].str.replace(' +',' ')  # remove any extra space\n",
    "    \n",
    "df['sentence_wo_emo'] = df['sentence'].str.replace(r'[\\u210d-\\U0001F9FF]','') # Remove emoticon. It does not help in POS tagging.\n",
    "\n",
    "df['sentence_wo_emo'] = df['sentence_wo_emo'].str.replace(' +',' ')  # remove any extra space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranlit2dev(doc, translit_hashtag=False):\n",
    "    from indicnlp.transliterate import acronym_transliterator\n",
    "    from indic_transliteration.sanscript import transliterate \n",
    "    from indic_transliteration import sanscript\n",
    "    import re\n",
    "\n",
    "    ack_transliterator=acronym_transliterator.LatinToIndicAcronymTransliterator()\n",
    "\n",
    "    words=re.split(r' ',doc) #break sentence on space\n",
    "    sent = []\n",
    "    for w in words:\n",
    "        \n",
    "        #if first letter is not # then Translit is True.\n",
    "        #if first Letter is # and tranlit_hastag=True then Translit is True\n",
    "        #else Translit is false.\n",
    "        Translit = (translit_hashtag and w[0]=='#') or (w[0]!='#')                                                      \n",
    "                                                        \n",
    "        \n",
    "        if Translit:\n",
    "            w1 = re.findall(r'[A-z]+',w) ##is  word Latin word?\n",
    "            if len(w1)>0: #Found english word\n",
    "                w2=w1[0]\n",
    "                pat = re.compile(r'([A-Z][a-z]+)') #English word with camel case. Break camel case into different words\n",
    "                w2= pat.split(w2)\n",
    "                w4=[]\n",
    "                for w3 in w2:\n",
    "                    if len(w3)>0:    \n",
    "                        if w3.upper()==w3: # all letters are capital in the word\n",
    "                            w4.append( ack_transliterator.transliterate(w3,lang='hi') )\n",
    "                        else:\n",
    "                            w3=w3.lower()\n",
    "                            w4.append( transliterate(w3, sanscript.ITRANS, sanscript.DEVANAGARI) )\n",
    "                    else:\n",
    "                        pass\n",
    "                    #print (w4)\n",
    "                w5=\"_\".join(w4)\n",
    "\n",
    "            else:\n",
    "                w5=w\n",
    "        else:\n",
    "            w5=w\n",
    "\n",
    "        sent.append(w5)\n",
    "    return \" \".join(sent)                                                                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['‡§ú‡•á‡§π‡§æ‡§¶‡•Ä ‡§Ø‡§æ‡§∏‡§ø‡§® Malik ‡§î‡§∞ ‡§Ü‡§ú‡§æ‡§¶ Kashmir ‡§ö‡§ø‡§≤‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞ ‡§ï‡•ã ‡§¨‡§ö‡§æ‡§®‡•á ‡§Ü‡§ó‡•á ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§è‡§Ç‡§ó‡•á #JammuKashmirFloods #KashmirFloods',\n",
    "       '‡§∏‡§∞‡§ø‡§Ø‡§æ ‡§ï‡§æ‡§®‡•Ç‡§® ‡§ï‡•á ‡§π‡§ø‡§∏‡§æ‡§¨ ‡§∏‡•á ‡§Ö‡§¨ WIFE IS MOTHER',\n",
    "       '‡§Ö‡§Å‡§ß‡•á‡§∞‡•Ä ‡§∞‡§æ‡§§‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§®‡§∏‡§æ‡§® TL ‡§™‡§∞ ‡§è‡§ï ‡§Ö‡§∂‡•ç‡§≤‡•Ä‡§≤ ‡§≤‡•å‡§Ç‡§°‡§æ ‡§®‡§ø‡§ï‡§≤‡§§‡§æ ‡§π‡•à ‡§â‡§∏‡•á ‡§≤‡•ã‡§ó Hawashmi ‡§ï‡§π‡§§‡•á ‡§π‡•à',\n",
    "       'DMK ‡§®‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§Ü‡§ß‡§æ ‡§§‡§≤‡§æ‡§ï‡§º ‡§¶‡§ø‡§Ø‡§æ SP&BSP ‡§®‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§∏‡•á ‡§Ü‡§ß‡§æ ‡§µ‡§ø‡§µ‡§æ‡§π ‡§ï‡§ø‡§Ø‡§æ ‡§™‡§∞ ‡§ö‡§æ‡§π‡§æ ‡§ï‡§∞ ‡§≠‡•Ä Honeymoon ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§®‡§æ ‡§∏‡§ï‡§§‡•á ‡§Ö‡§™‡§µ‡§ø‡§§‡•ç‡§∞ ‡§∞‡§ø‡§∂‡•ç‡§§‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§ê‡§∏‡§æ ‡§π‡•Ä ‡§π‡•ã‡§§‡§æ ‡§π‡•à .',\n",
    "       '‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§Æ‡•á‡§Ç ‡§∂‡§ø‡§≤‡§æ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§®‡•á ‡§ú‡•ã ‡§ú‡§®‡§§‡§æ ‡§™‡§∞ ‡§¨‡§∞‡•ç‡§¨‡§∞‡§§‡§æ ‡§∏‡•á ‡§ü‡§ø‡§Ø‡§∞ ‡§ó‡•à‡§∏ ‡§î‡§∞ ‡§°‡§Ç‡§°‡•á ‡§¨‡§∞‡§∏‡§æ‡§è ‡§•‡•á ‡§â‡§∏‡§ï‡•á ‡§™‡•Ä‡§õ‡•á ‡§ï‡§π‡•Ä‡§Ç ‡§µ‡§π 14 ‡§≤‡§æ‡§ñ ‡§´‡§∞‡•ç‡§ú‡•Ä ‡§µ‡•ã‡§ü ‡§§‡•ã ‡§™‡•ç‡§∞‡•á‡§∞‡§£‡§æ ‡§ï‡•á ‡§∂‡•ç‡§∞‡•ã‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§•‡•á AAP',\n",
    "       '‡§Ö‡§ó‡§∞ ‡§Æ‡•Å‡§≤‡§æ‡§Ø‡§Æ ‡§î‡§∞ ‡§Ö‡§ñ‡§ø‡§≤‡•á‡§∂ ‡§ö‡§æ‡§π‡•á‡§Ç ‡§§‡•ã UP ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¶‡§Ç‡§ó‡§æ ‡§î‡§∞ ‡§ï‡§∞‡§µ‡§æ ‡§ï‡•á ‡§¨‡§ø‡§≤ ‡§Æ‡§π‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§∏‡§ø‡§Ç‡§π ‡§ß‡•ã‡§®‡•Ä ‡§ï‡•á ‡§®‡§æ‡§Æ ‡§™‡•á ‡§´‡§æ‡§°‡§º ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç. #IndvsEng',\n",
    "       '‡§¶‡§ø‡§≤ ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§π‡§∞‡§æ‡§Æ‡•Ä ‡§ï‡§æ‡§Æ‡§ø‡§®‡•Ä ‡§ö‡•Ä‡§ú‡§º ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§Ö‡§ö‡•ç‡§õ‡•á ‡§ñ‡§æ‡§∏‡•á ‡§π‡§∏‡•ç‡§§‡•á ‡§ñ‡•á‡§≤‡§§‡•á ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•ã ‡§∏‡•á‡§Ç‡§ü‡§ø‡§Ø‡§æ‡§™‡§æ ‡§ï‡§æ ‡§è‡§°‡§Æ‡§ø‡§® ‡§¨‡§®‡§æ ‡§¶‡•á‡§§‡•Ä ‡§π‡•à #SadStoriesOfTwitter',\n",
    "       'NIA ‡§ï‡•Ä ‡§ó‡§ø‡§∞‡§´‡•ç‡§§ ‡§Æ‡•á‡§Ç ‡§§‡§æ‡§®‡§ø‡§Ø‡§æ ‡§™‡§∞‡§µ‡•Ä‡§® ‡§π‡§æ‡§´‡§ø‡§ú ‡§∏‡§à‡§¶ ‡§ï‡•ã ‡§ï‡§∞‡§§‡•Ä ‡§•‡•Ä ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü , ‡§π‡§•‡§ø‡§Ø‡§æ‡§∞‡•ã‡§Ç ‡§ï‡•Ä ‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§≤‡•á‡§®‡•á ‡§ú‡§æ‡§®‡§æ ‡§•‡§æ ‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§®',\n",
    "      '‡§Æ‡§ø‡§ó ‡§∏‡•á F16 ‡§â‡§°‡§º‡§æ‡§ä‡§Å ! ‡§ö‡§ø‡§°‡§º‡§ø‡§Ø‡§æ ‡§∏‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§ú ‡§ó‡§ø‡§∞‡§æ‡§ä‡§Å ! ‡§´‡§ø‡§∞ ‡§¶‡•Å‡§∂‡•ç‡§Æ‡§® ‡§ò‡§∞ ‡§Æ‡•á‡§Ç ‡§ö‡§æ‡§Ø ‡§™‡•Ä ‡§Ü‡§ä‡§Å ! ‡§Æ‡•à‡§Ç ‡§Ö‡§≠‡§ø‡§®‡§®‡•ç‡§¶‡§® ‡§®‡§æ‡§Æ ‡§ï‡§π‡§æ‡§ä‡§Å !',\n",
    "      '‡§î‡§∞ ‡§≠‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§™‡§§‡•ç‡§∞‡§ï‡§æ‡§∞‡•ã‡§Ç ‡§™‡§∞ F I R ‡§π‡•Å‡§à ‡§•‡•Ä ‡§µ‡§π ‡§§‡•ã ‡§§‡•Å‡§Æ‡§ï‡•ã ‡§®‡§π‡•Ä‡§Ç ‡§¶‡§ø‡§ñ‡•Ä ‡§Ö‡§ï‡•ç‡§≤ ‡§ï‡•á ‡§Ö‡§Ç‡§ß‡•á ‡§®‡§æ‡§Æ ‡§®‡§Ø‡§®‡§∏‡•Å‡§ñ']\n",
    "\n",
    "for doc in docs:\n",
    "    print( tranlit2dev(doc, translit_hashtag=True) )\n",
    "                                                                            \n",
    "\n",
    "print ('='*20)                                                                            \n",
    "for doc in docs:\n",
    "    print( tranlit2dev(doc, translit_hashtag=False) )     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> Seeing above results of transliteration I am deciding not to do any transliteration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index('ID')\n",
    "df['sentence_org']= df.sentence\n",
    "\n",
    "#remove # from the hashtag, else it will create word dic problem.\n",
    "df['sentence'] = df.sentence.str.replace(\"#\",\"\")\n",
    "df['sentence_wo_emo'] = df.sentence_wo_emo.str.replace(\"#\",\"\")\n",
    "\n",
    "\n",
    "df = df[['label','sentence','sentence_org','sentence_wo_emo']]\n",
    "\n",
    "#save file for transliteration\n",
    "df.to_csv(file_clean,sep=\"\\t\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\18-DS\\\\github\\\\SDSHL\\\\data\\\\processed\\\\2-Hinglish_Sarcasm_Clean.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def splitclusters(s):\n",
    "    \"\"\"Generate the grapheme clusters for the string s. (Not the full\n",
    "    Unicode text segmentation algorithm, but probably good enough for\n",
    "    Devanagari.)\n",
    "\n",
    "    \"\"\"\n",
    "    virama = u'\\N{DEVANAGARI SIGN VIRAMA}'\n",
    "    cluster = u''\n",
    "    last = None\n",
    "    for c in s:\n",
    "        cat = unicodedata.category(c)[0]\n",
    "        if cat == 'M' or cat == 'L' and last == virama:\n",
    "            cluster += c\n",
    "        else:\n",
    "            if cluster:\n",
    "                yield cluster\n",
    "            cluster = c\n",
    "        last = c\n",
    "    if cluster:\n",
    "        yield cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"‡§Æ‡•á‡§∞‡§æ ‡§®‡§æ‡§Æ ‡§π‡§∞‡•Ä ‡§•‡§™‡•ç‡§≤‡§ø‡§Ø‡§æ‡§≤ ‡§π‡•à\"\n",
    "a=\"‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡§æ\"\n",
    "list(splitclusters(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub('[‚Äî\\-`\":‚Äú‚Äù~]',' ','h~a  t‚Äîha-pliyal‚Äùri@pra`\"s‚Äúa:d')\n",
    "# re.sub('#[\\s]*','$',\"s # KashmirFloods\")\n",
    "\n",
    "# #re.sub(r'^(http|href|ftp|file)s?:\\/\\/.*[\\r\\n]*', '', 'file://pmlogy.com')\n",
    "\n",
    "\n",
    "# #re.sub(, '', 'hari @prasad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern_extraspace= \"\\s+\"\n",
    "# doc='üíô‡§ñ‡§º‡•Ç‡§¨‡§∏‡•Ç‡§∞‡§§‡•Ä ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•á üíô ‚ù£Ô∏è‡§ö‡•á‡§π‡§∞‡•á ‡§™‡§∞ ‡§∏‡§ú‡§§‡•Ä ‡§∞‡§π‡•á ‚ù£Ô∏è üíô‡§ñ‡§º‡•Å‡§∂‡•Ä ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•Ä ‡§ú‡§ø‡§®‡•ç‡§¶‡§ó‡•Ä üíô ‚ù£Ô∏è ‡§Æ‡•á‡§Ç ‡§Æ‡§π‡§ï‡§§‡•Ä ‡§∞‡§π‡•á‡•§‚ù£Ô∏è üíü .'\n",
    "# doc='‡§ï‡•Å‡§õ ‡§ê‡§∏‡•á ‡§≠‡•Ä ‡§π‡§æ‡§¶‡§∏‡•á ‡§π‡•ã‡§§‡•á ‡§π‡•à ‡§ú‡§ø‡§Ç‡§¶‡§ó‡•Ä ‡§Æ‡•á‡§Ç ‡§á‡§Ç‡§∏‡§æ‡§® ‡§¨‡§ö ‡§§‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§™‡§∞ ‡§ú‡§º‡§ø‡§Ç‡§¶‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§π‡§§‡§æ ‡§∂‡•Å‡§≠ ‡§∞‡§æ‡§§‡•ç‡§∞‡§ø ‡§Æ‡§ø‡§§‡•ç‡§∞‡•ã‡§Ç'\n",
    "# #sometmes emoji and normal text is without space. To create a space between normal word and emoji. \n",
    "# #Otherwise it will tokenization problem.\n",
    "# pattern_emo = '[\\u210d-\\U0001F900]'\n",
    "# emo_pos=re.finditer(pattern_emo , doc)\n",
    "# i=0\n",
    "# doc1=[]\n",
    "# for pos in emo_pos:\n",
    "#     print (pos)\n",
    "#     j=pos.end()-2\n",
    "#     if j<0:\n",
    "#         j=0\n",
    "#     doc1.append(doc[i:j]+\" \" )\n",
    "#     doc1.append( doc[pos.end()-1]+\" \" )\n",
    "#     #print (j,txt1[i:j],txt1[b.end()-1])\n",
    "#     i=pos.end()\n",
    "\n",
    "# if len(doc1)>0: doc=\"\".join(doc1)\n",
    "# #doc = re.sub( pattern_extraspace,' ',doc) #remove extra space\n",
    "\n",
    "# # tokenize document\n",
    "# tokens = nltk.word_tokenize(doc)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern_emo = '[\\u210d-\\U0001F9FF]'\n",
    "#pattern_emo = '[\\u210d-\\U0001F1F3]'\n",
    "#pattern_emo = '[\\u1F1E6-\\U0001F43B]'\n",
    "#pattern_emo = '[\\u1F600-\\U0001F6FA]'\n",
    "#[\\u263a-\\U0001f645]\n",
    "\n",
    "\n",
    "\n",
    "#a=re.search('\\u270d',a)\n",
    "txt='‡§§‡•á‡§∞‡•á ‡§ö‡•á‡§π‡§∞‡•á ‡§™‡§∞ ‡§ú‡•ã ‡§Æ‡§ï‡•ç‡§ï‡§æ‡§∞‡•Ä ‡§π‡•à ‡§â‡§∏‡•á ‡§õ‡§ø‡§™‡§æ‡§è‡§ó‡§æ ‡§ï‡•à‡§∏‡•á ‡§Æ‡•Å‡§≤‡•ç‡§ï ‡§∏‡•á ‡§ó‡§¶‡•ç‡§¶‡§æ‡§∞‡•Ä ‡§§‡•á‡§∞‡•á ‡§ñ‡•Ç‡§® ‡§Æ‡•á‡§Ç ‡§π‡•à ‡§â‡§∏‡•á ‡§Æ‡§ø‡§ü‡§æ‡§è‡§ó‡§æ ‡§ï‡•à‡§∏‡•á ‚úç‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞‡•á‡§ó‡§æ \\\n",
    "‡§ú‡§æ‡§®‡§ï‡§∞ ‡§Ö‡§ó‡§∞ ‡§Æ‡•Å‡§≤‡•ç‡§ï ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•Å‡§õ üòÄ ‡§ï‡§∞‡§®‡§æ ‡§π‡•Ä ‡§•‡§æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§§‡•ã ‡§™‡§π‡§≤‡•á ‡§ú‡§æ ‡§ï‡§∞ ‡§Ö‡§¨‡•ç‡§¶‡•Å‡§≤ ‡§ï‡§≤‡§æ‡§Æ ‡§ú‡•Ä ‡§ú‡•à‡§∏‡§æ ‡§¶‡•á‡§∂ ‡§≠‡§ï‡•ç‡§§ ‡§¨‡§®‡§ï‡§∞ ‡§¶‡§ø‡§ñ‡§§‡§æ ‡§ü‡•Å‡§ö‡•ç‡§ö‡•á ‡§Ü‡§¶‡§Æ‡•Ä'\n",
    "txt='‡§á‡§∏ ‡§â‡§†‡§æ‡§à‡§ó‡§ø‡§∞‡•á ‡§Ü‡§¶‡§Æ‡•Ä ‡§ï‡•Ä ‡§ê‡§∏‡•Ä ‡§ì‡§õ‡•Ä ‡§π‡§∞‡§ï‡§§ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ü‡•ç‡§∞‡•á‡§Ç‡§° ‡§®‡§π‡•Ä ‡§è‡§´‡§Ü‡§à‡§Ü‡§∞ ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡§î‡§∞ ‡§¶‡•Ç‡§∏‡§∞‡•Ä ‡§¨‡§æ‡§§ ‡§ï‡§≤ ‡§π‡•Ä ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§ø‡§Ç‡§ü‡§ø‡§Ç‡§ó ‡§™‡•ç‡§∞‡•á‡§∏ ‡§™‡§∞ ‡§ú‡§æ‡§ï‡§∞ ‡§á‡§∏‡§ï‡•á ‡§ü‡•ç‡§µ‡•Ä‡§ü üëá‡§ï‡§æ ‡§™‡•ç‡§∞‡§ø‡§Ç‡§ü‡§Ü‡§â‡§ü ‡§®‡§ø‡§ï‡§≤‡§¨‡§æ ‡§ï‡§∞ ‡§™‡•Ç‡§∞‡•á ‡§ú‡§ø‡§≤‡•á ‡§ï‡•á ‡§è‡§ï ‡§è‡§ï ‡§ò‡§∞ ‡§Æ‡•á‡§Ç ‡§¨‡§ü‡§µ‡§æ‡§â‡§ó‡§æ ‡§§‡§æ‡§ï‡§ø ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§§‡§æ ‡§ö‡§≤‡•á ‡§ï‡§æ‡§Ç‡§ó‡•ç‡§∞‡•á‡§∏ ‡§ï‡•á ‡§®‡•á‡§§‡§æ ‡§ï‡•Ä ‡§∏‡•ã‡§ö ‡§ï‡§ø‡§§‡§®‡•Ä ‡§ó‡§ø‡§∞‡•Ä ‡§π‡•Å‡§à ‡§î‡§∞ ‡§ò‡§ü‡§ø‡§Ø‡§æ ‡§π‡•à #‡§Ö‡§∞‡•á‡§∏‡•ç‡§ü_‡§™‡§Ç‡§ï‡§ú_‡§™‡•Ç‡§®‡§ø‡§Ø‡§æ'\n",
    "txt='‡§Æ‡•á‡§∞‡•á ‡§¶‡•á‡§∂ ‡§ï‡•á ‡§®‡•å‡§ú‡§µ‡§æ‡§®‡•ã‡§Ç ‡§â‡§†‡•ã ‡§î‡§∞ ‡§ú‡§≤‡•ç‡§¶‡•Ä Fb ‡§ñ‡•ã‡§≤‡•ã ‡§≤‡§≤‡§ï‡§ø‡§Ø‡§æ‡§Å üòç‡§ë‡§®‡§≤‡§æ‡§á‡§® ‡§∞‡§π‡•Ä ‡§π‡•à‡•§ üòâüòú'\n",
    "txt='‡§Ü‡§ú ‡§ú‡•ã ‡§ü‡•ç‡§µ‡•Ä‡§ü ‡§ï‡§∞ ‡§∞‡§π‡•Ä ‡§π‡•Ç‡§Ç ‡§µ‡•ã ‡§ü‡•ç‡§µ‡•Ä‡§ü ‡§Ö‡§¨ ‡§§‡§ï ‡§ï‡§æ ‡§¨‡•á‡§∏‡•ç‡§ü ‡§ü‡•ç‡§µ‡•Ä‡§ü ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§π‡•à‡•§ ‡§Ö‡§™‡§®‡•á ‡§Ö‡§™‡§®‡•á ‡§è‡§ï ‡§¶‡•á‡§∂ ‡§≠‡§ï‡•ç‡§§ ‡§π‡•Ä‡§∞‡•ã ‡§ï‡§æ ‡§®‡§æ‡§Æ üëá‡§ï‡§Æ‡•á‡§Ç‡§ü ‡§¨‡•â‡§ï‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§≤‡§ø‡§ñ‡•á ‡§Æ‡•á‡§∞‡•á ‡§§‡•ã ‡§π‡•à ‡§ö‡§Ç‡§¶‡•ç‡§∞‡§∂‡•á‡§ñ‡§∞ ‡§Ü‡§ú‡§æ‡§¶‡•§ ‡§î‡§∞ ‡§Ü‡§™‡§ï‡•á ?'\n",
    "txt='‡§∞‡§π‡§®‡•á ‡§¶‡•á ‡§Æ‡•Å‡§ù‡•á #‡§Ö‡§Å‡§ß‡•á‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§è ‡§ó‡§º‡§æ‡§≤‡§ø‡§¨ , üòí ‡§â‡§ú‡§æ‡§≤‡•ã ‡§Æ‡•á‡§Ç ‡§Æ‡•Å‡§ù‡•á ü§ó‡§Ö‡§™‡§®‡•ã‡§Ç ‡§ï‡•á #‡§Ö‡§∏‡§≤‡•Ä ‡§ö‡•á‡§π‡§∞‡•á ‡§®‡§ú‡§º‡§∞ üëÄ‡§Ü ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡•§ ! üòì #GoodMorningTwitte‡•§'\n",
    "#txt='‡§´‡•ç‡§∞‡•Ä ‡§ï‡•Ä ‡§∞‡•ã‡§ü‡•Ä ‡§§‡•ã ‡§ï‡•Å‡§§‡•ç‡§§‡•á ‡§≠‡•Ä ‡§ñ‡§æ‡§§‡•á ‡§π‡•à‡§Ç , ‡§≠‡•Ç‡§ñ ‡§§‡•ã ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§µ‡§æ‡§¶ ‡§ï‡•Ä ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è ‡•§ ‡§ú‡§Ø‡§π‡§ø‡§Ç‡§¶ üáÆüá≥ üö©üôè üôè‡§ú‡§Ø ‡§∏‡§®‡§æ‡§§‡§® ‡§ß‡§∞‡•ç‡§Æ üôèüèª'\n",
    "#txt='üôã‡§¨‡•á‡§ü‡•Ä ‡§¨‡•Ä‡§Æ‡§æ‡§∞ ‡§π‡•ã ‡§§‡•ã ‡§¨‡§°‡§º‡§æ ‡§¶‡•Å‡§ñ ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§¨‡§π‡•Ç ‡§¨‡•Ä‡§Æ‡§æ‡§∞ ‡§π‡•ã ‡§§‡•ã ‡§°‡•ç‡§∞‡§æ‡§Æ‡§æ ‡§≤‡§ó‡§§‡§æ ‡§π‡•à üòäüòä'\n",
    "#txt='OMGüíô‡§ñ‡§º‡•Ç‡§¨‡§∏‡•Ç‡§∞‡§§‡•Ä ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•á üíô ‚ù£Ô∏è‡§ö‡•á‡§π‡§∞‡•á ‡§™‡§∞ ‡§∏‡§ú‡§§‡•Ä ‡§∞‡§π‡•á ‚ù£Ô∏è üíô‡§ñ‡§º‡•Å‡§∂‡•Ä ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Ü‡§™‡§ï‡•Ä ‡§ú‡§ø‡§®‡•ç‡§¶‡§ó‡•Ä üíô ‚ù£Ô∏è ‡§Æ‡•á‡§Ç ‡§Æ‡§π‡§ï‡§§‡•Ä ‡§∞‡§π‡•á‡•§‚ù£Ô∏è üíü . ‡§≤‡§ó‡§§‡§æ ‡§π‡•à'\n",
    "#txt='OMGüò≥‡§Ø‡•á ‡§π‡•à ‡§á‡§∏‡•ç‡§≤‡§æ‡§Æ‡•Ä ‡§∏‡§æ‡§Å‡§™ ‡§Æ‡§¶‡§∞‡§∏‡•ã‡§Ç ‡§Æ‡•á ‡§™‡§¢‡§º‡•á ‡§π‡•Å‡§è ‡§∏‡§™‡•ã‡§≤‡•á ‡§Ö‡§ó‡§∞ ‡§Æ‡§¶‡§∞‡§∏‡•á ‡§¨‡§Ç‡§¶ ‡§®‡§π‡•Ä ‡§π‡•Å‡§è ‡§§‡•ã ‡§≠‡§æ‡§∞‡§§ ‡§Æ‡•á ‡§Ü‡§™ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ø‡§π‡•Ä ‡§π‡•ã‡§ó‡§æ'\n",
    "txt=\"‡§∏‡§¨ ‡§ó‡§è ‡§®‡§æ 2019 ‡§Æ‡•á‡§Ç üëá ‡§ï‡•ã‡§à ‡§õ‡•Ç‡§ü‡§æ ‡§§‡•ã ‡§®‡§π‡•Ä‡§Ç 2018 ‡§Æ‡•á‡§Ç üòù üòÖ üòÇ üòÜ #2018 #2019 #HappyNewYear2019 #HappyNewYear #‡§®‡§µ‡§µ‡§∞‡•ç‡§∑‡§ï‡•Ä‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Å #‡§®‡§µ‡§µ‡§∞‡•ç‡§∑\"\n",
    "txt1=txt\n",
    "a=re.finditer(pattern_emo , txt)\n",
    "\n",
    "i=0\n",
    "txt2=[]\n",
    "for b in a:\n",
    "    j=b.end()\n",
    "    if j<0:\n",
    "        j=0\n",
    "    print (j, txt1[i:j-1]+\" \"+txt1[j-1]+ \" \")\n",
    "    txt2.append(txt1[i:j-1]+\" \"+txt1[j-1]+ \" \" )\n",
    "    #txt2.append( txt1[j:]+\" \" )\n",
    "\n",
    "    i=b.end()\n",
    "\n",
    "txt2.append( txt1[j:]+\" \" )\n",
    "txt2=\"\".join(txt2)\n",
    "txt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hari Hello. What happened'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "a='Hari????? He*llo. What? /happened'\n",
    "re.sub('[,;‚Äò‚Äô‚Äî?\\-`\":‚Äú‚Äù~)(}{*/]','', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "txt = ' ‡•ß ‡§∏‡§æ‡§Å‡§™ ‡•® ‡§Æ‡§¶‡§∞‡§∏‡•ã‡§Ç ‡•© ‡§Æ‡•á ‡•™ ‡§™‡§¢‡§º‡•á ‡•´‡•´ ‡§π‡•Å‡§è ‡•¨‡•≠ ‡§∏‡§™‡•ã‡§≤‡•á ‡§Ö‡§ó‡§∞ ‡§Æ‡§¶‡§∞‡§∏‡•á ‡§¨‡§Ç‡§¶'\n",
    "print( dev2engNumeric(txt) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'hello', re.UNICODE)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_list = re.compile(r'hello')\n",
    "the_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'[a-z][A-Z]hello', re.UNICODE)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_list = re.compile(r'[a-z][A-Z]hello')\n",
    "the_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dmk ‡§®‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§Ü‡§ß‡§æ ‡§§‡§≤‡§æ‡§ï‡§º ‡§¶‡§ø‡§Ø‡§æ sp&bsp ‡§®‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§∏‡•á ‡§Ü‡§ß‡§æ ‡§µ‡§ø‡§µ‡§æ‡§π ‡§ï‡§ø‡§Ø‡§æ ‡§™‡§∞ ‡§ö‡§æ‡§π‡§æ ‡§ï‡§∞ ‡§≠‡•Ä ‡§∏‡•Å‡§π‡§æ‡§ó ‡§∞‡§æ‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§®‡§æ ‡§∏‡§ï‡§§‡•á ‡§Ö‡§™‡§µ‡§ø‡§§‡•ç‡§∞ ‡§∞‡§ø‡§∂‡•ç‡§§‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§ê‡§∏‡§æ ‡§π‡•Ä ‡§π‡•ã‡§§‡§æ ‡§π‡•à'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='dmk ‡§®‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•ã ‡§Ü‡§ß‡§æ ‡§§‡§≤‡§æ‡§ï‡§º ‡§¶‡§ø‡§Ø‡§æ sp & bsp ‡§®‡•á ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§∏‡•á ‡§Ü‡§ß‡§æ ‡§µ‡§ø‡§µ‡§æ‡§π ‡§ï‡§ø‡§Ø‡§æ ‡§™‡§∞ ‡§ö‡§æ‡§π‡§æ ‡§ï‡§∞ ‡§≠‡•Ä ‡§∏‡•Å‡§π‡§æ‡§ó ‡§∞‡§æ‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§®‡§æ ‡§∏‡§ï‡§§‡•á ‡§Ö‡§™‡§µ‡§ø‡§§‡•ç‡§∞ ‡§∞‡§ø‡§∂‡•ç‡§§‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§ê‡§∏‡§æ ‡§π‡•Ä ‡§π‡•ã‡§§‡§æ ‡§π‡•à'\n",
    "re.sub('[\\s]+&[\\s]+','&',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
