{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models\\POS-Tagger-Hindi\\hi_hdtb_models'\n",
    "\n",
    "filepath_data = datafolder_p + r'\\2-Hinglish_Sarcasm_Clean.csv'\n",
    "\n",
    "filepath_pos = datafolder_p + r'\\4-features_pos.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_em = pd.read_csv(datafolder_e + \"\\Emoticons.csv\")\n",
    "emoticonList= list(df_em.Browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start feature engieering start with pos features. Thease are already identified in a file.\n",
    "\n",
    "df= pd.read_csv( filepath_pos, sep='\\t', index_col='ID' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOUN</th>\n",
       "      <th>ADP</th>\n",
       "      <th>VERB</th>\n",
       "      <th>AUX</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PROPN</th>\n",
       "      <th>PART</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>SCONJ</th>\n",
       "      <th>...</th>\n",
       "      <th>NUM</th>\n",
       "      <th>ADV</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>X</th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence_org</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_wo_emo</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>‡§≤‡•ã‡§ó ‡§µ‡§§‡§® ‡§§‡§ï ‡§ñ‡§æ ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§á‡§∏‡§ï‡§æ ‡§á‡§∏‡•á ‡§Ø‡§ï‡•Ä‡§® ‡§®‡§π‡•Ä‡§Ç‡§Æ‡§æ‡§® ‡§ú...</td>\n",
       "      <td>‡§≤‡•ã‡§ó ‡§µ‡§§‡§® ‡§§‡§ï ‡§ñ‡§æ ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§á‡§∏‡§ï‡§æ ‡§á‡§∏‡•á ‡§Ø‡§ï‡•Ä‡§® ‡§®‡§π‡•Ä‡§Ç‡§Æ‡§æ‡§® ‡§ú...</td>\n",
       "      <td>‡§≤‡•ã‡§ó ‡§µ‡§§‡§® ‡§§‡§ï ‡§ñ‡§æ ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§á‡§∏‡§ï‡§æ ‡§á‡§∏‡•á ‡§Ø‡§ï‡•Ä‡§® ‡§®‡§π‡•Ä‡§Ç‡§Æ‡§æ‡§® ‡§ú...</td>\n",
       "      <td>NOUN, NOUN, ADP, VERB, AUX, AUX, PRON, PRON, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§§‡•Ä ‡§π‡•à ‡§á‡§∏‡§≤‡§ø‡§è ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ü‡•ç‡§µ‡§ø‡§ü ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ...</td>\n",
       "      <td>‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§§‡•Ä ‡§π‡•à ‡§á‡§∏‡§≤‡§ø‡§è ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ü‡•ç‡§µ‡§ø‡§ü ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ...</td>\n",
       "      <td>‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§§‡•Ä ‡§π‡•à ‡§á‡§∏‡§≤‡§ø‡§è ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ü‡•ç‡§µ‡§ø‡§ü ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ...</td>\n",
       "      <td>PROPN, PART, VERB, AUX, PRON, NOUN, NOUN, DET,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>‡§Ö‡§ó‡§∞ ‡§Æ‡•Å‡§≤‡§æ‡§Ø‡§Æ ‡§î‡§∞ ‡§Ö‡§ñ‡§ø‡§≤‡•á‡§∂ ‡§ö‡§æ‡§π‡•á‡§Ç ‡§§‡•ã up ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¶‡§Ç‡§ó‡§æ ‡§î...</td>\n",
       "      <td>‡§Ö‡§ó‡§∞ ‡§Æ‡•Å‡§≤‡§æ‡§Ø‡§Æ ‡§î‡§∞ ‡§Ö‡§ñ‡§ø‡§≤‡•á‡§∂ ‡§ö‡§æ‡§π‡•á‡§Ç ‡§§‡•ã up ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¶‡§Ç‡§ó‡§æ ‡§î...</td>\n",
       "      <td>‡§Ö‡§ó‡§∞ ‡§Æ‡•Å‡§≤‡§æ‡§Ø‡§Æ ‡§î‡§∞ ‡§Ö‡§ñ‡§ø‡§≤‡•á‡§∂ ‡§ö‡§æ‡§π‡•á‡§Ç ‡§§‡•ã up ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¶‡§Ç‡§ó‡§æ ‡§î...</td>\n",
       "      <td>SCONJ, PROPN, CCONJ, PROPN, VERB, SCONJ, NOUN,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NOUN  ADP  VERB  AUX  PRON  PROPN  PART  DET  ADJ  SCONJ  ...  NUM  ADV  \\\n",
       "ID                                                            ...             \n",
       "1      4    1     4    5     4      1     0    0    0      0  ...    0    0   \n",
       "5      3    0     4    4     1      1     1    1    2      1  ...    0    0   \n",
       "12     5    3     2    2     0      5     1    0    0      2  ...    1    0   \n",
       "\n",
       "    INTJ  X  words  label                                       sentence_org  \\\n",
       "ID                                                                             \n",
       "1      0  0     19      1  ‡§≤‡•ã‡§ó ‡§µ‡§§‡§® ‡§§‡§ï ‡§ñ‡§æ ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§á‡§∏‡§ï‡§æ ‡§á‡§∏‡•á ‡§Ø‡§ï‡•Ä‡§® ‡§®‡§π‡•Ä‡§Ç‡§Æ‡§æ‡§® ‡§ú...   \n",
       "5      0  0     19      1  ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§§‡•Ä ‡§π‡•à ‡§á‡§∏‡§≤‡§ø‡§è ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ü‡•ç‡§µ‡§ø‡§ü ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ...   \n",
       "12     0  0     24      1  ‡§Ö‡§ó‡§∞ ‡§Æ‡•Å‡§≤‡§æ‡§Ø‡§Æ ‡§î‡§∞ ‡§Ö‡§ñ‡§ø‡§≤‡•á‡§∂ ‡§ö‡§æ‡§π‡•á‡§Ç ‡§§‡•ã up ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¶‡§Ç‡§ó‡§æ ‡§î...   \n",
       "\n",
       "                                             sentence  \\\n",
       "ID                                                      \n",
       "1   ‡§≤‡•ã‡§ó ‡§µ‡§§‡§® ‡§§‡§ï ‡§ñ‡§æ ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§á‡§∏‡§ï‡§æ ‡§á‡§∏‡•á ‡§Ø‡§ï‡•Ä‡§® ‡§®‡§π‡•Ä‡§Ç‡§Æ‡§æ‡§® ‡§ú...   \n",
       "5   ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§§‡•Ä ‡§π‡•à ‡§á‡§∏‡§≤‡§ø‡§è ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ü‡•ç‡§µ‡§ø‡§ü ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ...   \n",
       "12  ‡§Ö‡§ó‡§∞ ‡§Æ‡•Å‡§≤‡§æ‡§Ø‡§Æ ‡§î‡§∞ ‡§Ö‡§ñ‡§ø‡§≤‡•á‡§∂ ‡§ö‡§æ‡§π‡•á‡§Ç ‡§§‡•ã up ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¶‡§Ç‡§ó‡§æ ‡§î...   \n",
       "\n",
       "                                      sentence_wo_emo  \\\n",
       "ID                                                      \n",
       "1   ‡§≤‡•ã‡§ó ‡§µ‡§§‡§® ‡§§‡§ï ‡§ñ‡§æ ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§á‡§∏‡§ï‡§æ ‡§á‡§∏‡•á ‡§Ø‡§ï‡•Ä‡§® ‡§®‡§π‡•Ä‡§Ç‡§Æ‡§æ‡§® ‡§ú...   \n",
       "5   ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§§‡•Ä ‡§π‡•à ‡§á‡§∏‡§≤‡§ø‡§è ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§ü‡•ç‡§µ‡§ø‡§ü ‡§ú‡•ç‡§Ø‡§æ‡§¶‡§æ...   \n",
       "12  ‡§Ö‡§ó‡§∞ ‡§Æ‡•Å‡§≤‡§æ‡§Ø‡§Æ ‡§î‡§∞ ‡§Ö‡§ñ‡§ø‡§≤‡•á‡§∂ ‡§ö‡§æ‡§π‡•á‡§Ç ‡§§‡•ã up ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§¶‡§Ç‡§ó‡§æ ‡§î...   \n",
       "\n",
       "                                                  pos  \n",
       "ID                                                     \n",
       "1   NOUN, NOUN, ADP, VERB, AUX, AUX, PRON, PRON, N...  \n",
       "5   PROPN, PART, VERB, AUX, PRON, NOUN, NOUN, DET,...  \n",
       "12  SCONJ, PROPN, CCONJ, PROPN, VERB, SCONJ, NOUN,...  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['eng_words'] = df['sentence_wo_emo'].str.findall(r'[A-z]+').apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract emoticons create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentences = list(df.sentence)\n",
    "emoticon_fld=[]\n",
    "emoticon_uniq_fld=[]\n",
    "hashtag_fld=[]\n",
    "newSents=[]\n",
    "for sentence in sentences: #for sentence\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    emotions =[]\n",
    "    i=0\n",
    "    for word in words:\n",
    "        #Extract emotions from text\n",
    "        if word in emoticonList:\n",
    "            emotions.append(word)\n",
    "            \n",
    "        i+=1     \n",
    "\n",
    "    emotions =','.join(emotions)\n",
    "    emotions_uniq = \",\".join( set(emotions) )\n",
    "    \n",
    "    emoticon_fld.append(emotions)\n",
    "    emoticon_uniq_fld.append(emotions_uniq)\n",
    "\n",
    "#extract unique emotions of sentence and put in separate field. Without altering original text\n",
    "df['emoticons']=emoticon_fld\n",
    "df['emoticons_uniq'] = emoticon_uniq_fld\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract hashtag and create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentences = list(df.sentence_org)\n",
    "hashtag_fld=[]\n",
    "newSents=[]\n",
    "for sentence in sentences: #for sentence\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    hashtags =[]\n",
    "    i=0\n",
    "    for word in words:\n",
    "        if word[0]==\"#\":\n",
    "            if not word[1:].isdigit():  #if number then don't create hashtag of it.\n",
    "                hashtags.append(word[1:])\n",
    "            \n",
    "        i+=1     \n",
    "\n",
    "    hashtags = \",\".join(hashtags)\n",
    "    \n",
    "    hashtag_fld.append(hashtags)\n",
    "\n",
    "#extract unique emotions of sentence and put in separate field. Without altering original text\n",
    "df['hashtags']=hashtag_fld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count hashtags & emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_len(x):\n",
    "    if len(x):\n",
    "        y = len(x.split(\",\"))\n",
    "    else:\n",
    "        y=0\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_emoticons'] = df['emoticons'].apply(list_len)\n",
    "df['no_hashtag'] = df['hashtags'].apply(list_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Extract More Features in this section </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after all feature engineering remove all non numeric field and save the file for model processing.\n",
    "y=df['label']\n",
    "df=df.drop(columns=['sentence','sentence_org','sentence_wo_emo','pos','emoticons','emoticons_uniq','hashtags','label'])\n",
    "df['label']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(datafolder_p + r\"\\4-Hinglish_Sarcasm_Clean_FE.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words=['üëªüíï','Hari','üòùüò∑','prasad', 'üò∑üò¨üòÇ']\n",
    "# sent=[]\n",
    "# emotions=[]\n",
    "\n",
    "# print (emotions)\n",
    "\n",
    "\n",
    "\n",
    "a[-1]\n",
    "\n",
    "nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'\\d+(.*?)(?:\\u263a|\\U0001f645)')\n",
    "\n",
    "regex = re.compile(r'\\d+(.*?)(?:\\u263a|\\U0001f645)')\n",
    "\n",
    "regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"‚úç\").encode(\"unicode_escape\"))\n",
    "#print((\"üáÆüá≥\").encode(\"unicode_escape\"))\n",
    "#print((\"‡§ï‡•ç‡§Ø‡§æ\").encode(\"unicode_escape\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect \n",
    "from langdetect import detect\n",
    "\n",
    "def isText(char):\n",
    "    try:\n",
    "        lng = detect(char)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "a='‚úçüáÆüá≥‡§ï‡•ç‡§Ø‡§æ'\n",
    "isText(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U+1F1EE,U+1F1F3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"‡§∏‡§¨ ‡§ó‡§è ‡§®‡§æ 2019 ‡§Æ‡•á‡§Ç üëá ‡§ï‡•ã‡§à ‡§õ‡•Ç‡§ü‡§æ ‡§§‡•ã ‡§®‡§π‡•Ä‡§Ç 2018 ‡§Æ‡•á‡§Ç üòù üòÖ üòÇ üòÜ #2018 #2019 #HappyNewYear2019 #HappyNewYear #‡§®‡§µ‡§µ‡§∞‡•ç‡§∑‡§ï‡•Ä‡§∂‡•Å‡§≠‡§ï‡§æ‡§Æ‡§®‡§æ‡§è‡§Å #‡§®‡§µ‡§µ‡§∞‡•ç‡§∑\"\n",
    "\n",
    "\n",
    "words = nltk.word_tokenize(txt)\n",
    "    \n",
    "emotions =[]\n",
    "hashtags =[]\n",
    "i=0\n",
    "for word in words:\n",
    "\n",
    "\n",
    "\n",
    "    i+=1\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNumeric(word):\n",
    "    try:\n",
    "        num = int(word)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
