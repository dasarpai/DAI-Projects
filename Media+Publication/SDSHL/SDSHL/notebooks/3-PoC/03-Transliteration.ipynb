{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models'\n",
    "\n",
    "vectorfolder_bert = modelfolder + r'\\BERT'\n",
    "vectorfolder_ft   = modelfolder + r'\\fasttext_wiki.hi'\n",
    "vectorfolder_standford = modelfolder + r'\\POS-Tagger-Hindi'\n",
    "\n",
    "file           = datafolder_p + r'\\1-Hinglish_SarcasmCSV.csv'\n",
    "file_clean0     = datafolder_p + r\"\\2-Hinglish_Sarcasm_Clean0.csv\"\n",
    "file_clean     = datafolder_p + r\"\\2-Hinglish_Sarcasm_Clean.csv\"\n",
    "file_ft        = datafolder_p + r\"\\2-Hinglish_Sarcasm_Clean-fasttext.csv\"\n",
    "train_datafile = datafolder_p + r'\\2-train.csv'\n",
    "test_datafile  = datafolder_p + r'\\2-test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries required\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# import re\n",
    "# import numpy as np\n",
    "\n",
    "df =pd.read_csv( file_clean0 , sep='\\t')\n",
    "corpus = list(df['sentence_wo_emo'])\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ससुर ने किया बलात्कार आईएसएलएएमआईसी एलएडब्ल्यू एनओडब्ल्यू डब्ल्यूआईएफई आईएस एमओटीएचईआर'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tranlit2dev(doc):\n",
    "    from indicnlp.transliterate import acronym_transliterator\n",
    "    from indic_transliteration.sanscript import transliterate \n",
    "    from indic_transliteration import sanscript\n",
    "    import re\n",
    "\n",
    "    ack_transliterator=acronym_transliterator.LatinToIndicAcronymTransliterator()\n",
    "\n",
    "    words=re.split(r' ',doc) #break sentence on space\n",
    "    sent = []\n",
    "    for w in words:\n",
    "        if w[0]!='#': #Don't convert hashtags\n",
    "            w1 = re.findall(r'[A-z]+',w) ##is  word Latin word?\n",
    "            if len(w1)>0: #Found english word\n",
    "                w2=w1[0]\n",
    "                pat = re.compile(r'([A-Z][a-z]+)') #English word with camel case. Break camel case into different words\n",
    "                w2= pat.split(w2)\n",
    "                w4=[]\n",
    "                for w3 in w2:\n",
    "                    if len(w3)>0:    \n",
    "                        if w3.upper()==w3: # all letters are capital in the word\n",
    "                            w4.append( ack_transliterator.transliterate(w3,lang='hi') )\n",
    "                        else:\n",
    "                            w3=w3.lower()\n",
    "                            w4.append( transliterate(w3, sanscript.ITRANS, sanscript.DEVANAGARI) )\n",
    "                    else:\n",
    "                        pass\n",
    "                    #print (w4)\n",
    "                w5=\"_\".join(w4)\n",
    "\n",
    "            else:\n",
    "                w5=w\n",
    "        else:\n",
    "            w5=w\n",
    "\n",
    "        sent.append(w5)\n",
    "    return \" \".join(sent)\n",
    "    \n",
    "    \n",
    "doc = 'हुर्रियत जेहादी यासिन मलिक Modi और आजाद Kashmir कश्मीर चिलाने वाले कश्मीर को बचाने आगे नहीं आएंगे #JammuKashmirFloods. #KashmirFloods'\n",
    "doc ='सरिया कानून के हिसाब से अब WIFE IS MOTHER'\n",
    "tranlit2dev(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Sad', '']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.split(r'(A-Z)+','ModiForIndia')\n",
    "\n",
    "p = re.compile(r'([A-Z][a-z]+)')\n",
    "x= p.split('Sad')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Sad', '', 'Stories', '', 'Of', '', 'Twitter', '']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['label','sentence']]\n",
    "#save file required in fasttext format\n",
    "df.to_csv(file_ft,sep=\"\\t\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aksharamukha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aksharamukha\n",
      "  Downloading aksharamukha-1.8.1-py3-none-any.whl (159 kB)\n",
      "Requirement already satisfied: requests in d:\\users\\admin\\anaconda3\\lib\\site-packages (from aksharamukha) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->aksharamukha) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->aksharamukha) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->aksharamukha) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->aksharamukha) (3.0.4)\n",
      "Installing collected packages: aksharamukha\n",
      "Successfully installed aksharamukha-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install aksharamukha\n",
    "#http://aksharamukha.appspot.com/documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aksharamukha import transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'नेwस् हरि मोदि अस्होक् नरेन्द्र वेनु नम्त्र विनित'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputtxt = \"News Hari Modi Ashok Narendra Venu Namtra Vinita\"\n",
    "transliterate.process( 'IAST', 'Devanagari',inputtxt, pre_options=['RemoveSchwaHindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputtxt = \"हरी मोदी नरेन्द्र वेणु नम्रता विनिता\"\n",
    "transliterate.process( 'Devanagari','IAST', inputtxt, pre_options=['RemoveSchwaHindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dharm bhārat kī śramaṇ paramparā se niklā dharm aur darśan hai'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate.process('HK', 'Tamil', 'bRhaspati gaMgA', False, post_options = ['TamilSubScript','TamilRemoveApostrophe'])\n",
    "\n",
    "transliterate.process('Thai', 'Devanagari', 'พุทธัง สะระณัง คัจฉามิ', pre_options=['ThaiOrthography'])\n",
    "\n",
    "transliterate.process('Devanagari', 'IAST', 'धर्म भारत की श्रमण परम्परा से निकला धर्म और दर्शन है', pre_options=['RemoveSchwaHindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ಧರ್ಮ್ ಭಾರತ್ ಕೀ ಶ್ರಮಣ್ ಪರಂಪರಾ ಸೇ ನಿಕ್ಲಾ ಧರ್ಮ್ ಔರ್ ದರ್ಶನ್ ಹೈ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transliterate.process('Devanagari', 'Kannada', 'धर्म भारत की श्रमण परम्परा से निकला धर्म और दर्शन है', pre_options=['RemoveSchwaHindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'वॆळ्ळियाऴ्च, तीर्त्तुं अप्रतीक्षितमायिट्टाण् प्रधानमन्त्रि नरेन्द्र मोदि लडाक्किल् ऎत्तियत्. संयुक्त स...'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputtxt=\"വെള്ളിയാഴ്ച, തീർത്തും അപ്രതീക്ഷിതമായിട്ടാണ് പ്രധാനമന്ത്രി നരേന്ദ്ര മോദി ലഡാക്കിൽ എത്തിയത്. സംയുക്ത സ...\"\n",
    "transliterate.process('Malayalam', 'Devanagari', inputtxt, pre_options=['RemoveSchwaHindi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# indic_nlp Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://nbviewer.jupyter.org/url/anoopkunchukuttan.github.io/indic_nlp_library/doc/indic_nlp_examples.ipynb\n",
    "\n",
    "#https://github.com/anoopkunchukuttan/indic_nlp_library\n",
    "#https://github.com/anoopkunchukuttan/indic_nlp_resources\n",
    "        \n",
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r'D:\\18-DS\\github\\Project_NLP\\indic_nlp_library'\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r'D:\\18-DS\\github\\Project_NLP\\indic_nlp_resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp import loader\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokentization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import sentence_tokenize\n",
    "\n",
    "indic_string=\"\"\"तो क्या विश्व कप 2019 में मैच का बॉस टॉस है? यानी मैच में हार-जीत में \\\n",
    "टॉस की भूमिका अहम है? आप ऐसा सोच सकते हैं। विश्वकप के अपने-अपने पहले मैच में बुरी तरह हारने वाली एशिया की दो टीमों \\\n",
    "पाकिस्तान और श्रीलंका के कप्तान ने हालांकि अपने हार के पीछे टॉस की दलील तो नहीं दी, लेकिन यह जरूर कहा था कि वह एक अहम टॉस हार गए थे।\"\"\"\n",
    "sentences=sentence_tokenize.sentence_split(indic_string, lang='hi')\n",
    "for t in sentences:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String: सुनो, कुछ आवाज़ आ रही है। फोन?\n",
      "Tokens: \n",
      "सुनो\n",
      ",\n",
      "कुछ\n",
      "आवाज़\n",
      "आ\n",
      "रही\n",
      "है\n",
      "।\n",
      "फोन\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.tokenize import indic_tokenize  \n",
    "\n",
    "indic_string='सुनो, कुछ आवाज़ आ रही है। फोन?'\n",
    "\n",
    "print('Input String: {}'.format(indic_string))\n",
    "print('Tokens: ')\n",
    "for t in indic_tokenize.trivial_tokenize(indic_string): \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_detokenize  \n",
    "indic_string='\" सुनो , कुछ आवाज़ आ रही है . \" , उसने कहा । '\n",
    "\n",
    "print('Input String: {}'.format(indic_string))\n",
    "print('Detokenized String: {}'.format(indic_detokenize.trivial_detokenize(indic_string,lang='hi')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ਰਾਜਸ੍ਥਾਨ ਮੁਝੇ ਬਹੁਤ ਪਸਂਦ ਹੈ\n",
      "ராஜஸ்தாந முசே பஹுத பஸஂத ஹை\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "UnicodeIndicTransliterator() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a089c7978e41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUnicodeIndicTransliterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransliterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"hi\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"pa\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUnicodeIndicTransliterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransliterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"hi\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0municode_transliterate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnicodeIndicTransliterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"hi\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: UnicodeIndicTransliterator() takes no arguments"
     ]
    }
   ],
   "source": [
    "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
    "from indicnlp.transliterate.script_unifier import unicode_transliterate\n",
    "input_text='राजस्थान मुझे बहुत पसंद है'\n",
    "# input_text='രാജസ്ഥാന'\n",
    "# input_text='රාජස්ථාන'\n",
    "print(UnicodeIndicTransliterator.transliterate(input_text,\"hi\",\"pa\"))\n",
    "print(UnicodeIndicTransliterator.transliterate(input_text,\"hi\",\"ta\"))\n",
    "print(unicode_transliterate.UnicodeIndicTransliterator(input_text,\"hi\",\"ta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Romanization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ाजान\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
    "\n",
    "input_text='राजस्थान'\n",
    "# input_text='ஆசிரியர்கள்'\n",
    "lang='hi'\n",
    "\n",
    "print(ItransTransliterator.to_itrans(input_text,lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanni\n",
      "74\n",
      "61\n",
      "6e\n",
      "6e\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
    "\n",
    "\n",
    "# input_text='rajasthAna'\n",
    "input_text='tanni'\n",
    "lang='ta'\n",
    "x=ItransTransliterator.from_itrans(input_text,lang)\n",
    "print(x)\n",
    "for y in x:\n",
    "    print('{:x}'.format(ord(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Phonetic Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-af07ea346fbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'क'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hi'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0misc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_phonetic_feature_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\indicnlp\\script\\indic_scripts.py\u001b[0m in \u001b[0;36mget_phonetic_feature_vector\u001b[1;34m(c, lang)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[0mphonetic_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphonetic_vectors\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_phonetic_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mphonetic_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Valid Vector Representation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minvalid_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "from indicnlp.script import  indic_scripts as isc\n",
    "\n",
    "c='क'\n",
    "lang='hi'\n",
    "y=isc.get_phonetic_feature_vector(c,lang)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(isc.PV_PROP_RANGES.items(),key=lambda x:x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.langinfo import *\n",
    "\n",
    "c='क'\n",
    "lang='hi'\n",
    "\n",
    "print('Is vowel?:  {}'.format(is_vowel(c,lang)))\n",
    "print('Is consonant?:  {}'.format(is_consonant(c,lang)))\n",
    "print('Is velar?:  {}'.format(is_velar(c,lang)))\n",
    "print('Is palatal?:  {}'.format(is_palatal(c,lang)))\n",
    "print('Is aspirated?:  {}'.format(is_aspirated(c,lang)))\n",
    "print('Is unvoiced?:  {}'.format(is_unvoiced(c,lang)))\n",
    "print('Is nasal?:  {}'.format(is_nasal(c,lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.script import  indic_scripts as isc\n",
    "from indicnlp.script import  phonetic_sim as psim\n",
    "\n",
    "c1='क'\n",
    "c2='ख'\n",
    "c3='भ'\n",
    "lang='hi'\n",
    "\n",
    "print('Similarity between {} and {}'.format(c1,c2))\n",
    "print(psim.cosine(\n",
    "    isc.get_phonetic_feature_vector(c1,lang),\n",
    "    isc.get_phonetic_feature_vector(c2,lang)\n",
    "    ))\n",
    "\n",
    "print()\n",
    "\n",
    "print(u'Similarity between {} and {}'.format(c1,c3))\n",
    "print(psim.cosine(\n",
    "    isc.get_phonetic_feature_vector(c1,lang),\n",
    "    isc.get_phonetic_feature_vector(c3,lang)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.script import  indic_scripts as isc\n",
    "from indicnlp.script import  phonetic_sim as psim\n",
    "\n",
    "\n",
    "slang='hi'\n",
    "tlang='ml'\n",
    "sim_mat=psim.create_similarity_matrix(psim.cosine,slang,tlang,normalize=False)\n",
    "\n",
    "c1='क'\n",
    "c2='ഖ'\n",
    "print('Similarity between {} and {}'.format(c1,c2))\n",
    "print(sim_mat[isc.get_offset(c1,slang),isc.get_offset(c2,tlang)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang='hi'\n",
    "tlang='ml'\n",
    "sim_mat=psim.create_similarity_matrix(psim.sim1,slang,tlang,normalize=True)\n",
    "\n",
    "c1='क'\n",
    "c2='ഖ'\n",
    "print(u'Similarity between {} and {}'.format(c1,c2))\n",
    "print(sim_mat[isc.get_offset(c1,slang),isc.get_offset(c2,tlang)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi string: पिछले दिनों हम लोगों ने कई उत्सव मनाये. कल, हिन्दुस्तान भर में श्री कृष्ण जन्म-महोत्सव मनाया गया.\n",
      "gu string: वीतेला दिवसोमां आपणे केटलाय उत्सवो उजव्या. हजी गइकाले ज पूरा हिंदुस्तानमां श्रीकृष्ण जन्मोत्सव उजववामां आव्यो.\n",
      "Both strings are shown in Devanagari script using script conversion for readability.\n",
      "LCSR: 0.5545454545454546\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.script import  indic_scripts as isc\n",
    "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
    "\n",
    "lang1_str='पिछले दिनों हम लोगों ने कई उत्सव मनाये. कल, हिन्दुस्तान भर में श्री कृष्ण जन्म-महोत्सव मनाया गया.'\n",
    "lang2_str='વીતેલા દિવસોમાં આપણે કેટલાય ઉત્સવો ઉજવ્યા. હજી ગઇકાલે જ પૂરા હિંદુસ્તાનમાં શ્રીકૃષ્ણ જન્મોત્સવ ઉજવવામાં આવ્યો.'\n",
    "lang1='hi'\n",
    "lang2='gu'\n",
    "\n",
    "lcsr, len1, len2 = isc.lcsr_indic(lang1_str,lang2_str,lang1,lang2)\n",
    "\n",
    "print('{} string: {}'.format(lang1, lang1_str))\n",
    "print('{} string: {}'.format(lang2, UnicodeIndicTransliterator.transliterate(lang2_str,lang2,lang1)))\n",
    "print('Both strings are shown in Devanagari script using script conversion for readability.')\n",
    "print('LCSR: {}'.format(lcsr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'पीएमओ'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indicnlp.transliterate import acronym_transliterator\n",
    "\n",
    "ack_transliterator=acronym_transliterator.LatinToIndicAcronymTransliterator()\n",
    "ack_transliterator.transliterate('PMO',lang='hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthographic Syllabification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-27fb07e0d971>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hi'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyllabifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morthographic_syllabify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\indicnlp\\syllable\\syllabifier.py\u001b[0m in \u001b[0;36morthographic_syllabify\u001b[1;34m(word, lang, vocab)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0morthographic_syllabify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m     \u001b[0mp_vectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_phonetic_feature_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0msyllables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\indicnlp\\syllable\\syllabifier.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0morthographic_syllabify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m     \u001b[0mp_vectors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_phonetic_feature_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0msyllables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\indicnlp\\script\\indic_scripts.py\u001b[0m in \u001b[0;36mget_phonetic_feature_vector\u001b[1;34m(c, lang)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[0mphonetic_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphonetic_vectors\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mget_phonetic_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mphonetic_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Valid Vector Representation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minvalid_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "from indicnlp.syllable import  syllabifier\n",
    "\n",
    "w='जगदीशचंद्र'\n",
    "lang='hi'\n",
    "\n",
    "print(' '.join(syllabifier.orthographic_syllabify(w,lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'morph\\\\morfessor\\\\mr.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-507c590e557e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mindicnlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munsupervised_morph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupervisedMorphAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mindic_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'आपल्या हिरड्यांच्या आणि दातांच्यामध्ये जीवाणू असतात .'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\indicnlp\\morph\\unsupervised_morph.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lang, add_marker)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmorfessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMorfessorIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morfessor_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_any_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINDIC_RESOURCES_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'morph'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'morfessor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'{}.model'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_script_range_pat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'^[{}-{}]+$'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanginfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCRIPT_RANGES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanginfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSCRIPT_RANGES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\morfessor\\io.py\u001b[0m in \u001b[0;36mread_any_model\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mmorfessor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaselineModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaselineModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_segmentations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_segmentation_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s was read as a segmentation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\morfessor\\baseline.py\u001b[0m in \u001b[0;36mload_segmentations\u001b[1;34m(self, segmentations)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \"\"\"\n\u001b[0;32m    524\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_segment_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmentation\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msegmentations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_compound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_compound_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegmentation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\morfessor\\io.py\u001b[0m in \u001b[0;36mread_segmentation_file\u001b[1;34m(self, file_name, has_counts, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \"\"\"\n\u001b[0;32m     54\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Reading segmentations from '%s'...\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_text_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhas_counts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompound_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\morfessor\\io.py\u001b[0m in \u001b[0;36m_read_text_file\u001b[1;34m(self, file_name, raw)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_text_file_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\morfessor\\io.py\u001b[0m in \u001b[0;36m_open_text_file_read\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mfile_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m                 \u001b[0mfile_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m                 \u001b[1;31m# Try to determine encoding if not set so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'morph\\\\morfessor\\\\mr.model'"
     ]
    }
   ],
   "source": [
    "from indicnlp.morph import unsupervised_morph \n",
    "from indicnlp import common\n",
    "\n",
    "analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('mr')\n",
    "\n",
    "indic_string='आपल्या हिरड्यांच्या आणि दातांच्यामध्ये जीवाणू असतात .'\n",
    "\n",
    "analyzes_tokens=analyzer.morph_analyze_document(indic_string.split(' '))\n",
    "\n",
    "for w in analyzes_tokens: \n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>Language Detection</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "ftmodel_lang = fasttext.load_model(vectorfolder+'\\lid.176.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n',len(ftmodel_lang.words) )\n",
    "# print('\\n',ftmodel_lang.get_dimension() )\n",
    "# #print(ftmodel_lang.get_labels())\n",
    "# print('\\n',ftmodel_lang.get_line(\"hari\"))\n",
    "# print('\\n',ftmodel_lang.get_analogies('apple','orange','juice',10) )\n",
    "\n",
    "# print ('\\n', ftmodel_lang.get_subwords('Hari') )\n",
    "# #print( ftmodel_lang.get_words())\n",
    "# print ('\\n',ftmodel_lang.get_sentence_vector('Hari Thapliyal'))\n",
    "# print ('\\n',ftmodel_lang.get_subword_id('apple') )\n",
    "# print ('\\n',ftmodel_lang.get_nearest_neighbors('apple') )\n",
    "# ftmodel_lang.predict(['Thapliyal', 'हर'])\n",
    "n=27\n",
    "list(zip(tokenized_corpus[n],ftmodel_lang.predict(tokenized_corpus[n])[0],ftmodel_lang.predict(tokenized_corpus[n])[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Using Langdtect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect \n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whatLang(char):\n",
    "    try:\n",
    "        return detect(char)\n",
    "    except:\n",
    "        return \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df.sentence)\n",
    "emoticon_fld=[]\n",
    "hashtag_fld=[]\n",
    "newSents=[]\n",
    "for sentence in sentences: #for sentence\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    emotions =[]\n",
    "    hashtags =[]\n",
    "    i=0\n",
    "    for word in words:\n",
    "        lng = whatLang(word)\n",
    "        print (word,lng)\n",
    "                \n",
    "\n",
    "#     emotions =' '.join(emotions)\n",
    "#     emotions = \"\".join( set(emotions) )\n",
    "#     hashtags = \",\".join(hashtags)\n",
    "    \n",
    "#     emoticon_fld.append(emotions)\n",
    "#     hashtag_fld.append(hashtags)\n",
    "\n",
    "# #extract unique emotions of sentence and put in separate field. Without altering original text\n",
    "# df['emoticon']=emoticon_fld\n",
    "# df['hashtags']=hashtag_fld\n",
    "# #df['sentence'] = newSents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transliteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Using indic-transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install indic-transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "णेव्स् आप सभी का यहान् स्वागत् है।\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from indic_transliteration import sanscript \n",
    "from indic_transliteration.sanscript import transliterate \n",
    "  \n",
    "# the text to be transliterated \n",
    "text = \"Apa sabhii kaa yahaan svaagat hai.\"\n",
    "#text = \"ThankYouAmitShah\"\n",
    "#text =\"Behaviour\"\n",
    "tkn = nltk.word_tokenize(text)\n",
    "\n",
    "# for k in tkn:\n",
    "#     #text = \"News\"\n",
    "#     # printing the transliterated text \n",
    "#     #print(transliterate(text, sanscript.ITRANS, sanscript.DEVANAGARI)) \n",
    "#     print(detect(k),k, transliterate(k, sanscript.ITRANS, sanscript.DEVANAGARI)) \n",
    "\n",
    "print(transliterate(text, sanscript.ITRANS, sanscript.DEVANAGARI)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'हरि थपलियल नेव्स्'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = 'idam adbhutam'\n",
    "data = \"narendra modi\"\n",
    "data =\"hari thapaliyala news\"\n",
    "transliterate(data, sanscript.ITRANS, sanscript.DEVANAGARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install indic-nlp-library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: 'https://download.pytorch.org/whl/torch_stable.html'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Url ''https://download.pytorch.org/whl/torch_stable.html'' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "ERROR: Could not find a version that satisfies the requirement torch==1.3.1+cpu (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\n",
      "ERROR: No matching distribution found for torch==1.3.1+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.3.1+cpu -f \n",
    "#'https://download.pytorch.org/whl/torch_stable.html'\n",
    "#'https://download.pytorch.org/whl/torch_stable.html'\n",
    "\n",
    "#!pip install inltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f2dff232ea54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# The languages supported are Hindi (hi), Punjabi (pa), Sanskrit (sa), Gujarati (gu), Kannada (kn),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\inltk\\inltk.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(language_code)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mloop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    590\u001b[0m         \"\"\"\n\u001b[0;32m    591\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This event loop is already running'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch==1.3.1+cpu -f 'https://download.pytorch.org/whl/torch_stable.html'\n",
    "    \n",
    "from inltk.inltk import setup\n",
    "setup('hi')\n",
    "\n",
    "# The languages supported are Hindi (hi), Punjabi (pa), Sanskrit (sa), Gujarati (gu), Kannada (kn), \n",
    "# Malayalam (ml), Nepali (ne), Odia (or), Marathi (mr), Bengali (bn), Tamil (ta), Urdu (ur), English (en).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁',\n",
       " '#',\n",
       " 'N',\n",
       " 'ew',\n",
       " 's',\n",
       " '▁तीन',\n",
       " '▁साल',\n",
       " '▁बाथ',\n",
       " 'रूम',\n",
       " '▁में',\n",
       " '▁बंद',\n",
       " '▁रही',\n",
       " '▁विवाहित',\n",
       " 'ा']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize\n",
    "from inltk.inltk import tokenize\n",
    "text ='#News तीन साल बाथरूम में बंद रही विवाहिता'\n",
    "tokenize( text,'hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\Users\\\\admin\\\\anaconda3\\\\lib\\\\site-packages\\\\inltk\\\\models\\\\all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-25031eea812e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0midentify_language\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_language_identifying_models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mreset_language_identifying_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# only if you've updated iNLTK version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0midentify_language\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\inltk\\inltk.py\u001b[0m in \u001b[0;36mreset_language_identifying_models\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreset_language_identifying_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mreset_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\inltk\\utils.py\u001b[0m in \u001b[0;36mreset_models\u001b[1;34m(folder_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreset_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'models'\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34mf'{folder_name}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[1;31m# can't continue even if onerror hook returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;31m# Allow introspection of whether or not the hardening against symlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    594\u001b[0m             \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscandir_it\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m             \u001b[0mentries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\Users\\\\admin\\\\anaconda3\\\\lib\\\\site-packages\\\\inltk\\\\models\\\\all'"
     ]
    }
   ],
   "source": [
    "from inltk.inltk import identify_language, reset_language_identifying_models\n",
    "\n",
    "reset_language_identifying_models() # only if you've updated iNLTK version\n",
    "identify_language(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\serialization.py:614: UserWarning: Couldn't retrieve source code for container of type CrossEntropyLoss. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n",
      "D:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\serialization.py:614: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n",
      "D:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\serialization.py:614: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n",
      "D:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\serialization.py:614: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n",
      "D:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\serialization.py:614: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'LSTM' object has no attribute '_flat_weights_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f89aa5c3d7fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0minltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_embedding_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_embedding_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'भारत'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'hi'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\inltk\\inltk.py\u001b[0m in \u001b[0;36mget_embedding_vectors\u001b[1;34m(input, language_code)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mdefaults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_learner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'models'\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34mf'{language_code}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36mload_learner\u001b[1;34m(path, file, test, **db_kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m     \u001b[0mcb_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cb_state'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[0mclas_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cls'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 626\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclas_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'callback_fns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#to avoid duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mload_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcb_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\fastai\\text\\learner.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, model, split_func, clip, alpha, beta, metrics, **learn_kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m                                                      isinstance(data.train_ds.y, LMLabelList)))\n\u001b[0;32m     51\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_class\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlearn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNNTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGradientClipping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups, add_time, silent, cb_fns_registered)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\fastai\\basic_train.py\u001b[0m in \u001b[0;36m__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;34m\"Setup path,metrics, callbacks and ensure model directory exists.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mifnone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlistify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n",
      "\u001b[1;31mModuleAttributeError\u001b[0m: 'LSTM' object has no attribute '_flat_weights_names'"
     ]
    }
   ],
   "source": [
    "from inltk.inltk import get_embedding_vectors\n",
    "\n",
    "vectors = get_embedding_vectors('भारत', 'hi')\n",
    "vectors[0].shape\n",
    "\n",
    "get_embedding_vectors('ਜਿਹਨਾਂ ਤੋਂ ਧਾਤਵੀ ਅਲੌਹ ਦਾ ਆਰਥਕ','pa')\n",
    "vectors = get_embedding_vectors('ਜਿਹਨਾਂ ਤੋਂ ਧਾਤਵੀ ਅਲੌਹ ਦਾ ਆਰਥਕ','pa')\n",
    "len(vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using indicnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ਆਜ ਮੌਸਮ ਅਚ੍ਛਾ ਹੈ੤ ਸੂਰਜ ਉਜ੍ਜ੍ਵਲ ਹੈ ਔਰ ਬਾਰਿਸ਼ ਕੇ ਕੋਈ ਸਂਕੇਤ ਨਹੀਂ ਹੈਂ੤ ਇਸਲਿਏ ਹਮ ਆਜ ਖੇਲ ਸਕਤੇ ਹੈਂ!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'आज मौसम अच्छा है। सूरज उज्ज्वल है और बारिश के कोई संकेत नहीं हैं। इसलिए हम आज खेल सकते हैं!'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
    "\n",
    "# Input text \"Today the weather is good. Sun is bright and there are no signs of rain. Hence we can play today.\"\n",
    "input_text='आज मौसम अच्छा है। सूरज उज्ज्वल है और बारिश के कोई संकेत नहीं हैं। इसलिए हम आज खेल सकते हैं!'\n",
    "\n",
    "# Transliterate from Hindi to Telugu\n",
    "print(UnicodeIndicTransliterator.transliterate(input_text,\"hi\",\"pa\"))\n",
    "\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
    "from indic_transliteration import sanscript\n",
    "transliterate(input_text, sanscript.HK, sanscript.ITRANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-86dc7e620502>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'code' is not defined"
     ]
    }
   ],
   "source": [
    "code('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVं\n"
     ]
    }
   ],
   "source": [
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate\n",
    "\n",
    "data = 'EVM'\n",
    "\n",
    "print(transliterate(data, sanscript.HK, sanscript.DEVANAGARI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(UnicodeIndicTransliterator.transliterate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आज मौसम अच्छा है। इसलिए हम आज खेल सकते हैं!\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
    "\n",
    "input_text='आज मौसम अच्छा है। इसलिए हम आज खेल सकते हैं!'\n",
    "\n",
    "# Transliterate Hindi to Roman\n",
    "print(ItransTransliterator.to_itrans(input_text, 'ro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ItransTransliterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indic_transliteration import xsanscript\n",
    "#from indic_transliteration.xsanscript import  SchemeMap, SCHEMES, transliterate\n",
    "from indic_transliteration.xsanscript import  transliterate\n",
    "#data = 'नेहरू चाचा का सन्देश। जिसे पढकर आप को चाचा को जहन्नुम से धरती पर लाकर चार जूते मारने का मन करेगा 😡 👇 #Nehru #नेहरू #कांग्रेसी #CongressMuktBharat Please #रिट्वीट'\n",
    "#data = \"Vishal Tripathi\"\n",
    "#data = \"WhyMediaIsSoAntiHindu\"\n",
    "\n",
    "#print(transliterate(data, xsanscript.DEVANAGARI, xsanscript.KANNADA))\n",
    "print(transliterate(data, xsanscript.IAST, xsanscript.DEVANAGARI))\n",
    "print(transliterate(data, xsanscript.DEVANAGARI, xsanscript.IAST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='हिरण, हिंदू, हिंदुस्तान_को_हिन्दूराष्ट्र_घोषित_करें, हल्ला_बोल, हर_हर_महादेव, सुप्रभात, \\\n",
    "सुना_है,iphone7, सीधासवाल, सबूत, शुभरात्रि, शत्रुघ्न,ठीक_है, व्यंग्य,कटाक्ष, व्यंग, विश्व_विख्यात,पप्पू, विकास_को_इंसाफ_कब, वावा, \\\n",
    "वसंतपंचमी, लौटकेराहुलघरकोआए,तपस्या, लोकतंत्र, लॉलीपॉप, लादेन,ये, लल्लनटॉप_सरकार, लम्बाई, राहुल_गाँधी,rahul_gandhi, \\\n",
    "रामदेव,भक्त, रानी_लक्ष्मीबाई, मोदी_फिरकी_ले_रहा_है, मोदी,केजरी, मोदी,2019Elections, मोदी, मुंबई, मायावती,शम्भुनाथ,हिंदुत्व, \\\n",
    "महादेव,शिव_, महादेव, महातूफान,उम्पुन, ममता_का_जंगलराज, मजदूर, भैरोंसिंह_शेखावत, भारतीय_सेना_हमें_आप_पर_गर्व_है, भारत_बंद, \\\n",
    "भारत_को_हिन्दुराष्ट्र_घोसित_करो, भाजपा_का_विजयरथ, ब्राह्मण_।, बेहद_दुखद_खबर, बेचारी_बहन_का_बेचारा_दल्ला_भाई, \\\n",
    "बेगानी_शादी_में_अब्दुल्ला_दीवाना, बालाकोट, बारिश, बलात्कारी_बापू, बधाई_बेशर्म_केजरीवाल, बटवारा,\\\n",
    "PulwamaAttack,PulwamaEncounter,Pulwama,PulwamaTerrorAttack,पुलवामा_घात,पुलवामा, फ्री,कूट, \\\n",
    "फिलस्तीन,नेतन्याहू, प्रधानमंत्री,राजमाता।, पूर्वांचल_विरोधी_केजरीवाल, पाकिस्तान, पप्पू_दिवस, पप्पू,राहुल_गांधी,राहुल,RahulGandhiInDubai, \\\n",
    "पपु_यादव, पद्मावत, पकिस्तान, नशा_मुक्ति_भारत_अभियान, नमो, धरना_भी, धन्य_है_भारत_भूमि, दोगली_कांग्रेस, दिल्ली_सर, \\\n",
    "दगाबाज_चीन, तीन,पाँच,निजी_अनुभव, तिब्बत,JunkOneChina, ड्रामेबाज_कांग्रेस, झूठ_लूट_की_सरकार, जोकर, जयहिंद,वंदेमातरम, \\\n",
    "जयश्रीराम, जय_हिंद_की_सेना, जय_श्रीराम,जय_बजरंगबली, जय_श्रीराम, जय_श्री_कृष्णा,सुप्रभात।, जन्मदिन_मुबारक_बादशाह, \\\n",
    "छत्तीसगढ़,माओवादियों,संपत्ति,कुर्क, चुनाव,अब, चीनी_चमचे_भाई_भाई, गौमाता_को_इंसाफ_दो, गृहमंत्री,नाराज_फूफाजी'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='अँधेरे, अंगना, अकेला, अनिलकपूर, अब, अमित_शाह, अम्बेडकर, अयाश_आसाराम, अय्याश, अय्याश_आसाराम, अय्याशी, अरेस्ट_चंद्रशेख।, अरेस्ट_चंद्रशेखर_रावण, अरेस्ट_चन्द्रशेखर_रावण, अरेस्ट_पंकज_पूनिया, असली, आरक्षण, आरक्षण_भीख_है, आशा, आशुतोष, आसाराम, आसाराम_की_नाजायज_औलाद_हितेन्द्र, इंशाल्ह_covid_played_well, इटालियन_परिवार_तिजोरी_खोलो, उम्पुन, ऊंट, एकऔरएकग्यारह, एडीजीपी, ओणम, कंगना, कटाक्ष, कदम, कांग्रेस_मुक्त_भारत, कांग्रेसियों, कांग्रेसी, किरण, कुर्क, कूट, केजरी, केजरीवाल, कोरोना, कोरोनावायरस, खूनीDemonetisation, गांव_बासनी_हरिसिंह, गिरफ्तार_करो_ओवैसी_को, गुफा, गृहमंत्री, गौमाता_को_इंसाफ_दो, चमचे_मुल्ले, चीनी_चमचे_भाई_भाई, चुनाव, छत्तीसगढ़, जन्मदिन_मुबारक_बादशाह, जय_बजरंगबली, जय_श्री_कृष्णा, जय_श्री_महांकाल, जय_श्रीराम, जय_हिंद, जय_हिंद_की_सेना, जयश्रीराम, जयहिंद, जयहिंद।, जागो।, जातपात, जोकर, झूठ_लूट_की_सरकार, ठीक_है, ड्रामेबाज_कांग्रेस, तपस्या, तिब्बत, तीन, दगाबाज_चीन, दिल्ली_सर, दोगली_कांग्रेस, धन्य_है_भारत_भूमि, धरना_भी, नमो, नमो_अगेन, नमो_नमः, नववर्ष, नववर्षकीशुभकामनाएँ, नशा_मुक्ति_भारत_अभियान, नाराज_फूफाजी, निजी_अनुभव, नेतन्याहू, नेहरू, पकिस्तान, पथ, पद्मावत, पपु_यादव, पप्पू, पप्पू_दिवस, पाँच, पाकिस्तान, पुलवामा, पुलवामा_आतंकी_हमला, पुलवामा_घात, पुलवामा_हमला, पूर्वांचल_विरोधी_केजरीवाल, प्रधानमंत्री, फिलस्तीन, फ्री, बटवारा, बधाई_बेशर्म_केजरीवाल, बलात्कारी_बापू, बारिश, बालाकोट, बेगानी_शादी_में_अब्दुल्ला_दीवाना, बेचारी_बहन_का_बेचारा_दल्ला_भाई, बेहद_दुखद_खबर, ब्राह्मण_।, भक्त, भाजपा_का_विजयरथ, भारत_को_हिन्दुराष्ट्र_घोसित_करो, भारत_बंद, भारतीय_सेना_हमें_आप_पर_गर्व_है, भैरोंसिंह_शेखावत, मजदूर, मदरसों, ममता_का_जंगलराज, महा_चू, महातूफान, महादेव, महासमर, माओवादियों, मायावती, मुंबई, मोदी, मोदी_फिरकी_ले_रहा_है, मोदी_लाओ_देश_बचाओ, यूँही, ये, राजमाता।, रानी_लक्ष्मीबाई, रामदेव, राष्ट्रवाद, राहुल, राहुल_गाँधी, राहुल_गांधी, राहुलगांधी, रिट्वीट, लम्बाई, लल्लनटॉप_सरकार, लादेन, लॉलीपॉप, लोकतंत्र, लौटकेराहुलघरकोआए, वंदेमातरम, वसंतपंचमी, वावा, विकास_को_इंसाफ_कब, विश्व_विख्यात, विहान, व्यंग, व्यंग्य, शत्रुघ्न, शम्भुनाथ, शिव_, शुभरात्रि, संपत्ति, सच, सबूत, सीधासवाल, सुना_है, सुप्रभात, सुप्रभात।, हर_हर_महादेव, हल्ला_बोल, हिंदुत्व, हिंदुस्तान_को_हिन्दूराष्ट्र_घोषित_करें, हिंदू, हिन्दू_हिन्दू_भाई_भाई, हिरण'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expertiments Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "stanfordnlp.download('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(stanfordnlp.Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanfordnlp.Pipeline(processors = \"pos\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "config = {\n",
    "        'processors': 'tokenize,pos',\n",
    "        'tokenize_pretokenized': True,\n",
    "        'pos_model_path': './en_ewt_models/en_ewt_tagger.pt',\n",
    "        'pos_pretrain_path': './en_ewt_models/en_ewt.pretrain.pt',\n",
    "        'pos_batch_size': 1000\n",
    "         }\n",
    "nlp = stanfordnlp.Pipeline(**config)\n",
    "# doc = nlp('Joe Smith lives in California .\\nHe loves pizza .')\n",
    "# print(doc.conll_file.conll_as_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {\n",
    "'CC': 'coordinating conjunction','CD': 'cardinal digit','DT': 'determiner',\n",
    "'EX': 'existential there (like: \\\"there is\\\" ... think of it like \\\"there exists\\\")',\n",
    "'FW': 'foreign word','IN':  'preposition/subordinating conjunction','JJ': 'adjective \\'big\\'',\n",
    "'JJR': 'adjective, comparative \\'bigger\\'','JJS': 'adjective, superlative \\'biggest\\'',\n",
    "'LS': 'list marker 1)','MD': 'modal could, will','NN': 'noun, singular \\'desk\\'',\n",
    "'NNS': 'noun plural \\'desks\\'','NNP': 'proper noun, singular \\'Harrison\\'',\n",
    "'NNPS': 'proper noun, plural \\'Americans\\'','PDT': 'predeterminer \\'all the kids\\'',\n",
    "'POS': 'possessive ending parent\\'s','PRP': 'personal pronoun I, he, she',\n",
    "'PRP$': 'possessive pronoun my, his, hers','RB': 'adverb very, silently,',\n",
    "'RBR': 'adverb, comparative better','RBS': 'adverb, superlative best',\n",
    "'RP': 'particle give up','TO': 'to go \\'to\\' the store.','UH': 'interjection errrrrrrrm',\n",
    "'VB': 'verb, base form take','VBD': 'verb, past tense took',\n",
    "'VBG': 'verb, gerund/present participle taking','VBN': 'verb, past participle taken',\n",
    "'VBP': 'verb, sing. present, non-3d take','VBZ': 'verb, 3rd person sing. present takes',\n",
    "'WDT': 'wh-determiner which','WP': 'wh-pronoun who, what','WP$': 'possessive wh-pronoun whose',\n",
    "'WRB': 'wh-abverb where, when','QF' : 'quantifier, bahut, thoda, kam (Hindi)','VM' : 'main verb',\n",
    "'PSP' : 'postposition, common in indian langs','DEM' : 'demonstrative, common in indian langs'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_doc = nlp(\"\"\"केंद्र की मोदी सरकार ने शुक्रवार को अपना अंतरिम बजट पेश किया. कार्यवाहक वित्त मंत्री पीयूष गोयल \\\n",
    "ने अपने बजट में किसान, मजदूर, करदाता, महिला वर्ग समेत हर किसी के लिए बंपर ऐलान किए. हालांकि, \\\n",
    "बजट के बाद भी टैक्स को लेकर काफी कन्फ्यूजन बना रहा. केंद्र सरकार के इस अंतरिम बजट क्या खास रहा \\\n",
    "और किसको क्या मिला, आसान भाषा में यहां समझें\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract parts of speech\n",
    "def extract_pos(doc):\n",
    "    parsed_text = {'word':[], 'pos':[], 'exp':[]}\n",
    "    for sent in doc.sentences:\n",
    "        for wrd in sent.words:\n",
    "            if wrd.pos in pos_dict.keys():\n",
    "                pos_exp = pos_dict[wrd.pos]\n",
    "            else:\n",
    "                pos_exp = 'NA'\n",
    "            parsed_text['word'].append(wrd.text)\n",
    "            parsed_text['pos'].append(wrd.pos)\n",
    "            parsed_text['exp'].append(pos_exp)\n",
    "    #return a dataframe of pos and text\n",
    "    return pd.DataFrame(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "MODELS_DIR = '.'\n",
    "stanfordnlp.download('en', MODELS_DIR) # Download the English models\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,pos', models_dir=MODELS_DIR, treebank='hi_ewt', use_gpu=True, pos_batch_size=3000) # Build the pipeline, specify part-of-speech processor's batch size\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.\") # Run the pipeline on input text\n",
    "doc.sentences[0].print_tokens() # Look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.\") # run annotation over a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "MODELS_DIR = '.'\n",
    "stanfordnlp.download('en', MODELS_DIR) # Download the English models\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,pos', models_dir=MODELS_DIR, treebank='en_ewt', use_gpu=True, pos_batch_size=3000) \n",
    "# Build the pipeline, specify part-of-speech processor's batch size\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.\") # Run the pipeline on input text\n",
    "doc.sentences[0].print_tokens() # Look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ='Sad_Stories_Of_Twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<module 'indic_transliteration.sanscript.schemes.roman' from 'D:\\\\Users\\\\admin\\\\anaconda3\\\\lib\\\\site-packages\\\\indic_transliteration\\\\sanscript\\\\schemes\\\\roman.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3dfa3d564bfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransliterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'News'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msanscript\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroman\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msanscript\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEVANAGARI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\indic_transliteration\\sanscript\\__init__.py\u001b[0m in \u001b[0;36mtransliterate\u001b[1;34m(data, _from, _to, scheme_map, **kw)\u001b[0m\n\u001b[0;32m    228\u001b[0m   \"\"\"\n\u001b[0;32m    229\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mscheme_map\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mscheme_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_scheme_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_from\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m   options = {\n\u001b[0;32m    232\u001b[0m     \u001b[1;34m'togglers'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'##'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\anaconda3\\lib\\site-packages\\indic_transliteration\\sanscript\\__init__.py\u001b[0m in \u001b[0;36m_get_scheme_map\u001b[1;34m(input_encoding, output_encoding)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0moutput_encoding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInput\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mMust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdefined\u001b[0m \u001b[1;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mSCHEMES\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \"\"\"\n\u001b[1;32m--> 209\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mSchemeMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSCHEMES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_encoding\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSCHEMES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput_encoding\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtransliterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_from\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheme_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: <module 'indic_transliteration.sanscript.schemes.roman' from 'D:\\\\Users\\\\admin\\\\anaconda3\\\\lib\\\\site-packages\\\\indic_transliteration\\\\sanscript\\\\schemes\\\\roman.py'>"
     ]
    }
   ],
   "source": [
    "transliterate('News', sanscript.roman, sanscript.DEVANAGARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
