{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split   \n",
    "import sklearn.metrics as metrics\n",
    "from keras.preprocessing.text import Tokenizer                    \n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models'\n",
    "resultsfolder = r'D:\\18-DS\\github\\SDSHL\\data\\results'\n",
    "\n",
    "file_train = datafolder_p + r'\\2-train.csv'\n",
    "file_test  = datafolder_p + r'\\2-test.csv'\n",
    "file_data  = datafolder_p + r'\\2-Hinglish_Sarcasm_Clean.csv'\n",
    "file_FE  = datafolder_p + r'\\4-Hinglish_Sarcasm_Clean_FE.csv'\n",
    "\n",
    "modelfolder_ft       = r'D:\\18-DS\\data\\models\\fasttext_wiki.hi'\n",
    "modelfolder_ft_ind   = r'D:\\18-DS\\data\\models\\fasttext_indicnlp.hi'\n",
    "\n",
    "prediction={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> Load \"2-Hinglish_Sarcasm_Clean.csv\" File & Train-Test Split</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(file_data, sep='\\t', index_col=\"ID\")\n",
    "sent_size = max([len(s.split()) for s in df.sentence])\n",
    "sent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train  = pd.read_csv(file_train, sep='\\t', index_col=\"ID\")\n",
    "df_train = df_train[['sentence','label']]\n",
    "idx_train = df_train.index\n",
    "\n",
    "df_val   = pd.read_csv(file_test, sep='\\t', index_col=\"ID\")\n",
    "df_val   = df_val[['sentence','label']]\n",
    "idx_val  = df_val.index\n",
    "\n",
    "df  = pd.read_csv(file_data, sep='\\t', index_col=\"ID\")\n",
    "full_text = df['sentence']\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000) #pickup only 5000 top words\n",
    "tokenizer.fit_on_texts(full_text)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(df_train['sentence'])\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=sent_size)\n",
    "X_train = pd.DataFrame(X_train, index=idx_train)\n",
    "y_train = df_train['label']\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(df_val['sentence'])\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=sent_size)\n",
    "X_val = pd.DataFrame(X_val, index=idx_val)\n",
    "y_val = df_val['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> CNN from Original File </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000) #pickup only 5000 top words\n",
    "tokenizer.fit_on_texts(df['sentence'])\n",
    "\n",
    "df_train = tokenizer.texts_to_sequences(df_train['sentence'])\n",
    "df_train = pad_sequences(df_train, padding='post', maxlen=sent_size)\n",
    "df_train = pd.DataFrame(df_train, index=idx_train)\n",
    "\n",
    "df_val = tokenizer.texts_to_sequences(df_val['sentence'])\n",
    "df_val = pad_sequences(df_val, padding='post', maxlen=sent_size)\n",
    "df_val = pd.DataFrame(df_val, index=idx_val)\n",
    "\n",
    "vocab_size=len(tokenizer.word_index) + 1 #+1 for padding\n",
    "\n",
    "embedding_dim = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9156"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1800, 119), (1800,), (200, 119), (200,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token to Sequence (embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit( X_train, y_train,\n",
    "#                     epochs=10,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(df, batch_size):\n",
    "    print( 'batch size =', batch_size)\n",
    "    \n",
    "    while True:  \n",
    "        df_size = len(df)\n",
    "        \n",
    "        num_batches = df_size//batch_size # calculate the number of batches\n",
    "        remaining_records= df_size %batch_size\n",
    "        i=0\n",
    "        for j in range(num_batches):\n",
    "            tempdf = df.iloc[i*batch_size: (i+1)*batch_size,:]\n",
    "            print (j,'\\n',tempdf.shape)\n",
    "            yield tempdf.iloc[:,:100], tempdf['label']\n",
    " \n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if remaining_records!=0:\n",
    "            tempdf = df.iloc[(i+1)*batch_size:,:]\n",
    "            print ('here',j,'\\n',tempdf.shape)\n",
    "            yield tempdf.iloc[:,:100], tempdf['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture : CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9156, 200, 119, (1800, 119), (1800,), (200, 119), (200,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, embedding_dim, sent_size,  X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 119, 200)          1831200   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 115, 128)          128128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,960,629\n",
      "Trainable params: 1,960,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=sent_size))\n",
    "cnnmodel.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(layers.GlobalMaxPooling1D())\n",
    "cnnmodel.add(layers.Dense(10, activation='relu'))\n",
    "cnnmodel.add(layers.Dense(1, activation='sigmoid'))\n",
    "cnnmodel.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Print summary of model\n",
    "print(cnnmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 3s 156ms/step - loss: 0.6918 - accuracy: 0.5083 - val_loss: 0.6865 - val_accuracy: 0.5100\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 3s 152ms/step - loss: 0.6637 - accuracy: 0.7128 - val_loss: 0.6725 - val_accuracy: 0.7200\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 3s 156ms/step - loss: 0.6032 - accuracy: 0.9222 - val_loss: 0.6335 - val_accuracy: 0.7250\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 3s 167ms/step - loss: 0.4740 - accuracy: 0.9211 - val_loss: 0.5731 - val_accuracy: 0.6950\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 3s 153ms/step - loss: 0.2630 - accuracy: 0.9517 - val_loss: 0.5637 - val_accuracy: 0.7300\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 3s 149ms/step - loss: 0.0969 - accuracy: 0.9867 - val_loss: 0.6502 - val_accuracy: 0.7250\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 3s 144ms/step - loss: 0.0297 - accuracy: 0.9978 - val_loss: 0.7831 - val_accuracy: 0.7000\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 0.0139 - accuracy: 0.9983 - val_loss: 0.9062 - val_accuracy: 0.6900\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 3s 145ms/step - loss: 0.0069 - accuracy: 0.9994 - val_loss: 0.9252 - val_accuracy: 0.6950\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 3s 160ms/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 0.9749 - val_accuracy: 0.6850\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "# history = model.fit_generator( train_generator,\n",
    "#                               steps_per_epoch=training_steps_per_epoch,\n",
    "#                               epochs=10,\n",
    "#                               validation_data=val_generator )\n",
    "\n",
    "\n",
    "history = cnnmodel.fit( X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture : RNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 119, 200)          1831200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 119, 128)          168448    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,007,969\n",
      "Trainable params: 2,007,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#deep learning library\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "\n",
    "rnnmodel=Sequential()\n",
    "\n",
    "#embedding layer\n",
    "rnnmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=sent_size))\n",
    "\n",
    "#lstm layer\n",
    "rnnmodel.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
    "\n",
    "#Global Maxpooling\n",
    "rnnmodel.add(GlobalMaxPooling1D())\n",
    "\n",
    "#Dense Layer\n",
    "rnnmodel.add(Dense(64,activation='relu')) \n",
    "rnnmodel.add(Dense(1,activation='sigmoid')) \n",
    "\n",
    "#Add loss function, metrics, optimizer\n",
    "rnnmodel.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n",
    "\n",
    "#Adding callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
    "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
    "\n",
    "#Print summary of model\n",
    "print(rnnmodel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get train & test data generator. Only for large dataset size, which can not be loaded in memeory.\n",
    "# batch_size = 200\n",
    "# training_steps_per_epoch = round(len(X_train) / batch_size)\n",
    "\n",
    "# train_generator = generator(df_train, batch_size)\n",
    "# val_generator   = generator(df_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18/18 [==============================] - 7s 389ms/step - loss: 0.6928 - acc: 0.4983 - val_loss: 0.6874 - val_acc: 0.6050\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 7s 371ms/step - loss: 0.6653 - acc: 0.7017 - val_loss: 0.6302 - val_acc: 0.6400\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 6s 330ms/step - loss: 0.4972 - acc: 0.7783 - val_loss: 0.7051 - val_acc: 0.6600\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 6s 343ms/step - loss: 0.4459 - acc: 0.7911 - val_loss: 0.7377 - val_acc: 0.6200\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 6s 349ms/step - loss: 0.4348 - acc: 0.7867 - val_loss: 0.8023 - val_acc: 0.6400\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 6s 351ms/step - loss: 0.3755 - acc: 0.8250 - val_loss: 0.8871 - val_acc: 0.6400\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 6s 346ms/step - loss: 0.2731 - acc: 0.8961 - val_loss: 0.9150 - val_acc: 0.6550\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 6s 348ms/step - loss: 0.1969 - acc: 0.9372 - val_loss: 0.9505 - val_acc: 0.6750\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 6s 339ms/step - loss: 0.1816 - acc: 0.9511 - val_loss: 0.8933 - val_acc: 0.6700\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 6s 322ms/step - loss: 0.1306 - acc: 0.9678 - val_loss: 0.9785 - val_acc: 0.6750\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "\n",
    "# history = model.fit_generator( train_generator,\n",
    "#                               steps_per_epoch=training_steps_per_epoch,\n",
    "#                               epochs=10,\n",
    "#                               validation_data=val_generator )\n",
    "\n",
    "history = rnnmodel.fit( X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of model on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred):\n",
    "    threshold=0.5\n",
    "    roc = np.round( metrics.roc_auc_score(y_val,y_pred), 2)\n",
    "    \n",
    "    y_pred1=[]\n",
    "    for i in y_pred :  \n",
    "\n",
    "        if i>threshold:\n",
    "            y_pred1.append(1)\n",
    "        else:\n",
    "            y_pred1.append(0)\n",
    "\n",
    "    acc = np.round( metrics.accuracy_score(y_val,y_pred1), 2)\n",
    "    recall = np.round( metrics.recall_score(y_val,y_pred1), 2)\n",
    "    precision = np.round( metrics.precision_score(y_val,y_pred1), 2)\n",
    "    f1 = np.round( metrics.f1_score(y_val,y_pred1), 2)\n",
    "\n",
    "    print(\"\\nAccuracy : \", acc )\n",
    "    print(\"Recall   : \", recall )\n",
    "    print(\"Precision: \", precision )\n",
    "    print(\"F1       : \", f1 )\n",
    "    print(\"ROC      : \", roc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[\"CNN\"] = list(np.reshape( cnnmodel.predict(X_val),-1))\n",
    "prediction[\"RNN\"] = list(np.reshape( rnnmodel.predict(X_val),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy :  0.68\n",
      "Recall   :  0.65\n",
      "Precision:  0.7\n",
      "F1       :  0.67\n",
      "ROC      :  0.74\n"
     ]
    }
   ],
   "source": [
    "print_metrics(prediction['CNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy :  0.68\n",
      "Recall   :  0.7\n",
      "Precision:  0.67\n",
      "F1       :  0.68\n",
      "ROC      :  0.74\n"
     ]
    }
   ],
   "source": [
    "print_metrics(prediction['RNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Transfer & CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # turn a doc into clean tokens\n",
    "# def clean_doc(doc, vocab):\n",
    "#     # split into tokens by white space\n",
    "#     tokens = doc.split()\n",
    "#     # remove punctuation from each token\n",
    "#     table = str.maketrans('', '', punctuation)\n",
    "#     tokens = [w.translate(table) for w in tokens]\n",
    "#     # filter out tokens not in vocab\n",
    "#     tokens = [w for w in tokens if w in vocab]\n",
    "#     tokens = ' '.join(tokens)\n",
    "#     return tokens\n",
    "\n",
    "# # load all docs in a directory\n",
    "# def process_docs(directory, vocab, is_trian):\n",
    "#     documents = list()\n",
    "#     # walk through all files in the folder\n",
    "#     for filename in listdir(directory):\n",
    "#         # skip any reviews in the test set\n",
    "#         if is_trian and filename.startswith('cv9'):\n",
    "#             continue\n",
    "#         if not is_trian and not filename.startswith('cv9'):\n",
    "#             continue\n",
    "#         # create the full path of the file to open\n",
    "#         path = directory + '/' + filename\n",
    "#         # load the doc\n",
    "#         doc = load_doc(path)\n",
    "#         # clean doc\n",
    "#         tokens = clean_doc(doc, vocab)\n",
    "#         # add to list\n",
    "#         documents.append(tokens)\n",
    "#     return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "dim=300\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r', encoding=\"utf8\")\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    df = pd.read_csv(filename, encoding=\"utf8\", sep=\" \")\n",
    "    embedding = dict()\n",
    "    for record in range(len(df)):\n",
    "        embedding[ df.iloc[record,0] ] = df.iloc[record,1:].to_numpy()\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):   \n",
    "    \n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, dim))\n",
    "\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    i=0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            vector = embedding[word]\n",
    "        except:\n",
    "            vector = np.zeros(dim)\n",
    "\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector.reshape(1,-1)\n",
    "        i+=1\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> CNN with ft_Wiki300 Finetuned Word Vector Embedding </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 119, 300)          2746800   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 115, 128)          192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,940,229\n",
      "Trainable params: 193,429\n",
      "Non-trainable params: 2,746,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "57/57 - 3s - loss: 0.6755 - accuracy: 0.5600\n",
      "Epoch 2/10\n",
      "57/57 - 3s - loss: 0.5454 - accuracy: 0.7761\n",
      "Epoch 3/10\n",
      "57/57 - 2s - loss: 0.3831 - accuracy: 0.8856\n",
      "Epoch 4/10\n",
      "57/57 - 2s - loss: 0.2183 - accuracy: 0.9789\n",
      "Epoch 5/10\n",
      "57/57 - 3s - loss: 0.0993 - accuracy: 0.9978\n",
      "Epoch 6/10\n",
      "57/57 - 2s - loss: 0.0467 - accuracy: 0.9978\n",
      "Epoch 7/10\n",
      "57/57 - 2s - loss: 0.0262 - accuracy: 0.9983\n",
      "Epoch 8/10\n",
      "57/57 - 3s - loss: 0.0185 - accuracy: 0.9983\n",
      "Epoch 9/10\n",
      "57/57 - 2s - loss: 0.0184 - accuracy: 0.9978\n",
      "Epoch 10/10\n",
      "57/57 - 2s - loss: 0.0108 - accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x231ca2e2be0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = modelfolder_ft + '\\wiki300_finetuned_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "filename=modelfolder_ft+ '\\wiki300_finetuned.vec'\n",
    "raw_embedding = load_embedding(filename)\n",
    "\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, dim, weights=[embedding_vectors], input_length=sent_size, trainable=False)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X_train, y_train, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.999998\n",
      "\n",
      "Accuracy :  0.65\n",
      "Recall   :  0.74\n",
      "Precision:  0.63\n",
      "F1       :  0.68\n",
      "ROC      :  0.74\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "\n",
    "prediction[\"CNN_Wiki300\"] = list(np.reshape( model.predict(X_val),-1))\n",
    "\n",
    "print_metrics(prediction['CNN_Wiki300'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> IndicFT300 Finedtuned Vector </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 119, 300)          2746800   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 115, 128)          192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,940,229\n",
      "Trainable params: 193,429\n",
      "Non-trainable params: 2,746,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "57/57 - 3s - loss: 0.6879 - accuracy: 0.5606\n",
      "Epoch 2/10\n",
      "57/57 - 3s - loss: 0.6765 - accuracy: 0.5761\n",
      "Epoch 3/10\n",
      "57/57 - 3s - loss: 0.6578 - accuracy: 0.6100\n",
      "Epoch 4/10\n",
      "57/57 - 3s - loss: 0.6366 - accuracy: 0.6428\n",
      "Epoch 5/10\n",
      "57/57 - 3s - loss: 0.5960 - accuracy: 0.6983\n",
      "Epoch 6/10\n",
      "57/57 - 3s - loss: 0.5503 - accuracy: 0.7400\n",
      "Epoch 7/10\n",
      "57/57 - 3s - loss: 0.5022 - accuracy: 0.7828\n",
      "Epoch 8/10\n",
      "57/57 - 3s - loss: 0.4506 - accuracy: 0.8167\n",
      "Epoch 9/10\n",
      "57/57 - 4s - loss: 0.4074 - accuracy: 0.8489\n",
      "Epoch 10/10\n",
      "57/57 - 3s - loss: 0.3479 - accuracy: 0.8939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x231d6134d00>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = modelfolder_ft_ind + '\\indicnlpi300_finetuned_vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "filename=modelfolder_ft_ind+ '\\\\indicnlp300_finetuned.vec'\n",
    "raw_embedding = load_embedding(filename)\n",
    "\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, dim, weights=[embedding_vectors], input_length=sent_size, trainable=False)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X_train, y_train, epochs=10, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 66.000003\n",
      "\n",
      "Accuracy :  0.66\n",
      "Recall   :  0.65\n",
      "Precision:  0.66\n",
      "F1       :  0.66\n",
      "ROC      :  0.71\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n",
    "\n",
    "prediction[\"CNN_IndicFT300\"] = list(np.reshape( model.predict(X_val),-1))\n",
    "\n",
    "print_metrics(prediction['CNN_IndicFT300'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save CNN, RNN Predictions Results to compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = pd.DataFrame(prediction, columns=[\"CNN\",\"RNN\",\"CNN_Wiki300\", \"CNN_IndicFT300\"], index=idx_val)\n",
    "df_prediction.to_csv(resultsfolder + r'\\model_predictions_NN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNN</th>\n",
       "      <th>RNN</th>\n",
       "      <th>CNN_Wiki300</th>\n",
       "      <th>CNN_IndicFT300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>0.819261</td>\n",
       "      <td>0.943548</td>\n",
       "      <td>0.920691</td>\n",
       "      <td>0.364997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>0.930264</td>\n",
       "      <td>0.941898</td>\n",
       "      <td>0.755416</td>\n",
       "      <td>0.814725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.020987</td>\n",
       "      <td>0.608887</td>\n",
       "      <td>0.390835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.908897</td>\n",
       "      <td>0.897375</td>\n",
       "      <td>0.103974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>0.002150</td>\n",
       "      <td>0.029380</td>\n",
       "      <td>0.208650</td>\n",
       "      <td>0.414263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.085806</td>\n",
       "      <td>0.005806</td>\n",
       "      <td>0.107279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8819</th>\n",
       "      <td>0.238204</td>\n",
       "      <td>0.974613</td>\n",
       "      <td>0.579415</td>\n",
       "      <td>0.884838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>0.108273</td>\n",
       "      <td>0.048052</td>\n",
       "      <td>0.826889</td>\n",
       "      <td>0.318257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>0.151846</td>\n",
       "      <td>0.030068</td>\n",
       "      <td>0.567009</td>\n",
       "      <td>0.723224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>0.945159</td>\n",
       "      <td>0.970650</td>\n",
       "      <td>0.979359</td>\n",
       "      <td>0.581800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           CNN       RNN  CNN_Wiki300  CNN_IndicFT300\n",
       "ID                                                   \n",
       "5212  0.819261  0.943548     0.920691        0.364997\n",
       "8028  0.930264  0.941898     0.755416        0.814725\n",
       "2364  0.000632  0.020987     0.608887        0.390835\n",
       "5805  0.002664  0.908897     0.897375        0.103974\n",
       "5236  0.002150  0.029380     0.208650        0.414263\n",
       "...        ...       ...          ...             ...\n",
       "7171  0.000079  0.085806     0.005806        0.107279\n",
       "8819  0.238204  0.974613     0.579415        0.884838\n",
       "2686  0.108273  0.048052     0.826889        0.318257\n",
       "8692  0.151846  0.030068     0.567009        0.723224\n",
       "8533  0.945159  0.970650     0.979359        0.581800\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy :  0.68\n",
      "Recall   :  0.7\n",
      "Precision:  0.67\n",
      "F1       :  0.68\n",
      "ROC      :  0.74\n",
      "\n",
      "Accuracy :  0.65\n",
      "Recall   :  0.74\n",
      "Precision:  0.63\n",
      "F1       :  0.68\n",
      "ROC      :  0.74\n",
      "\n",
      "Accuracy :  0.66\n",
      "Recall   :  0.65\n",
      "Precision:  0.66\n",
      "F1       :  0.66\n",
      "ROC      :  0.71\n",
      "\n",
      "Accuracy :  0.68\n",
      "Recall   :  0.65\n",
      "Precision:  0.7\n",
      "F1       :  0.67\n",
      "ROC      :  0.74\n"
     ]
    }
   ],
   "source": [
    "print_metrics(prediction['RNN'])\n",
    "print_metrics(prediction['CNN_Wiki300'])\n",
    "print_metrics(prediction['CNN_IndicFT300'])\n",
    "print_metrics(prediction['CNN'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
