{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://indicnlp.ai4bharat.org/indicft/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths required\n",
    "\n",
    "datafolder_p = r'D:\\github\\1-Projects\\Media-NLP05-SDSHL\\data\\processed'\n",
    "datafolder_e = r'D:\\github\\1-Projects\\Media-NLP05-SDSHL\\data\\external'\n",
    "datafolder_i = r'D:\\github\\1-Projects\\Media-NLP05-SDSHL\\data\\internal'\n",
    "modelfolder  = r'D:\\18-DS\\data\\models' #r'D:\\github\\1-Projects\\Media-NLP05-SDSHL\\data\\models'\n",
    "resultsfolder = r'D:\\github\\1-Projects\\Media-NLP05-SDSHL\\data\\results'\n",
    "\n",
    "\n",
    "# datafolder_p = r'D:\\18-DS\\github\\SDSHL\\data\\processed'\n",
    "# datafolder_e = r'D:\\18-DS\\github\\SDSHL\\data\\external'\n",
    "# datafolder_i = r'D:\\18-DS\\github\\SDSHL\\data\\internal'\n",
    "# modelfolder  = r'D:\\18-DS\\data\\models'\n",
    "# resultsfolder = r'D:\\18-DS\\github\\SDSHL\\data\\results'\n",
    "\n",
    "modelfolder_ft       = modelfolder + r'\\fasttext_wiki.hi'\n",
    "modelfolder_ft_ind   = modelfolder + r'\\fasttext_indicnlp.hi'\n",
    "modelfolder_ft_local   = modelfolder + r'\\fasttext_local'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\18-DS\\\\data\\\\models\\\\ai4bharat_wordvec\\\\fasttext_indicnlp.hi'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfolder_ft_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttax pretrained Hindi binary/text format vectors \n",
    "#Common Crawl and Wikipedia \n",
    "cc300 = r'\\cc.hi.300.bin'\n",
    "cc100 = r'\\cc.hi.100.bin'\n",
    "\n",
    "#wiki\n",
    "wiki100    = r'\\wiki.hi.100.bin'\n",
    "wiki300    = r'\\wiki.hi.300.bin'\n",
    "wiki300_vec= r'\\wiki.hi.300.vec'\n",
    "\n",
    "#indicnlp\n",
    "indicnlp300     = r'\\indicnlp.ft.hi.300.bin'\n",
    "indicnlp300_vec = r'\\indicnlp.ft.hi.300.vec'\n",
    "\n",
    "# FastText Model donot need dataset but filename in a specific fasttext format for training\n",
    "train_file = datafolder_p+ r'\\3-train_ft.csv'\n",
    "test_file = datafolder_p+ r'\\3-test_ft.csv'\n",
    "\n",
    "# to predict the result with ID we need this file. Because fasttext format doesnot have ID\n",
    "#train_file1 = datafolder_p+ r'\\2-train.csv'\n",
    "test_file1 = datafolder_p+ r'\\2-test.csv'\n",
    "\n",
    "#This file is needed to create fasttext sentence embedding of full dataset \n",
    "filepath_fulldata = datafolder_p+ r'\\2-Hinglish_Sarcasm_Clean.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# file = datafolder_p + r'\\2-Hinglish_Sarcasm_Clean.csv'\n",
    "# df = pd.read_csv(file, sep='\\t', index_col=\"ID\")\n",
    "# idx = list(df.index)\n",
    "\n",
    "# file =  datafolder_p+ r'\\2-train.csv' \n",
    "# df = pd.read_csv(file, sep='\\t', index_col=\"ID\")\n",
    "# idx_train = df.index\n",
    "\n",
    "# file =  datafolder_p+ r'\\2-test.csv'\n",
    "# df = pd.read_csv(file, sep='\\t', index_col=\"ID\")\n",
    "# idx_test = df.index\n",
    "\n",
    "# df = pd.read_csv(file_data, sep='\\t', header=None)\n",
    "# df['ID']=idx\n",
    "# df=df.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install pytorch\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "#!pip uninstall fasttext\n",
    "import fasttext\n",
    "from fasttext import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #C:\\Users\\admin\\AppData\\Roaming\\nltk_data.\n",
    "\n",
    "# nltk.download('brown')\n",
    "# nltk.download('gutenberg')\n",
    "# nltk.download('inaugural')\n",
    "# nltk.download('nps_chat')\n",
    "# nltk.download('webtext')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# from nltk.book import nps_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(train_file, index_col=None, header=None, sep='\\t')\n",
    "corpus = list(df[1])\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in corpus]\n",
    "df['sentence_tkn'] = tokenized_corpus\n",
    "\n",
    "#This is required to create word embedding of full dataset\n",
    "df_full =pd.read_csv(filepath_fulldata, usecols=['ID','label','sentence'], sep=\"\\t\", index_col=\"ID\")\n",
    "corpus_full = list(df_full['sentence'])\n",
    "tokenized_corpus_full = [nltk.word_tokenize(doc) for doc in corpus_full]\n",
    "df_full['sentence_tkn'] = tokenized_corpus_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load 2-train data in memory. This file is required for lookup and creating wordvec for each sentence.\n",
    "# df =pd.read_csv(train_file1, usecols=['ID','label','sentence'], sep=\"\\t\", index_col=\"ID\")\n",
    "# corpus = list(df['sentence'])\n",
    "# tokenized_corpus = [nltk.word_tokenize(doc) for doc in corpus]\n",
    "# df['sentence_tkn'] = tokenized_corpus\n",
    "\n",
    "df_test=pd.read_csv(test_file1, usecols=['ID','label','sentence'], sep=\"\\t\", index_col=\"ID\")\n",
    "y_test =np.array(df_test['label'])\n",
    "X_test=df_test['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification using fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\18-DS\\\\data\\\\models\\\\fasttext_wiki.hi'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " vectorfolder = modelfolder_ft #+ modewiki300_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# We are not using it for lang detection\n",
    "ftmodel_lang = fasttext.load_model(vectorfolder+'\\lid.176.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fasttext.FastText._FastText"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ftmodel_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('अब', ['__label__hi'], array([0.81621677], dtype=float32)),\n",
       " ('कंगना', ['__label__hi'], array([0.9968561], dtype=float32)),\n",
       " ('रनौत', ['__label__hi'], array([0.9668991], dtype=float32)),\n",
       " ('ने', ['__label__hi'], array([0.99174315], dtype=float32)),\n",
       " ('कुछ', ['__label__hi'], array([0.89612055], dtype=float32)),\n",
       " ('लोगों', ['__label__hi'], array([0.9982083], dtype=float32)),\n",
       " ('के', ['__label__su'], array([0.5360917], dtype=float32)),\n",
       " ('चेहरों', ['__label__hi'], array([0.9836988], dtype=float32)),\n",
       " ('को', ['__label__hi'], array([0.99904203], dtype=float32)),\n",
       " ('बेनकाब', ['__label__new'], array([0.5821358], dtype=float32)),\n",
       " ('करते', ['__label__mr'], array([0.8925002], dtype=float32)),\n",
       " ('हुए', ['__label__hi'], array([0.97335386], dtype=float32)),\n",
       " ('बड़े', ['__label__hi'], array([0.98123103], dtype=float32)),\n",
       " ('खुलासे', ['__label__hi'], array([0.94990045], dtype=float32)),\n",
       " ('किए', ['__label__hi'], array([0.99907106], dtype=float32)),\n",
       " ('हैं', ['__label__zh'], array([0.9499712], dtype=float32)),\n",
       " ('kangnaranaut', ['__label__en'], array([0.4462302], dtype=float32)),\n",
       " ('javedakhtar', ['__label__en'], array([0.26162758], dtype=float32)),\n",
       " ('bollywood', ['__label__en'], array([0.30014294], dtype=float32))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print('\\n',len(ftmodel_lang.words) )\n",
    "# print('\\n',ftmodel_lang.get_dimension() )\n",
    "# #print(ftmodel_lang.get_labels())\n",
    "# print('\\n',ftmodel_lang.get_line(\"hari\"))\n",
    "# print('\\n',ftmodel_lang.get_analogies('apple','orange','juice',10) )\n",
    "\n",
    "# print ('\\n', ftmodel_lang.get_subwords('Hari') )\n",
    "# #print( ftmodel_lang.get_words())\n",
    "# print ('\\n',ftmodel_lang.get_sentence_vector('Hari Thapliyal'))\n",
    "# print ('\\n',ftmodel_lang.get_subword_id('apple') )\n",
    "# print ('\\n',ftmodel_lang.get_nearest_neighbors('apple') )\n",
    "# ftmodel_lang.predict(['Thapliyal', 'हर'])\n",
    "n=27\n",
    "list(zip(tokenized_corpus[n],ftmodel_lang.predict(tokenized_corpus[n])[0],ftmodel_lang.predict(tokenized_corpus[n])[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Get Word Vector Without Pre-Training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "feature_size = 50    # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model\n",
    "\n",
    "#window_length = 200 # The amount of words we look at per example. Experiment with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FastText in module gensim.models.fasttext:\n",
      "\n",
      "class FastText(gensim.models.word2vec.Word2Vec)\n",
      " |  FastText(sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n",
      " |  \n",
      " |  Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
      " |  \n",
      " |  Warnings\n",
      " |  --------\n",
      " |  This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
      " |  such as lambda functions etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FastText\n",
      " |      gensim.models.word2vec.Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n",
      " |      Train, use and evaluate word representations learned using the method\n",
      " |      described in `Enriching Word Vectors with Subword Information <https://arxiv.org/abs/1607.04606>`_,\n",
      " |      aka FastText.\n",
      " |      \n",
      " |      The model can be stored/loaded via its :meth:`~gensim.models.fasttext.FastText.save` and\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load` methods, or loaded from a format compatible with the\n",
      " |      original Fasttext implementation via :func:`~gensim.models.fasttext.load_facebook_model`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str, optional\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus'\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such\n",
      " |          examples. If you don't supply `sentences`, the model is left uninitialized -- use if you plan to\n",
      " |          initialize it in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left\n",
      " |          uninitialized).\n",
      " |      min_count : int, optional\n",
      " |          The model ignores all words with total frequency lower than this.\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          The maximum distance between the current and predicted word within a sentence.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      sg : {1, 0}, optional\n",
      " |          Training algorithm: skip-gram if `sg=1`, otherwise CBOW.\n",
      " |      hs : {1,0}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {1,0}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during\n",
      " |          :meth:`~gensim.models.fasttext.FastText.build_vocab` and is not stored as part of themodel.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      sorted_vocab : {1,0}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      min_n : int, optional\n",
      " |          Minimum length of char n-grams to be used for training word representations.\n",
      " |      max_n : int, optional\n",
      " |          Max length of char ngrams to be used for training word representations. Set `max_n` to be\n",
      " |          lesser than `min_n` to avoid char ngrams being used.\n",
      " |      word_ngrams : int, optional\n",
      " |          In Facebook's FastText, \"max length of word ngram\" - but gensim only supports the\n",
      " |          default of 1 (regular unigram word handling).\n",
      " |      bucket : int, optional\n",
      " |          Character ngrams are hashed into a fixed number of buckets, in order to limit the\n",
      " |          memory usage of the model. This option specifies the number of buckets used by the model.\n",
      " |          The default value of 2000000 consumes as much memory as having 2000000 more in-vocabulary\n",
      " |          words in your model.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically selecting\n",
      " |          ``min_count```.  If the specified ``min_count`` is more than the\n",
      " |          automatically calculated ``min_count``, the former will be used.\n",
      " |          Set to ``None`` if not required.\n",
      " |      shrink_windows : bool, optional\n",
      " |          New in 4.1. Experimental.\n",
      " |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      " |          for each target word during training, to match the original word2vec algorithm's\n",
      " |          approximate weighting of context words by distance. Otherwise, the effective\n",
      " |          window size is always fixed to `window` words to either side.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a `FastText` model:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = FastText(sentences, min_count=1)\n",
      " |          >>> say_vector = model.wv['say']  # get vector for word\n",
      " |          >>> of_vector = model.wv['of']  # get vector for out-of-vocab word\n",
      " |      \n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      wv : :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          This object essentially contains the mapping between words and embeddings. These are similar to\n",
      " |          the embedding computed in the :class:`~gensim.models.word2vec.Word2Vec`, however here we also\n",
      " |          include vectors for n-grams. This allows the model to compute embeddings even for **unseen**\n",
      " |          words (that do not exist in the vocabulary), as the aggregate of the n-grams included in the word.\n",
      " |          After training the model, this attribute can be used directly to query those embeddings in various\n",
      " |          ways. Check the module level docstring for some examples.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate memory that will be needed to train a model, and print the estimates to log.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors. Obsoleted.\n",
      " |      \n",
      " |      If you need a single unit-normalized vector for some key, call\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      " |      ``fasttext_model.wv.get_vector(key, norm=True)``.\n",
      " |      \n",
      " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      " |          You lose information if you do this.\n",
      " |  \n",
      " |  load_binary_data(self, encoding='utf8')\n",
      " |      Load data from a binary file created by Facebook's native FastText.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      encoding : str, optional\n",
      " |          Specifies the encoding.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the Fasttext model. This saved model can be loaded again using\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load`, which supports incremental training\n",
      " |      and getting vectors for out-of-vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Store the model to this file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load`\n",
      " |          Load :class:`~gensim.models.fasttext.FastText` model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastText` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastText`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastText.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastText` model.\n",
      " |  \n",
      " |  load_fasttext_format(model_file, encoding='utf8') from builtins.type\n",
      " |      Deprecated.\n",
      " |      \n",
      " |      Use :func:`gensim.models.fasttext.load_facebook_model` or\n",
      " |      :func:`gensim.models.fasttext.load_facebook_vectors` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.word2vec.Word2Vec:\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  add_null_word(self)\n",
      " |  \n",
      " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  create_binary_tree(self)\n",
      " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      " |      word counts. Frequent words will have shorter binary codes.\n",
      " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |  \n",
      " |  make_cum_table(self, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |      \n",
      " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Note this performs a CBOW-style propagation, even in SG models,\n",
      " |      and doesn't quite weight the surrounding words the same as in\n",
      " |      training -- so it's just one crude way of using a trained model\n",
      " |      as a predictor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of (str and/or int)\n",
      " |          List of context words, which may be words themselves (str)\n",
      " |          or their index in `self.wv.vectors` (int).\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |      \n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |      \n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |  \n",
      " |  prepare_weights(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      " |      then share all vocabulary-related structures other than vectors, neither should then\n",
      " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  seeded_vector(self, seed_string, vector_size)\n",
      " |  \n",
      " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |      \n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |      \n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |      \n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |      \n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |      \n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23768/1989170435.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m ft_model = FastText(tokenized_corpus, vector_size=feature_size, \n\u001b[0;32m      5\u001b[0m                           \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_word_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                           sg=sg, sample=sample, epochs=5000)\n\u001b[0m",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\gensim\\models\\fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, sg, hs, vector_size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mnull_word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnull_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mns_exponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mns_exponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashfxn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhashfxn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             min_alpha=min_alpha, shrink_windows=shrink_windows)\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_post_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    427\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=self.compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    430\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1072\u001b[1;33m                     callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m   1073\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1430\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m   1431\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1432\u001b[1;33m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1433\u001b[0m         )\n\u001b[0;32m   1434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##THIS STEP TAKES VERY LONG TIME\n",
    "\n",
    "from gensim.models.fasttext import FastText\n",
    "ft_model = FastText(tokenized_corpus, vector_size=feature_size, \n",
    "                          window=window_context, min_count = min_word_count,\n",
    "                          sg=sg, sample=sample, epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\18-DS\\\\data\\\\models\\\\ai4bharat_wordvec\\\\fasttext_local'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelfolder_ft_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "ft_model.save(modelfolder_ft_local + r'\\model_fasttext_local.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113250</td>\n",
       "      <td>0.393049</td>\n",
       "      <td>-0.124152</td>\n",
       "      <td>-0.018442</td>\n",
       "      <td>-0.242854</td>\n",
       "      <td>-0.188267</td>\n",
       "      <td>-0.201020</td>\n",
       "      <td>0.179092</td>\n",
       "      <td>0.949274</td>\n",
       "      <td>-1.049332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116026</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.292781</td>\n",
       "      <td>-0.569737</td>\n",
       "      <td>-0.309208</td>\n",
       "      <td>-0.353282</td>\n",
       "      <td>-0.657647</td>\n",
       "      <td>0.525472</td>\n",
       "      <td>-0.536230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.094515</td>\n",
       "      <td>0.552234</td>\n",
       "      <td>-0.440714</td>\n",
       "      <td>0.428757</td>\n",
       "      <td>-0.866321</td>\n",
       "      <td>0.708462</td>\n",
       "      <td>0.140710</td>\n",
       "      <td>0.136355</td>\n",
       "      <td>0.560219</td>\n",
       "      <td>0.144039</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.658728</td>\n",
       "      <td>0.726066</td>\n",
       "      <td>-0.317365</td>\n",
       "      <td>-0.496862</td>\n",
       "      <td>-0.215485</td>\n",
       "      <td>0.250157</td>\n",
       "      <td>-0.371972</td>\n",
       "      <td>0.356819</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.282127</td>\n",
       "      <td>0.270265</td>\n",
       "      <td>0.167093</td>\n",
       "      <td>-0.262338</td>\n",
       "      <td>-0.908698</td>\n",
       "      <td>-0.280853</td>\n",
       "      <td>-0.163105</td>\n",
       "      <td>0.094803</td>\n",
       "      <td>0.604418</td>\n",
       "      <td>-0.233787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240828</td>\n",
       "      <td>0.919755</td>\n",
       "      <td>0.178243</td>\n",
       "      <td>-0.197920</td>\n",
       "      <td>0.331276</td>\n",
       "      <td>0.217927</td>\n",
       "      <td>-0.667150</td>\n",
       "      <td>-0.122560</td>\n",
       "      <td>0.645058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.123207</td>\n",
       "      <td>0.176443</td>\n",
       "      <td>-0.382284</td>\n",
       "      <td>-0.109580</td>\n",
       "      <td>-0.977572</td>\n",
       "      <td>-0.045764</td>\n",
       "      <td>-0.642126</td>\n",
       "      <td>0.485138</td>\n",
       "      <td>1.217646</td>\n",
       "      <td>1.455380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335150</td>\n",
       "      <td>0.920655</td>\n",
       "      <td>-0.210099</td>\n",
       "      <td>-0.247958</td>\n",
       "      <td>-0.737850</td>\n",
       "      <td>0.302777</td>\n",
       "      <td>-0.883255</td>\n",
       "      <td>0.923765</td>\n",
       "      <td>-0.863660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.800903</td>\n",
       "      <td>0.305093</td>\n",
       "      <td>0.647492</td>\n",
       "      <td>0.044573</td>\n",
       "      <td>-1.111414</td>\n",
       "      <td>-1.025995</td>\n",
       "      <td>0.910517</td>\n",
       "      <td>0.291303</td>\n",
       "      <td>0.323005</td>\n",
       "      <td>0.122516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014140</td>\n",
       "      <td>0.488761</td>\n",
       "      <td>1.027995</td>\n",
       "      <td>-0.048864</td>\n",
       "      <td>-0.011445</td>\n",
       "      <td>0.379882</td>\n",
       "      <td>-0.987482</td>\n",
       "      <td>-0.138530</td>\n",
       "      <td>-0.379341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>-0.524530</td>\n",
       "      <td>0.453356</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>0.665401</td>\n",
       "      <td>-0.772397</td>\n",
       "      <td>-0.360819</td>\n",
       "      <td>-0.173865</td>\n",
       "      <td>0.103056</td>\n",
       "      <td>0.981604</td>\n",
       "      <td>-0.574369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177424</td>\n",
       "      <td>0.726944</td>\n",
       "      <td>0.132383</td>\n",
       "      <td>-0.122776</td>\n",
       "      <td>-0.476988</td>\n",
       "      <td>-0.034219</td>\n",
       "      <td>0.318259</td>\n",
       "      <td>0.207669</td>\n",
       "      <td>-0.567384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>-0.982270</td>\n",
       "      <td>0.430234</td>\n",
       "      <td>-0.285464</td>\n",
       "      <td>0.047331</td>\n",
       "      <td>-0.529492</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.603319</td>\n",
       "      <td>0.068376</td>\n",
       "      <td>1.036530</td>\n",
       "      <td>-0.106622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084576</td>\n",
       "      <td>1.039707</td>\n",
       "      <td>0.568916</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>-0.364070</td>\n",
       "      <td>-0.261785</td>\n",
       "      <td>-1.055225</td>\n",
       "      <td>0.217264</td>\n",
       "      <td>-0.675432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9349</th>\n",
       "      <td>-0.490666</td>\n",
       "      <td>0.370356</td>\n",
       "      <td>0.118045</td>\n",
       "      <td>-0.011550</td>\n",
       "      <td>-0.081390</td>\n",
       "      <td>-0.306760</td>\n",
       "      <td>0.482190</td>\n",
       "      <td>0.709745</td>\n",
       "      <td>1.060151</td>\n",
       "      <td>0.126580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157165</td>\n",
       "      <td>0.172537</td>\n",
       "      <td>-0.178930</td>\n",
       "      <td>0.496653</td>\n",
       "      <td>0.122513</td>\n",
       "      <td>-0.105301</td>\n",
       "      <td>-0.777782</td>\n",
       "      <td>0.188966</td>\n",
       "      <td>0.139645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9350</th>\n",
       "      <td>-0.657282</td>\n",
       "      <td>-0.053026</td>\n",
       "      <td>-0.022728</td>\n",
       "      <td>0.626659</td>\n",
       "      <td>-0.590326</td>\n",
       "      <td>-0.215105</td>\n",
       "      <td>-0.091611</td>\n",
       "      <td>0.258552</td>\n",
       "      <td>0.873490</td>\n",
       "      <td>-0.352337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236384</td>\n",
       "      <td>0.401849</td>\n",
       "      <td>0.128313</td>\n",
       "      <td>-0.178577</td>\n",
       "      <td>-0.189429</td>\n",
       "      <td>0.053128</td>\n",
       "      <td>-0.215852</td>\n",
       "      <td>0.524360</td>\n",
       "      <td>-0.563904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9351</th>\n",
       "      <td>-0.674439</td>\n",
       "      <td>0.346410</td>\n",
       "      <td>-0.357537</td>\n",
       "      <td>0.231102</td>\n",
       "      <td>-0.495081</td>\n",
       "      <td>0.177392</td>\n",
       "      <td>-0.233204</td>\n",
       "      <td>-0.014425</td>\n",
       "      <td>0.975001</td>\n",
       "      <td>-0.191166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.541902</td>\n",
       "      <td>0.477970</td>\n",
       "      <td>0.383349</td>\n",
       "      <td>-0.546162</td>\n",
       "      <td>0.154837</td>\n",
       "      <td>-0.140200</td>\n",
       "      <td>-1.261848</td>\n",
       "      <td>-0.469225</td>\n",
       "      <td>-0.197777</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "ID                                                                           \n",
       "1     0.113250  0.393049 -0.124152 -0.018442 -0.242854 -0.188267 -0.201020   \n",
       "5    -0.094515  0.552234 -0.440714  0.428757 -0.866321  0.708462  0.140710   \n",
       "12   -0.282127  0.270265  0.167093 -0.262338 -0.908698 -0.280853 -0.163105   \n",
       "13    0.123207  0.176443 -0.382284 -0.109580 -0.977572 -0.045764 -0.642126   \n",
       "26   -0.800903  0.305093  0.647492  0.044573 -1.111414 -1.025995  0.910517   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9347 -0.524530  0.453356  0.021641  0.665401 -0.772397 -0.360819 -0.173865   \n",
       "9348 -0.982270  0.430234 -0.285464  0.047331 -0.529492  0.000563  0.603319   \n",
       "9349 -0.490666  0.370356  0.118045 -0.011550 -0.081390 -0.306760  0.482190   \n",
       "9350 -0.657282 -0.053026 -0.022728  0.626659 -0.590326 -0.215105 -0.091611   \n",
       "9351 -0.674439  0.346410 -0.357537  0.231102 -0.495081  0.177392 -0.233204   \n",
       "\n",
       "             7         8         9  ...        41        42        43  \\\n",
       "ID                                  ...                                 \n",
       "1     0.179092  0.949274 -1.049332  ... -0.116026  0.191881  0.292781   \n",
       "5     0.136355  0.560219  0.144039  ... -0.658728  0.726066 -0.317365   \n",
       "12    0.094803  0.604418 -0.233787  ...  0.240828  0.919755  0.178243   \n",
       "13    0.485138  1.217646  1.455380  ... -0.335150  0.920655 -0.210099   \n",
       "26    0.291303  0.323005  0.122516  ...  0.014140  0.488761  1.027995   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9347  0.103056  0.981604 -0.574369  ... -0.177424  0.726944  0.132383   \n",
       "9348  0.068376  1.036530 -0.106622  ... -0.084576  1.039707  0.568916   \n",
       "9349  0.709745  1.060151  0.126580  ... -0.157165  0.172537 -0.178930   \n",
       "9350  0.258552  0.873490 -0.352337  ... -0.236384  0.401849  0.128313   \n",
       "9351 -0.014425  0.975001 -0.191166  ... -0.541902  0.477970  0.383349   \n",
       "\n",
       "            44        45        46        47        48        49  label  \n",
       "ID                                                                       \n",
       "1    -0.569737 -0.309208 -0.353282 -0.657647  0.525472 -0.536230      1  \n",
       "5    -0.496862 -0.215485  0.250157 -0.371972  0.356819 -0.034821      1  \n",
       "12   -0.197920  0.331276  0.217927 -0.667150 -0.122560  0.645058      1  \n",
       "13   -0.247958 -0.737850  0.302777 -0.883255  0.923765 -0.863660      1  \n",
       "26   -0.048864 -0.011445  0.379882 -0.987482 -0.138530 -0.379341      1  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "9347 -0.122776 -0.476988 -0.034219  0.318259  0.207669 -0.567384      1  \n",
       "9348  0.003099 -0.364070 -0.261785 -1.055225  0.217264 -0.675432      1  \n",
       "9349  0.496653  0.122513 -0.105301 -0.777782  0.188966  0.139645      1  \n",
       "9350 -0.178577 -0.189429  0.053128 -0.215852  0.524360 -0.563904      1  \n",
       "9351 -0.546162  0.154837 -0.140200 -1.261848 -0.469225 -0.197777      1  \n",
       "\n",
       "[2000 rows x 51 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ft_tnseEmbedded, ft_words, ft_wvs, ft_labels = getTNSE_Embedded(ft_model)\n",
    "\n",
    "# get document level embeddings\n",
    "ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus_full, model=ft_model,\n",
    "                                             num_features=feature_size)\n",
    "\n",
    "ft_features = pd.DataFrame(ft_doc_features, index=df_full.index)\n",
    "ft_features['label']=df_full.label\n",
    "ft_features.to_csv(datafolder_p + r\"\\embedding_ft_local.csv\")\n",
    "ft_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Using Pretrained Vector</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compress large size pretrained binary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load cc.hi.300.bin \n",
    "# ft300 = fasttext.load_model(modelfolder_ft+ r'\\cc.hi.300.bin')\n",
    "# ft300.get_dimension()\n",
    "\n",
    "# #reduce dimension from 300 to 100\n",
    "# import fasttext.util\n",
    "# fasttext.util.reduce_model(ft300, 100)\n",
    "# ft300.get_dimension()\n",
    "\n",
    "# #save the model with reduced dimension\n",
    "# ft300.save_model(vectorfolder+'\\cc.hi.100.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Load a pretrained model in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftmodel_cc300 = fasttext.load_model(modelfolder_ft+cc300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model file name:'+modelfolder_ft+cc300)\n",
    "#print (ftmodel_cc300.get_dimension(), len(ftmodel_cc300.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftmodel_cc100 = fasttext.load_model(modelfolder_ft+cc100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('file name:'+modelfolder_ft+cc100)\n",
    "#print (ftmodel_cc100.get_dimension(), len(ftmodel_cc100.words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftmodel_indnlp300 = fasttext.load_model(modelfolder_ft_ind+indicnlp300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model file name:'+modelfolder_ft_ind+indicnlp300)\n",
    "#print (ftmodel_indnlp300.get_dimension(), len(ftmodel_indnlp300.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Read details of Pretrained Vector of Text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttest wiki indicnlp vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n",
    "dict_ftvec=load_vectors(modelfolder_ft+wiki300_vec)\n",
    "print ( len(dict_ftvec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Create model from pretrained-vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>wiki300_vec<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\18-DS\\github\\SDSHL\\data\\processed\\3-train_ft.csv\n"
     ]
    }
   ],
   "source": [
    "#used pretrained vector to get the word embedding. This model can also be used for prediction\n",
    "#df should be in fastText format\n",
    "print (train_file)\n",
    "ftmodel_wiki300_vec = fasttext.train_supervised(train_file,dim=300,  \\\n",
    "                                     lr=0.5, epoch=25, wordNgrams=2, bucket=200000,\\\n",
    "                                     pretrainedVectors=modelfolder_ft + wiki300_vec)\n",
    "\n",
    "#loss='ova'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159765"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ftmodel_wiki300_vec.words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki300_vec has 158016 words - Method 3\n",
      "ftmodel_wiki300_vec - a finetuned model from wiki300_vec vector has  159765 word - Method 4\n",
      "ftmodel_vec300 model is created from existing vector\n"
     ]
    }
   ],
   "source": [
    "print ( 'wiki300_vec has 158016 words - Method 3')\n",
    "print ( 'ftmodel_wiki300_vec - a finetuned model from wiki300_vec vector has  159765 word - Method 4')\n",
    "print ( 'ftmodel_vec300 model is created from existing vector')\n",
    "#It means 160659-158016 are the new words in the fine tuned model as done above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red> Create FINETUNED Wordvector - ftmodel_wiki300_vec- Needed for CNN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec={}\n",
    "for word in ftmodel_wiki300_vec.words:\n",
    "    wordvec[word] = ftmodel_wiki300_vec.get_word_vector( word )\n",
    "\n",
    "#create dataframe    \n",
    "vec=pd.DataFrame(wordvec.values(), index=wordvec.keys())\n",
    "\n",
    "#save finetued vector\n",
    "vec.to_csv(modelfolder_ft + '\\wiki300_finetuned.vec', header=None, sep=\" \", encoding=\"utf-8\")\n",
    "\n",
    "#create vocabulary\n",
    "vocab = vec.index\n",
    "\n",
    "pd.DataFrame(vocab).to_csv(modelfolder_ft + '\\wiki300_finetuned_vocab.txt', index=False, header=None, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>Indicnlp300_vec<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "#r=requests.get('https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/embedding-v2/indicnlp.ft.hi.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "D:\\github\\1-Projects\\Media-NLP05-SDSHL\\data\\models\\fasttext_indicnlp.hi\\indicnlp.ft.hi.300.vec cannot be opened for loading!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23768/2378971659.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m ftmodel_indicnlp300_vec = fasttext.train_supervised(train_file,dim=300, minn=2, maxn=5, \\\n\u001b[0;32m      5\u001b[0m                                      \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwordNgrams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                                      pretrainedVectors=modelfolder_ft_ind + indicnlp300_vec)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#loss='ova'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\fasttext\\FastText.py\u001b[0m in \u001b[0;36mtrain_supervised\u001b[1;34m(*kargs, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanually_set_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[0mft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m     \u001b[0mfasttext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m     \u001b[0mft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: D:\\github\\1-Projects\\Media-NLP05-SDSHL\\data\\models\\fasttext_indicnlp.hi\\indicnlp.ft.hi.300.vec cannot be opened for loading!"
     ]
    }
   ],
   "source": [
    "#used pretrained vector to get the word embedding. This model can also be used for prediction\n",
    "#df should be in fastText format\n",
    "D:\\18-DS\\data\\models\\ai4bharat_wordvec\\indicnlp.v1.hi.vec\n",
    "\n",
    "ftmodel_indicnlp300_vec = fasttext.train_supervised(train_file,dim=300, minn=2, maxn=5, \\\n",
    "                                     lr=0.5, epoch=25, wordNgrams=2, bucket=200000,\\\n",
    "                                     pretrainedVectors=modelfolder_ft_ind + indicnlp300_vec)\n",
    "#loss='ova'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> Create finetune word vector - indicnlp300_vec- Needed for CNN<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ftmodel_indicnlp300_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23768/2979420646.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwordvec1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mftmodel_indicnlp300_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mwordvec1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mftmodel_indicnlp300_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_word_vector\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#create dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ftmodel_indicnlp300_vec' is not defined"
     ]
    }
   ],
   "source": [
    "wordvec1={}\n",
    "for word in ftmodel_indicnlp300_vec.words:\n",
    "    wordvec1[word] = ftmodel_indicnlp300_vec.get_word_vector( word )\n",
    "\n",
    "#create dataframe    \n",
    "vec=pd.DataFrame(wordvec1.values(), index=wordvec1.keys())\n",
    "\n",
    "#save finetued vector\n",
    "vec.to_csv(modelfolder_ft_ind + '\\indicnlp300_finetuned.vec', header=None, sep=\" \", encoding=\"utf-8\")\n",
    "\n",
    "#create vocabulary\n",
    "vocab = vec.index\n",
    "\n",
    "pd.DataFrame(vocab).to_csv(modelfolder_ft_ind + '\\indicnlpi300_finetuned_vocab.txt', index=False, header=None, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328169"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ftmodel_indicnlp300_vec.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \\****Indicnlp based model has more words than wiki based model\\****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Performance of models </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> Performance of ftmodel_wiki300_vec model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 0.9994444444444445, 0.9994444444444445)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftmodel_wiki300_vec.test(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_for_test(model):\n",
    "    y_pred=[]\n",
    "    for i in range(len(X_test)):\n",
    "        p=model.predict(X_test.iloc[i])\n",
    "        x1 = int(p[0][-1][-1])\n",
    "        x2 = p[1][0]\n",
    "        if x1==0: x2= 1-x2\n",
    "        y_pred.append( x2 )\n",
    "    return y_pred    \n",
    "\n",
    "\n",
    "def print_matrics(y_pred):\n",
    "    threshold=0.5\n",
    "    y_pred1=[]\n",
    "    for i in y_pred :\n",
    "        if i>threshold:\n",
    "            y_pred1.append(1)\n",
    "        else:\n",
    "            y_pred1.append(0)\n",
    "\n",
    "    acc = np.round( metrics.accuracy_score(y_test,y_pred1), 2)\n",
    "    recall = np.round( metrics.recall_score(y_test,y_pred1), 2)\n",
    "    precision = np.round( metrics.precision_score(y_test,y_pred1), 2)\n",
    "    f1 = np.round( metrics.f1_score(y_test,y_pred1), 2)\n",
    "    roc = np.round( metrics.roc_auc_score(y_test,y_pred1), 2)\n",
    "\n",
    "    print(\"Accuracy : \", acc )\n",
    "    print(\"Recall   : \", recall )\n",
    "    print(\"Precision: \", precision )\n",
    "    print(\"F1       : \", f1 )\n",
    "    print(\"ROC      : \", roc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.76\n",
      "Recall   :  0.71\n",
      "Precision:  0.79\n",
      "F1       :  0.75\n",
      "ROC      :  0.76\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_for_test(ftmodel_wiki300_vec)\n",
    "prediction['fastTextWiki_TT'] = y_pred\n",
    "print_matrics(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> Performance of ftmodel_indicnlp300_vec </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 0.9994444444444445, 0.9994444444444445)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftmodel_indicnlp300_vec.test(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 0.745, 0.745)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftmodel_indicnlp300_vec.test(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.74\n",
      "Recall   :  0.71\n",
      "Precision:  0.76\n",
      "F1       :  0.74\n",
      "ROC      :  0.75\n"
     ]
    }
   ],
   "source": [
    "y_pred = pred_for_test(ftmodel_indicnlp300_vec)\n",
    "prediction['fastTextIndicft_TT'] = y_pred\n",
    "print_matrics(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> Save Model Results of both models </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fasttext_wiki_pretrained</th>\n",
       "      <th>fasttext_indicft_pretrained</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>0.207967</td>\n",
       "      <td>0.009818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>0.932400</td>\n",
       "      <td>0.956647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>0.043242</td>\n",
       "      <td>0.074351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>0.022448</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.001730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8819</th>\n",
       "      <td>0.726941</td>\n",
       "      <td>0.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2686</th>\n",
       "      <td>0.031220</td>\n",
       "      <td>-0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>0.056129</td>\n",
       "      <td>0.004203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>0.998793</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fasttext_wiki_pretrained  fasttext_indicft_pretrained\n",
       "ID                                                         \n",
       "5212                  0.207967                     0.009818\n",
       "8028                  0.932400                     0.956647\n",
       "2364                  0.043242                     0.074351\n",
       "5805                  0.003770                     0.001026\n",
       "5236                  0.022448                     0.000071\n",
       "...                        ...                          ...\n",
       "7171                 -0.000010                     0.001730\n",
       "8819                  0.726941                     0.999600\n",
       "2686                  0.031220                    -0.000009\n",
       "8692                  0.056129                     0.004203\n",
       "8533                  0.998793                     1.000000\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction= pd.DataFrame(prediction, index=df_test.index)\n",
    "df_prediction.to_csv(resultsfolder+ r'\\model_predictions_fasttext.csv')\n",
    "df_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Convert Sentences to Vector & save Embedding</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use ftmodel_cc100 to generate senetence vector. This vector later on can be \\\n",
    "#used in classical ML models for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_vec(n_features,df,model ):\n",
    "\n",
    "    sent_all_vec = np.zeros((len(df), n_features), dtype='float32')\n",
    "    i=0\n",
    "    for row in np.array(df):\n",
    "        sent_vec=np.zeros(n_features)\n",
    "        j=0\n",
    "        for word in row:\n",
    "            sent_vec=sent_vec+ model.get_word_vector(word).astype('float32') \n",
    "            j+=1\n",
    "        sent_vec = sent_vec/j\n",
    "        sent_all_vec[i]=sent_vec\n",
    "        i+=1\n",
    "    return sent_all_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Embedding of ftmodel_wiki300_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentence to vector using pretrained fasttext\n",
    "df1 = pd.DataFrame( df_to_vec(300, df_full['sentence_tkn'], ftmodel_wiki300_vec), index=df_full.index)\n",
    "df1['label'] = df_full['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(datafolder_p+r'\\embedding_ft300_wiki_pretrained.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Embedding of ftmodel_indicnlp300_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentence to vector using pretrained fasttext\n",
    "df1 = pd.DataFrame( df_to_vec(300, df_full['sentence_tkn'], ftmodel_indicnlp300_vec), index=df_full.index)\n",
    "df1['label'] = df_full['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(datafolder_p+r'\\embedding_ft300_indicft_pretrained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 5: How to cerate a new model using our corpus. This can be used as a pretrained model for other NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_datafile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23768/573288855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Skipgram model :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m ftmodel_usup = fasttext.train_unsupervised(train_datafile, model='skipgram', dim=300, minn=2, maxn=5,\\\n\u001b[0m\u001b[0;32m      4\u001b[0m                                      lr=0.5, epoch=5, wordNgrams=2, bucket=200000)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_datafile' is not defined"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "# Skipgram model :\n",
    "train_datafile=\n",
    "ftmodel_usup = fasttext.train_unsupervised(train_datafile, model='skipgram', dim=300, minn=2, maxn=5,\\\n",
    "                                     lr=0.5, epoch=5, wordNgrams=2, bucket=200000)\n",
    "\n",
    "# or, cbow model :\n",
    "#model = fasttext.train_unsupervised('data.txt', model='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(model.words)) \n",
    "#print (model.words)\n",
    "ftmodel_usup.save_model(\"sdsms.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved & trained model.\n",
    "ftmodel_usup = fasttext.load_model(\"sdsms.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ftmodel_usup.get_dimension())\n",
    "print(ftmodel_usup.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus, model=ftmodel_usup,\n",
    "                                             num_features=feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftmodel_vec300.predict('हिंसा प्रति हिंसा को रोकना दुरुह कार्य है.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "# vecfile_name= vectorfolder + '\\wiki.hi.vec'\n",
    "\n",
    "# fin = io.open(vecfile_name, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "# data = {}\n",
    "# for line in fin:\n",
    "#     tokens = line.rstrip().split(' ')\n",
    "#     data[tokens[0]] = map(float, tokens[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a in data[\"माता\"]:\n",
    "#     print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "#help(fasttext.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.get_line('I am hari')\n",
    "\n",
    "# test.get_sentence_vector('Hari Thapliyal')\n",
    "\n",
    "#test.get_input_vector(1000000)\n",
    "\n",
    "#test.get_subwords('Hari Thapliyal')\n",
    "\n",
    "test.get_label_id('orange')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n',len(test.words) )\n",
    "# print('\\n',test.get_dimension() )\n",
    "# #print(test.get_labels())\n",
    "# print('\\n',test.get_line(\"hari\"))\n",
    "# print('\\n',test.get_analogies('apple','orange','juice',10) )\n",
    "\n",
    "# print ('\\n', test.get_subwords('Hari') )\n",
    "# #print( test.get_words())\n",
    "# print ('\\n',test.get_sentence_vector('Hari Thapliyal'))\n",
    "# print ('\\n',test.get_subword_id('apple') )\n",
    "# print ('\\n',test.get_nearest_neighbors('apple') )\n",
    "# test.predict(['Thapliyal', 'हर'])\n",
    "n=1000\n",
    "list(zip(tokenized_corpus[n],test.predict(tokenized_corpus[n])[0],test.predict(tokenized_corpus[n])[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array(tokenized_corpus)\n",
    "a.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for c  in a:\n",
    "    b.append( \" \".join(c))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=set(\"\".join(b).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_corpus)\n",
    "tokenized_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.array(a)\n",
    "b.shape\n",
    "c= set( list(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext.FastText(tokenized_corpus)\n",
    "\n",
    "test= supervised(tokenized_corpus, size=feature_size, \n",
    "                              window=window_context, min_count = min_word_count,\n",
    "                              sg=sg, sample=sample, iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.sys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
