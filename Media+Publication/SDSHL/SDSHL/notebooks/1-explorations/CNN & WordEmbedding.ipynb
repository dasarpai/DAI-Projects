{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspiration\n",
    "# https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder           = r'D:\\18-DS\\data\\Datasets_nlp\\IMDB_Polarity_data\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n",
      "25767\n",
      "25767\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    " \n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_trian and filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_trian and not filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    " \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs(datafolder+'txt_sentoken/neg', vocab, True)\n",
    "process_docs(datafolder+'txt_sentoken/pos', vocab, True)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "\n",
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "\t# convert lines to a single blob of text\n",
    "\tdata = '\\n'.join(lines)\n",
    "\t# open file\n",
    "\tfile = open(filename, 'w')\n",
    "\t# write text\n",
    "\tfile.write(data)\n",
    "\t# close file\n",
    "\tfile.close()\n",
    " \n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# filter out tokens not in vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "\tdocuments = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_trian and filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_trian and not filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load the doc\n",
    "\t\tdoc = load_doc(path)\n",
    "\t\t# clean doc\n",
    "\t\ttokens = clean_doc(doc, vocab)\n",
    "\t\t# add to list\n",
    "\t\tdocuments.append(tokens)\n",
    "\treturn documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1317, 100)         2576800   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1310, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 655, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20960)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                209610    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,812,053\n",
      "Trainable params: 2,812,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "57/57 - 11s - loss: 0.6886 - accuracy: 0.5211\n",
      "Epoch 2/10\n",
      "57/57 - 11s - loss: 0.5553 - accuracy: 0.7050\n",
      "Epoch 3/10\n",
      "57/57 - 10s - loss: 0.1915 - accuracy: 0.9567\n",
      "Epoch 4/10\n",
      "57/57 - 11s - loss: 0.0190 - accuracy: 0.9956\n",
      "Epoch 5/10\n",
      "57/57 - 10s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "57/57 - 12s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "57/57 - 12s - loss: 8.8430e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 12s - loss: 6.8971e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 11s - loss: 5.5892e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 11s - loss: 4.6440e-04 - accuracy: 1.0000\n",
      "Test Accuracy: 88.999999\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# load all training reviews\n",
    "positive_docs = process_docs(datafolder + 'txt_sentoken/pos', vocab, True)\n",
    "negative_docs = process_docs(datafolder + 'txt_sentoken/neg', vocab, True)\n",
    "train_docs = negative_docs + positive_docs\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs(datafolder+ 'txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_docs(datafolder + 'txt_sentoken/neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  25,   16,  899, ...,    0,    0,    0],\n",
       "       [ 581, 5670,  900, ...,    0,    0,    0],\n",
       "       [  30,    4,   21, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 124, 1043, 4061, ...,    0,    0,    0],\n",
       "       [ 962,  357,    2, ...,    0,    0,    0],\n",
       "       [1980,  616,  280, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 58109\n",
      "Vocabulary size: 25767\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "\tclean_lines = list()\n",
    "\tlines = doc.splitlines()\n",
    "\tfor line in lines:\n",
    "\t\t# split into tokens by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# remove punctuation from each token\n",
    "\t\ttable = str.maketrans('', '', punctuation)\n",
    "\t\ttokens = [w.translate(table) for w in tokens]\n",
    "\t\t# filter out tokens not in vocab\n",
    "\t\ttokens = [w for w in tokens if w in vocab]\n",
    "\t\tclean_lines.append(tokens)\n",
    "\treturn clean_lines\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "\tlines = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_trian and filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_trian and not filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load and clean the doc\n",
    "\t\tdoc = load_doc(path)\n",
    "\t\tdoc_lines = doc_to_clean_lines(doc, vocab)\n",
    "\t\t# add lines to list\n",
    "\t\tlines += doc_lines\n",
    "\treturn lines\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# load training data\n",
    "positive_docs = process_docs(datafolder+'txt_sentoken/pos', vocab, True)\n",
    "negative_docs = process_docs(datafolder+'txt_sentoken/neg', vocab, True)\n",
    "sentences = negative_docs + positive_docs\n",
    "print('Total training sentences: %d' % len(sentences))\n",
    "\n",
    "# train word2vec model\n",
    "model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "\n",
    "# save model in ASCII (word2vec) format\n",
    "filename = 'embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['critique',\n",
       " 'mindfuck',\n",
       " 'movie',\n",
       " 'teen',\n",
       " 'generation',\n",
       " 'touches',\n",
       " 'cool',\n",
       " 'idea',\n",
       " 'presents',\n",
       " 'bad',\n",
       " 'package']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(sentences)[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained word2vec Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel_folder = r'D:\\18-DS\\data\\models\\glove.6B\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1317, 100)         2576800   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1313, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 656, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 83968)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 83969     \n",
      "=================================================================\n",
      "Total params: 2,724,897\n",
      "Trainable params: 148,097\n",
      "Non-trainable params: 2,576,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "57/57 - 10s - loss: 0.8276 - accuracy: 0.5050\n",
      "Epoch 2/10\n",
      "57/57 - 10s - loss: 0.6028 - accuracy: 0.7489\n",
      "Epoch 3/10\n",
      "57/57 - 10s - loss: 0.4026 - accuracy: 0.8322\n",
      "Epoch 4/10\n",
      "57/57 - 10s - loss: 0.2140 - accuracy: 0.9489\n",
      "Epoch 5/10\n",
      "57/57 - 11s - loss: 0.1027 - accuracy: 0.9950\n",
      "Epoch 6/10\n",
      "57/57 - 11s - loss: 0.0528 - accuracy: 0.9989\n",
      "Epoch 7/10\n",
      "57/57 - 10s - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "57/57 - 10s - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "57/57 - 10s - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "57/57 - 10s - loss: 0.0078 - accuracy: 1.0000\n",
      "Test Accuracy: 74.000001\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# filter out tokens not in vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "\tdocuments = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_trian and filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_trian and not filename.startswith('cv9'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load the doc\n",
    "\t\tdoc = load_doc(path)\n",
    "\t\t# clean doc\n",
    "\t\ttokens = clean_doc(doc, vocab)\n",
    "\t\t# add to list\n",
    "\t\tdocuments.append(tokens)\n",
    "\treturn documents\n",
    "\n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "\t# load embedding into memory, skip first line\n",
    "\tfile = open(filename,'r', encoding=\"utf8\")\n",
    "\tlines = file.readlines()\n",
    "\tfile.close()\n",
    "\t# create a map of words to vectors\n",
    "\tembedding = dict()\n",
    "\tfor line in lines:\n",
    "\t\tparts = line.split()\n",
    "\t\t# key is string word, value is numpy array for vector\n",
    "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "\treturn embedding\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "\t# total vocabulary size plus 0 for unknown words\n",
    "\tvocab_size = len(vocab) + 1\n",
    "\t# define weight matrix dimensions with all 0\n",
    "\tweight_matrix = zeros((vocab_size, 100))\n",
    "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
    "\tfor word, i in vocab.items():\n",
    "\t\tvector = embedding.get(word)\n",
    "\t\tif vector is not None:\n",
    "\t\t\tweight_matrix[i] = vector\n",
    "\treturn weight_matrix\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# load all training reviews\n",
    "positive_docs = process_docs(datafolder+'txt_sentoken/pos', vocab, True)\n",
    "negative_docs = process_docs(datafolder +'txt_sentoken/neg', vocab, True)\n",
    "train_docs = negative_docs + positive_docs\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs(datafolder + 'txt_sentoken/pos', vocab, False)\n",
    "negative_docs = process_docs(datafolder + 'txt_sentoken/neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding(w2vmodel_folder+ 'glove.6B.100d.txt') \n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25768, 1317)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1317)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25768"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'film': 1,\n",
       " 'one': 2,\n",
       " 'movie': 3,\n",
       " 'like': 4,\n",
       " 'even': 5,\n",
       " 'good': 6,\n",
       " 'time': 7,\n",
       " 'story': 8,\n",
       " 'films': 9,\n",
       " 'would': 10,\n",
       " 'much': 11,\n",
       " 'also': 12,\n",
       " 'characters': 13,\n",
       " 'get': 14,\n",
       " 'character': 15,\n",
       " 'two': 16,\n",
       " 'first': 17,\n",
       " 'see': 18,\n",
       " 'way': 19,\n",
       " 'well': 20,\n",
       " 'make': 21,\n",
       " 'really': 22,\n",
       " 'little': 23,\n",
       " 'life': 24,\n",
       " 'plot': 25,\n",
       " 'people': 26,\n",
       " 'bad': 27,\n",
       " 'could': 28,\n",
       " 'scene': 29,\n",
       " 'movies': 30,\n",
       " 'never': 31,\n",
       " 'best': 32,\n",
       " 'new': 33,\n",
       " 'scenes': 34,\n",
       " 'man': 35,\n",
       " 'many': 36,\n",
       " 'doesnt': 37,\n",
       " 'know': 38,\n",
       " 'dont': 39,\n",
       " 'hes': 40,\n",
       " 'great': 41,\n",
       " 'another': 42,\n",
       " 'action': 43,\n",
       " 'love': 44,\n",
       " 'us': 45,\n",
       " 'go': 46,\n",
       " 'director': 47,\n",
       " 'end': 48,\n",
       " 'something': 49,\n",
       " 'still': 50,\n",
       " 'seems': 51,\n",
       " 'back': 52,\n",
       " 'made': 53,\n",
       " 'work': 54,\n",
       " 'theres': 55,\n",
       " 'makes': 56,\n",
       " 'however': 57,\n",
       " 'years': 58,\n",
       " 'world': 59,\n",
       " 'every': 60,\n",
       " 'big': 61,\n",
       " 'though': 62,\n",
       " 'better': 63,\n",
       " 'enough': 64,\n",
       " 'take': 65,\n",
       " 'around': 66,\n",
       " 'seen': 67,\n",
       " 'performance': 68,\n",
       " 'real': 69,\n",
       " 'role': 70,\n",
       " 'going': 71,\n",
       " 'audience': 72,\n",
       " 'gets': 73,\n",
       " 'isnt': 74,\n",
       " 'may': 75,\n",
       " 'think': 76,\n",
       " 'things': 77,\n",
       " 'actually': 78,\n",
       " 'look': 79,\n",
       " 'last': 80,\n",
       " 'comedy': 81,\n",
       " 'funny': 82,\n",
       " 'almost': 83,\n",
       " 'fact': 84,\n",
       " 'played': 85,\n",
       " 'thing': 86,\n",
       " 'nothing': 87,\n",
       " 'say': 88,\n",
       " 'although': 89,\n",
       " 'right': 90,\n",
       " 'thats': 91,\n",
       " 'since': 92,\n",
       " 'come': 93,\n",
       " 'find': 94,\n",
       " 'script': 95,\n",
       " 'plays': 96,\n",
       " 'long': 97,\n",
       " 'cast': 98,\n",
       " 'john': 99,\n",
       " 'old': 100,\n",
       " 'ever': 101,\n",
       " 'comes': 102,\n",
       " 'young': 103,\n",
       " 'without': 104,\n",
       " 'actors': 105,\n",
       " 'show': 106,\n",
       " 'part': 107,\n",
       " 'least': 108,\n",
       " 'lot': 109,\n",
       " 'takes': 110,\n",
       " 'acting': 111,\n",
       " 'original': 112,\n",
       " 'point': 113,\n",
       " 'away': 114,\n",
       " 'star': 115,\n",
       " 'goes': 116,\n",
       " 'quite': 117,\n",
       " 'course': 118,\n",
       " 'might': 119,\n",
       " 'family': 120,\n",
       " 'cant': 121,\n",
       " 'minutes': 122,\n",
       " 'three': 123,\n",
       " 'must': 124,\n",
       " 'im': 125,\n",
       " 'rather': 126,\n",
       " 'place': 127,\n",
       " 'interesting': 128,\n",
       " 'anything': 129,\n",
       " 'screen': 130,\n",
       " 'effects': 131,\n",
       " 'guy': 132,\n",
       " 'far': 133,\n",
       " 'day': 134,\n",
       " 'yet': 135,\n",
       " 'watch': 136,\n",
       " 'didnt': 137,\n",
       " 'seem': 138,\n",
       " 'year': 139,\n",
       " 'times': 140,\n",
       " 'instead': 141,\n",
       " 'sense': 142,\n",
       " 'always': 143,\n",
       " 'fun': 144,\n",
       " 'picture': 145,\n",
       " 'special': 146,\n",
       " 'give': 147,\n",
       " 'home': 148,\n",
       " 'making': 149,\n",
       " 'trying': 150,\n",
       " 'bit': 151,\n",
       " 'kind': 152,\n",
       " 'job': 153,\n",
       " 'want': 154,\n",
       " 'wife': 155,\n",
       " 'series': 156,\n",
       " 'american': 157,\n",
       " 'becomes': 158,\n",
       " 'pretty': 159,\n",
       " 'along': 160,\n",
       " 'set': 161,\n",
       " 'men': 162,\n",
       " 'together': 163,\n",
       " 'help': 164,\n",
       " 'probably': 165,\n",
       " 'become': 166,\n",
       " 'woman': 167,\n",
       " 'actor': 168,\n",
       " 'everything': 169,\n",
       " 'hard': 170,\n",
       " 'hollywood': 171,\n",
       " 'money': 172,\n",
       " 'given': 173,\n",
       " 'gives': 174,\n",
       " 'dialogue': 175,\n",
       " 'whole': 176,\n",
       " 'sure': 177,\n",
       " 'high': 178,\n",
       " 'black': 179,\n",
       " 'watching': 180,\n",
       " 'wants': 181,\n",
       " 'got': 182,\n",
       " 'death': 183,\n",
       " 'music': 184,\n",
       " 'feel': 185,\n",
       " 'perhaps': 186,\n",
       " 'moments': 187,\n",
       " 'next': 188,\n",
       " 'play': 189,\n",
       " 'especially': 190,\n",
       " 'less': 191,\n",
       " 'done': 192,\n",
       " 'everyone': 193,\n",
       " 'james': 194,\n",
       " 'different': 195,\n",
       " 'city': 196,\n",
       " 'looks': 197,\n",
       " 'sex': 198,\n",
       " 'simply': 199,\n",
       " 'completely': 200,\n",
       " 'whose': 201,\n",
       " 'reason': 202,\n",
       " 'shows': 203,\n",
       " 'friends': 204,\n",
       " 'horror': 205,\n",
       " 'rest': 206,\n",
       " 'performances': 207,\n",
       " 'case': 208,\n",
       " 'father': 209,\n",
       " 'left': 210,\n",
       " 'theyre': 211,\n",
       " 'several': 212,\n",
       " 'couple': 213,\n",
       " 'entire': 214,\n",
       " 'looking': 215,\n",
       " 'anyone': 216,\n",
       " 'put': 217,\n",
       " 'mind': 218,\n",
       " 'evil': 219,\n",
       " 'ending': 220,\n",
       " 'humor': 221,\n",
       " 'lost': 222,\n",
       " 'getting': 223,\n",
       " 'small': 224,\n",
       " 'michael': 225,\n",
       " 'problem': 226,\n",
       " 'shes': 227,\n",
       " 'night': 228,\n",
       " 'line': 229,\n",
       " 'true': 230,\n",
       " 'girl': 231,\n",
       " 'human': 232,\n",
       " 'main': 233,\n",
       " 'begins': 234,\n",
       " 'turns': 235,\n",
       " 'use': 236,\n",
       " 'found': 237,\n",
       " 'half': 238,\n",
       " 'stars': 239,\n",
       " 'ive': 240,\n",
       " 'either': 241,\n",
       " 'later': 242,\n",
       " 'thought': 243,\n",
       " 'alien': 244,\n",
       " 'soon': 245,\n",
       " 'based': 246,\n",
       " 'town': 247,\n",
       " 'someone': 248,\n",
       " 'name': 249,\n",
       " 'comic': 250,\n",
       " 'certainly': 251,\n",
       " 'mother': 252,\n",
       " 'final': 253,\n",
       " 'wrong': 254,\n",
       " 'else': 255,\n",
       " 'idea': 256,\n",
       " 'unfortunately': 257,\n",
       " 'school': 258,\n",
       " 'believe': 259,\n",
       " 'friend': 260,\n",
       " 'relationship': 261,\n",
       " 'second': 262,\n",
       " 'sequence': 263,\n",
       " 'tries': 264,\n",
       " 'works': 265,\n",
       " 'david': 266,\n",
       " 'house': 267,\n",
       " 'group': 268,\n",
       " 'used': 269,\n",
       " 'keep': 270,\n",
       " 'war': 271,\n",
       " 'called': 272,\n",
       " 'dead': 273,\n",
       " 'named': 274,\n",
       " 'playing': 275,\n",
       " 'often': 276,\n",
       " 'behind': 277,\n",
       " 'said': 278,\n",
       " 'despite': 279,\n",
       " 'written': 280,\n",
       " 'tell': 281,\n",
       " 'finally': 282,\n",
       " 'youre': 283,\n",
       " 'hand': 284,\n",
       " 'head': 285,\n",
       " 'able': 286,\n",
       " 'maybe': 287,\n",
       " 'turn': 288,\n",
       " 'past': 289,\n",
       " 'kids': 290,\n",
       " 'finds': 291,\n",
       " 'including': 292,\n",
       " 'seeing': 293,\n",
       " 'days': 294,\n",
       " 'perfect': 295,\n",
       " 'game': 296,\n",
       " 'supposed': 297,\n",
       " 'mr': 298,\n",
       " 'dark': 299,\n",
       " 'shot': 300,\n",
       " 'nice': 301,\n",
       " 'fight': 302,\n",
       " 'book': 303,\n",
       " 'directed': 304,\n",
       " 'run': 305,\n",
       " 'lives': 306,\n",
       " 'person': 307,\n",
       " 'running': 308,\n",
       " 'side': 309,\n",
       " 'lines': 310,\n",
       " 'starts': 311,\n",
       " 'camera': 312,\n",
       " 'style': 313,\n",
       " 'boy': 314,\n",
       " 'live': 315,\n",
       " 'tv': 316,\n",
       " 'car': 317,\n",
       " 'nearly': 318,\n",
       " 'moment': 319,\n",
       " 'need': 320,\n",
       " 'worth': 321,\n",
       " 'face': 322,\n",
       " 'care': 323,\n",
       " 'son': 324,\n",
       " 'others': 325,\n",
       " 'entertaining': 326,\n",
       " 'daughter': 327,\n",
       " 'upon': 328,\n",
       " 'start': 329,\n",
       " 'worst': 330,\n",
       " 'joe': 331,\n",
       " 'try': 332,\n",
       " 'full': 333,\n",
       " 'video': 334,\n",
       " 'example': 335,\n",
       " 'violence': 336,\n",
       " 'exactly': 337,\n",
       " 'opening': 338,\n",
       " 'hour': 339,\n",
       " 'matter': 340,\n",
       " 'summer': 341,\n",
       " 'major': 342,\n",
       " 'direction': 343,\n",
       " 'kevin': 344,\n",
       " 'whos': 345,\n",
       " 'let': 346,\n",
       " 'children': 347,\n",
       " 'beautiful': 348,\n",
       " 'review': 349,\n",
       " 'throughout': 350,\n",
       " 'already': 351,\n",
       " 'wasnt': 352,\n",
       " 'sequences': 353,\n",
       " 'title': 354,\n",
       " 'problems': 355,\n",
       " 'eyes': 356,\n",
       " 'version': 357,\n",
       " 'early': 358,\n",
       " 'short': 359,\n",
       " 'drama': 360,\n",
       " 'act': 361,\n",
       " 'robert': 362,\n",
       " 'classic': 363,\n",
       " 'order': 364,\n",
       " 'team': 365,\n",
       " 'fine': 366,\n",
       " 'production': 367,\n",
       " 'obvious': 368,\n",
       " 'knows': 369,\n",
       " 'kill': 370,\n",
       " 'top': 371,\n",
       " 'roles': 372,\n",
       " 'question': 373,\n",
       " 'hit': 374,\n",
       " 'boring': 375,\n",
       " 'truly': 376,\n",
       " 'guys': 377,\n",
       " 'screenplay': 378,\n",
       " 'sometimes': 379,\n",
       " 'sort': 380,\n",
       " 'beginning': 381,\n",
       " 'simple': 382,\n",
       " 'supporting': 383,\n",
       " 'jackie': 384,\n",
       " 'earth': 385,\n",
       " 'body': 386,\n",
       " 'known': 387,\n",
       " 'jack': 388,\n",
       " 'save': 389,\n",
       " 'white': 390,\n",
       " 'jokes': 391,\n",
       " 'space': 392,\n",
       " 'women': 393,\n",
       " 'yes': 394,\n",
       " 'hell': 395,\n",
       " 'deep': 396,\n",
       " 'killer': 397,\n",
       " 'tells': 398,\n",
       " 'novel': 399,\n",
       " 'scream': 400,\n",
       " 'tom': 401,\n",
       " 'coming': 402,\n",
       " 'wont': 403,\n",
       " 'room': 404,\n",
       " 'york': 405,\n",
       " 'strong': 406,\n",
       " 'particularly': 407,\n",
       " 'peter': 408,\n",
       " 'attempt': 409,\n",
       " 'extremely': 410,\n",
       " 'manages': 411,\n",
       " 'ends': 412,\n",
       " 'saw': 413,\n",
       " 'four': 414,\n",
       " 'genre': 415,\n",
       " 'heart': 416,\n",
       " 'happens': 417,\n",
       " 'worse': 418,\n",
       " 'five': 419,\n",
       " 'girls': 420,\n",
       " 'stupid': 421,\n",
       " 'possible': 422,\n",
       " 'sound': 423,\n",
       " 'quickly': 424,\n",
       " 'romantic': 425,\n",
       " 'lead': 426,\n",
       " 'says': 427,\n",
       " 'thriller': 428,\n",
       " 'lee': 429,\n",
       " 'future': 430,\n",
       " 'result': 431,\n",
       " 'wonder': 432,\n",
       " 'arent': 433,\n",
       " 'dog': 434,\n",
       " 'appears': 435,\n",
       " 'murder': 436,\n",
       " 'taking': 437,\n",
       " 'stop': 438,\n",
       " 'hope': 439,\n",
       " 'hero': 440,\n",
       " 'police': 441,\n",
       " 'involved': 442,\n",
       " 'fiction': 443,\n",
       " 'level': 444,\n",
       " 'whats': 445,\n",
       " 'attention': 446,\n",
       " 'close': 447,\n",
       " 'involving': 448,\n",
       " 'falls': 449,\n",
       " 'sets': 450,\n",
       " 'child': 451,\n",
       " 'de': 452,\n",
       " 'voice': 453,\n",
       " 'mostly': 454,\n",
       " 'planet': 455,\n",
       " 'experience': 456,\n",
       " 'hours': 457,\n",
       " 'van': 458,\n",
       " 'fall': 459,\n",
       " 'career': 460,\n",
       " 'material': 461,\n",
       " 'elements': 462,\n",
       " 'fans': 463,\n",
       " 'living': 464,\n",
       " 'note': 465,\n",
       " 'eventually': 466,\n",
       " 'ship': 467,\n",
       " 'among': 468,\n",
       " 'emotional': 469,\n",
       " 'bring': 470,\n",
       " 'ones': 471,\n",
       " 'lack': 472,\n",
       " 'dr': 473,\n",
       " 'wild': 474,\n",
       " 'laugh': 475,\n",
       " 'obviously': 476,\n",
       " 'single': 477,\n",
       " 'number': 478,\n",
       " 'chris': 479,\n",
       " 'meet': 480,\n",
       " 'aliens': 481,\n",
       " 'happen': 482,\n",
       " 'late': 483,\n",
       " 'enjoy': 484,\n",
       " 'youll': 485,\n",
       " 'none': 486,\n",
       " 'leads': 487,\n",
       " 'alone': 488,\n",
       " 'brother': 489,\n",
       " 'piece': 490,\n",
       " 'word': 491,\n",
       " 'taken': 492,\n",
       " 'guess': 493,\n",
       " 'husband': 494,\n",
       " 'attempts': 495,\n",
       " 'theater': 496,\n",
       " 'mean': 497,\n",
       " 'feels': 498,\n",
       " 'chance': 499,\n",
       " 'brothers': 500,\n",
       " 'needs': 501,\n",
       " 'laughs': 502,\n",
       " 'talent': 503,\n",
       " 'leave': 504,\n",
       " 'killed': 505,\n",
       " 'george': 506,\n",
       " 'computer': 507,\n",
       " 'talk': 508,\n",
       " 'feeling': 509,\n",
       " 'within': 510,\n",
       " 'battle': 511,\n",
       " 'whether': 512,\n",
       " 'god': 513,\n",
       " 'usually': 514,\n",
       " 'wonderful': 515,\n",
       " 'oscar': 516,\n",
       " 'across': 517,\n",
       " 'mission': 518,\n",
       " 'easy': 519,\n",
       " 'poor': 520,\n",
       " 'interest': 521,\n",
       " 'deal': 522,\n",
       " 'williams': 523,\n",
       " 'parents': 524,\n",
       " 'history': 525,\n",
       " 'science': 526,\n",
       " 'television': 527,\n",
       " 'words': 528,\n",
       " 'somehow': 529,\n",
       " 'told': 530,\n",
       " 'paul': 531,\n",
       " 'call': 532,\n",
       " 'feature': 533,\n",
       " 'features': 534,\n",
       " 'premise': 535,\n",
       " 'seemed': 536,\n",
       " 'expect': 537,\n",
       " 'basically': 538,\n",
       " 'meets': 539,\n",
       " 'success': 540,\n",
       " 'tale': 541,\n",
       " 'form': 542,\n",
       " 'impressive': 543,\n",
       " 'apparently': 544,\n",
       " 'crew': 545,\n",
       " 'stuff': 546,\n",
       " 'forced': 547,\n",
       " 'serious': 548,\n",
       " 'power': 549,\n",
       " 'parts': 550,\n",
       " 'recent': 551,\n",
       " 'except': 552,\n",
       " 'cool': 553,\n",
       " 'events': 554,\n",
       " 'filmmakers': 555,\n",
       " 'entertainment': 556,\n",
       " 'ryan': 557,\n",
       " 'oh': 558,\n",
       " 'score': 559,\n",
       " 'disney': 560,\n",
       " 'easily': 561,\n",
       " 'ben': 562,\n",
       " 'working': 563,\n",
       " 'giving': 564,\n",
       " 'middle': 565,\n",
       " 'release': 566,\n",
       " 'batman': 567,\n",
       " 'difficult': 568,\n",
       " 'surprise': 569,\n",
       " 'sequel': 570,\n",
       " 'released': 571,\n",
       " 'reality': 572,\n",
       " 'runs': 573,\n",
       " 'went': 574,\n",
       " 'smith': 575,\n",
       " 'local': 576,\n",
       " 'robin': 577,\n",
       " 'using': 578,\n",
       " 'hilarious': 579,\n",
       " 'ago': 580,\n",
       " 'happy': 581,\n",
       " 'william': 582,\n",
       " 'lets': 583,\n",
       " 'king': 584,\n",
       " 'viewers': 585,\n",
       " 'brings': 586,\n",
       " 'crime': 587,\n",
       " 'came': 588,\n",
       " 'complete': 589,\n",
       " 'return': 590,\n",
       " 'audiences': 591,\n",
       " 'change': 592,\n",
       " 'important': 593,\n",
       " 'somewhat': 594,\n",
       " 'turned': 595,\n",
       " 'certain': 596,\n",
       " 'die': 597,\n",
       " 'art': 598,\n",
       " 'remember': 599,\n",
       " 'youve': 600,\n",
       " 'dramatic': 601,\n",
       " 'ill': 602,\n",
       " 'effective': 603,\n",
       " 'strange': 604,\n",
       " 'popular': 605,\n",
       " 'viewer': 606,\n",
       " 'suspense': 607,\n",
       " 'credits': 608,\n",
       " 'presence': 609,\n",
       " 'similar': 610,\n",
       " 'begin': 611,\n",
       " 'girlfriend': 612,\n",
       " 'due': 613,\n",
       " 'surprisingly': 614,\n",
       " 'wouldnt': 615,\n",
       " 'figure': 616,\n",
       " 'blood': 617,\n",
       " 'quality': 618,\n",
       " 'ways': 619,\n",
       " 'business': 620,\n",
       " 'anyway': 621,\n",
       " 'couldnt': 622,\n",
       " 'decides': 623,\n",
       " 'beyond': 624,\n",
       " 'sexual': 625,\n",
       " 'light': 626,\n",
       " 'latest': 627,\n",
       " 'rich': 628,\n",
       " 'familiar': 629,\n",
       " 'personal': 630,\n",
       " 'writing': 631,\n",
       " 'absolutely': 632,\n",
       " 'mystery': 633,\n",
       " 'nature': 634,\n",
       " 'brilliant': 635,\n",
       " 'uses': 636,\n",
       " 'flick': 637,\n",
       " 'towards': 638,\n",
       " 'previous': 639,\n",
       " 'jim': 640,\n",
       " 'successful': 641,\n",
       " 'cut': 642,\n",
       " 'means': 643,\n",
       " 'read': 644,\n",
       " 'amazing': 645,\n",
       " 'predictable': 646,\n",
       " 'type': 647,\n",
       " 'powerful': 648,\n",
       " 'kid': 649,\n",
       " 'shots': 650,\n",
       " 'intelligent': 651,\n",
       " 'visual': 652,\n",
       " 'gone': 653,\n",
       " 'situation': 654,\n",
       " 'talking': 655,\n",
       " 'annoying': 656,\n",
       " 'stories': 657,\n",
       " 'third': 658,\n",
       " 'starring': 659,\n",
       " 'felt': 660,\n",
       " 'office': 661,\n",
       " 'secret': 662,\n",
       " 'clear': 663,\n",
       " 'red': 664,\n",
       " 'following': 665,\n",
       " 'cinema': 666,\n",
       " 'giant': 667,\n",
       " 'potential': 668,\n",
       " 'excellent': 669,\n",
       " 'leaving': 670,\n",
       " 'cop': 671,\n",
       " 'questions': 672,\n",
       " 'romance': 673,\n",
       " 'add': 674,\n",
       " 'create': 675,\n",
       " 'actress': 676,\n",
       " 'understand': 677,\n",
       " 'doubt': 678,\n",
       " 'sam': 679,\n",
       " 'project': 680,\n",
       " 'learn': 681,\n",
       " 'writer': 682,\n",
       " 'move': 683,\n",
       " 'present': 684,\n",
       " 'clever': 685,\n",
       " 'company': 686,\n",
       " 'former': 687,\n",
       " 'leaves': 688,\n",
       " 'rock': 689,\n",
       " 'party': 690,\n",
       " 'thinking': 691,\n",
       " 'definitely': 692,\n",
       " 'id': 693,\n",
       " 'effect': 694,\n",
       " 'trek': 695,\n",
       " 'motion': 696,\n",
       " 'opens': 697,\n",
       " 'scary': 698,\n",
       " 'huge': 699,\n",
       " 'water': 700,\n",
       " 'bill': 701,\n",
       " 'seriously': 702,\n",
       " 'follows': 703,\n",
       " 'general': 704,\n",
       " 'married': 705,\n",
       " 'america': 706,\n",
       " 'age': 707,\n",
       " 'usual': 708,\n",
       " 'directors': 709,\n",
       " 'near': 710,\n",
       " 'happened': 711,\n",
       " 'perfectly': 712,\n",
       " 'bob': 713,\n",
       " 'wedding': 714,\n",
       " 'saying': 715,\n",
       " 'straight': 716,\n",
       " 'solid': 717,\n",
       " 'created': 718,\n",
       " 'jones': 719,\n",
       " 'prison': 720,\n",
       " 'large': 721,\n",
       " 'heard': 722,\n",
       " 'sweet': 723,\n",
       " 'unlike': 724,\n",
       " 'merely': 725,\n",
       " 'smart': 726,\n",
       " 'slow': 727,\n",
       " 'likely': 728,\n",
       " 'mark': 729,\n",
       " 'villain': 730,\n",
       " 'took': 731,\n",
       " 'bunch': 732,\n",
       " 'various': 733,\n",
       " 'realize': 734,\n",
       " 'dumb': 735,\n",
       " 'plan': 736,\n",
       " 'million': 737,\n",
       " 'escape': 738,\n",
       " 'subject': 739,\n",
       " 'stay': 740,\n",
       " 'message': 741,\n",
       " 'moving': 742,\n",
       " 'ultimately': 743,\n",
       " 'scott': 744,\n",
       " 'agent': 745,\n",
       " 'country': 746,\n",
       " 'enjoyable': 747,\n",
       " 'break': 748,\n",
       " 'decent': 749,\n",
       " 'force': 750,\n",
       " 'points': 751,\n",
       " 'fails': 752,\n",
       " 'impossible': 753,\n",
       " 'mess': 754,\n",
       " 'follow': 755,\n",
       " 'tim': 756,\n",
       " 'immediately': 757,\n",
       " 'private': 758,\n",
       " 'political': 759,\n",
       " 'appear': 760,\n",
       " 'exciting': 761,\n",
       " 'harry': 762,\n",
       " 'pay': 763,\n",
       " 'filled': 764,\n",
       " 'brought': 765,\n",
       " 'murphy': 766,\n",
       " 'soundtrack': 767,\n",
       " 'dream': 768,\n",
       " 'animated': 769,\n",
       " 'overall': 770,\n",
       " 'mars': 771,\n",
       " 'favorite': 772,\n",
       " 'spend': 773,\n",
       " 'truth': 774,\n",
       " 'cold': 775,\n",
       " 'biggest': 776,\n",
       " 'element': 777,\n",
       " 'inside': 778,\n",
       " 'members': 779,\n",
       " 'fan': 780,\n",
       " 'keeps': 781,\n",
       " 'effort': 782,\n",
       " 'trouble': 783,\n",
       " 'eddie': 784,\n",
       " 'studio': 785,\n",
       " 'english': 786,\n",
       " 'talented': 787,\n",
       " 'particular': 788,\n",
       " 'liked': 789,\n",
       " 'neither': 790,\n",
       " 'eye': 791,\n",
       " 'silly': 792,\n",
       " 'hands': 793,\n",
       " 'focus': 794,\n",
       " 'carter': 795,\n",
       " 'slightly': 796,\n",
       " 'bond': 797,\n",
       " 'credit': 798,\n",
       " 'otherwise': 799,\n",
       " 'musical': 800,\n",
       " 'cannot': 801,\n",
       " 'offers': 802,\n",
       " 'actual': 803,\n",
       " 'constantly': 804,\n",
       " 'richard': 805,\n",
       " 'havent': 806,\n",
       " 'earlier': 807,\n",
       " 'martin': 808,\n",
       " 'purpose': 809,\n",
       " 'drug': 810,\n",
       " 'rating': 811,\n",
       " 'ideas': 812,\n",
       " 'ten': 813,\n",
       " 'wars': 814,\n",
       " 'chase': 815,\n",
       " 'memorable': 816,\n",
       " 'bruce': 817,\n",
       " 'totally': 818,\n",
       " 'thinks': 819,\n",
       " 'brief': 820,\n",
       " 'open': 821,\n",
       " 'box': 822,\n",
       " 'wait': 823,\n",
       " 'british': 824,\n",
       " 'control': 825,\n",
       " 'view': 826,\n",
       " 'entirely': 827,\n",
       " 'ask': 828,\n",
       " 'waste': 829,\n",
       " 'wanted': 830,\n",
       " 'atmosphere': 831,\n",
       " 'humans': 832,\n",
       " 'cinematography': 833,\n",
       " 'steve': 834,\n",
       " 'gun': 835,\n",
       " 'greatest': 836,\n",
       " 'fast': 837,\n",
       " 'showing': 838,\n",
       " 'disaster': 839,\n",
       " 'lots': 840,\n",
       " 'law': 841,\n",
       " 'state': 842,\n",
       " 'aspect': 843,\n",
       " 'ability': 844,\n",
       " 'suddenly': 845,\n",
       " 'critics': 846,\n",
       " 'situations': 847,\n",
       " 'subplot': 848,\n",
       " 'frank': 849,\n",
       " 'society': 850,\n",
       " 'terrible': 851,\n",
       " 'modern': 852,\n",
       " 'wish': 853,\n",
       " 'gave': 854,\n",
       " 'spent': 855,\n",
       " 'fear': 856,\n",
       " 'air': 857,\n",
       " 'moves': 858,\n",
       " 'army': 859,\n",
       " 'setting': 860,\n",
       " 'park': 861,\n",
       " 'animation': 862,\n",
       " 'female': 863,\n",
       " 'fairly': 864,\n",
       " 'max': 865,\n",
       " 'nick': 866,\n",
       " 'government': 867,\n",
       " 'mary': 868,\n",
       " 'sit': 869,\n",
       " 'whatever': 870,\n",
       " 'expected': 871,\n",
       " 'ridiculous': 872,\n",
       " 'depth': 873,\n",
       " 'sees': 874,\n",
       " 'steven': 875,\n",
       " 'outside': 876,\n",
       " 'song': 877,\n",
       " 'class': 878,\n",
       " 'hear': 879,\n",
       " 'west': 880,\n",
       " 'tension': 881,\n",
       " 'approach': 882,\n",
       " 'street': 883,\n",
       " 'dull': 884,\n",
       " 'indeed': 885,\n",
       " 'typical': 886,\n",
       " 'killing': 887,\n",
       " 'tone': 888,\n",
       " 'list': 889,\n",
       " 'amount': 890,\n",
       " 'cameron': 891,\n",
       " 'boys': 892,\n",
       " 'recently': 893,\n",
       " 'clearly': 894,\n",
       " 'screenwriter': 895,\n",
       " 'ii': 896,\n",
       " 'stand': 897,\n",
       " 'trailer': 898,\n",
       " 'teen': 899,\n",
       " 'quick': 900,\n",
       " 'provide': 901,\n",
       " 'minor': 902,\n",
       " 'hate': 903,\n",
       " 'violent': 904,\n",
       " 'imagine': 905,\n",
       " 'plenty': 906,\n",
       " 'cheap': 907,\n",
       " 'background': 908,\n",
       " 'sister': 909,\n",
       " 'shown': 910,\n",
       " 'island': 911,\n",
       " 'hold': 912,\n",
       " 'subtle': 913,\n",
       " 'dreams': 914,\n",
       " 'joke': 915,\n",
       " 'free': 916,\n",
       " 'convincing': 917,\n",
       " 'miss': 918,\n",
       " 'ride': 919,\n",
       " 'college': 920,\n",
       " 'titanic': 921,\n",
       " 'stone': 922,\n",
       " 'meanwhile': 923,\n",
       " 'okay': 924,\n",
       " 'awful': 925,\n",
       " 'front': 926,\n",
       " 'basic': 927,\n",
       " 'common': 928,\n",
       " 'possibly': 929,\n",
       " 'provides': 930,\n",
       " 'complex': 931,\n",
       " 'knew': 932,\n",
       " 'puts': 933,\n",
       " 'charm': 934,\n",
       " 'scifi': 935,\n",
       " 'woody': 936,\n",
       " 'images': 937,\n",
       " 'fire': 938,\n",
       " 'key': 939,\n",
       " 'sight': 940,\n",
       " 'grace': 941,\n",
       " 'carry': 942,\n",
       " 'chan': 943,\n",
       " 'detective': 944,\n",
       " 'appearance': 945,\n",
       " 'theme': 946,\n",
       " 'amusing': 947,\n",
       " 'somewhere': 948,\n",
       " 'beauty': 949,\n",
       " 'sounds': 950,\n",
       " 'telling': 951,\n",
       " 'flat': 952,\n",
       " 'leading': 953,\n",
       " 'incredibly': 954,\n",
       " 'master': 955,\n",
       " 'impact': 956,\n",
       " 'language': 957,\n",
       " 'seven': 958,\n",
       " 'road': 959,\n",
       " 'french': 960,\n",
       " 'writers': 961,\n",
       " 'cinematic': 962,\n",
       " 'reasons': 963,\n",
       " 'la': 964,\n",
       " 'sent': 965,\n",
       " 'opportunity': 966,\n",
       " 'lies': 967,\n",
       " 'development': 968,\n",
       " 'forget': 969,\n",
       " 'famous': 970,\n",
       " 'tarzan': 971,\n",
       " 'truman': 972,\n",
       " 'trip': 973,\n",
       " 'sean': 974,\n",
       " 'considering': 975,\n",
       " 'remains': 976,\n",
       " 'climax': 977,\n",
       " 'terrific': 978,\n",
       " 'mysterious': 979,\n",
       " 'consider': 980,\n",
       " 'aside': 981,\n",
       " 'highly': 982,\n",
       " 'rated': 983,\n",
       " 'decide': 984,\n",
       " 'ready': 985,\n",
       " 'delivers': 986,\n",
       " 'realistic': 987,\n",
       " 'mike': 988,\n",
       " 'willis': 989,\n",
       " 'member': 990,\n",
       " 'average': 991,\n",
       " 'believable': 992,\n",
       " 'thanks': 993,\n",
       " 'powers': 994,\n",
       " 'etc': 995,\n",
       " 'proves': 996,\n",
       " 'longer': 997,\n",
       " 'chemistry': 998,\n",
       " 'six': 999,\n",
       " 'race': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
