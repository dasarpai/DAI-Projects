{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50zI83PCrng1"
      },
      "source": [
        "https://youtu.be/DxygPxcfW_I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD2HZEhunUpG"
      },
      "source": [
        "# **This code is an implementation of a chatbot using the GPT-2 language model from Hugging Face's Transformers library.**\n",
        "<br>\n",
        "Reference: https://huggingface.co/transformers/v2.2.0/pretrained_models.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Load Configuraiton \n",
        "import hbqaconfig as conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCd-oTGxXyc1",
        "outputId": "4a1a26df-5945-40be-dfef-3afef17fa8b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=5b05057e61deab0b59e4a0c9ca1183e71308b067b826aa88f02355e35beb8cdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n",
            "Collecting huggingface\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: huggingface\n",
            "Successfully installed huggingface-0.0.1\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.15.10-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.36-py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=1af0f7db30f3cacb026f13bd7d7c6768d9cc36d5082832164f86210a11633d1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.36 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.31.0 setproctitle-1.3.2 smmap-5.0.1 wandb-0.15.10\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers\n",
        "# !pip install torch\n",
        "# !pip install -U PyPDF2\n",
        "# !pip install python-docx\n",
        "# !pip install huggingface\n",
        "# !pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgo6w30qbwMp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLMNpatgQ1TM"
      },
      "outputs": [],
      "source": [
        "# !pip install accelerate>=0.20.1\n",
        "# !pip install transformers[torch]\n",
        "# # You need to restart the kernel after this step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHHvUoZYFbnn",
        "outputId": "cda9dbae-239e-4b49-c8d8-4c2feff6f90e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW-HVrFbGQOU"
      },
      "outputs": [],
      "source": [
        "apipath = r'H:\\\\My Drive\\\\config\\\\hbqa.txt'\n",
        "apipath = r'/content/drive/MyDrive/config/hbqa-colab.txt'\n",
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read(apipath)\n",
        "\n",
        "secret_key = config['global']['OPENAI_KEY']\n",
        "datapath = config['global']['DATA_FOLDER']\n",
        "corpuspath = config['global']['CORPUS_FOLDER']\n",
        "corpus_sectionpath = config['global']['CORPUS_FOLDER_SECTIONS']\n",
        "PINECONE_API_KEY = config['global']['PINECONE_API_KEY']\n",
        "# Original corpus in the form of pdf or text or doc file. Originally this is from github.\n",
        "PINECONE_ENV = config['global']['PINECONE_ENV']\n",
        "CHATGPT_KEY = config['global']['CHATGPT_KEY']\n",
        "WANDB_KEY= config['global']['WANDB_KEY']\n",
        "\n",
        "# PINECONE is Vector Database. To store the vector so that we can quickly search the vector space.\n",
        "# https://app.pinecone.io\n",
        "# get PINECONE_API_KEY key from app.pinecone.io\n",
        "# find your PINECONE_ENVIRONMENT next to the api key in pinecone console\n",
        "\n",
        "corpuspath = datapath + \"MBBook-from-HTML-Chapters/Trainfiles\"\n",
        "modelpath  = datapath + \"MBBook-from-HTML-Chapters/Trainfiles/gpt2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfqnHAnMYfWF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "import docx\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmyKIMQTQ7k",
        "outputId": "a6b09a22-691c-4011-fb37-0428d1113444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUxxJK03Tc9o",
        "outputId": "392250e8-14ab-439b-f1c5-90c79941398a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=WANDB_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9AkvWhHTf_U"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Bn7925nhYk"
      },
      "source": [
        "Required functions to read text from various files located in a directory. Files can be a mix of pdf, docx, or txt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjhi8kpfYdlV"
      },
      "outputs": [],
      "source": [
        "# Functions to read different file types\n",
        "def read_pdf(file_path):\n",
        "    with open(file_path, \"rb\") as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "def read_word(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text = \"\"\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def read_documents_from_directory(directory):\n",
        "    combined_text = \"\"\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            combined_text += read_pdf(file_path)\n",
        "        elif filename.endswith(\".docx\"):\n",
        "            combined_text += read_word(file_path)\n",
        "        elif filename.endswith(\".txt\"):\n",
        "            combined_text += read_txt(file_path)\n",
        "    return combined_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1LqRO_Unfmk"
      },
      "source": [
        "The train_chatbot function uses the combined text data to train a GPT-2 model using the provided training arguments. The resulting trained model and tokenizer are then saved to a specified output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "X_D9_vpHbSgU",
        "outputId": "185be731-22dd-4f6b-cf1e-bb35275adaa5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhari-thapliyal\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230921_063510-ffeh6r55\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mHBQA_GPT2_Medium\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hari-thapliyal/DBA\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/hari-thapliyal/DBA/runs/ffeh6r55\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hari-thapliyal/DBA/runs/ffeh6r55?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f7b5426c4c0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"DBA\", name=\"HBQA_GPT2_Medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvOeTzWgQmkj"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWoL2G1Xagqa"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_BHg6giqvtb"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the EarlyStoppingCallback\n",
        "Early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,  # Adjust the patience value as needed\n",
        "    early_stopping_threshold=None,  # You can set a threshold for improvement if desired\n",
        ")\n",
        "\n",
        "callback_list = [Early_stopping_callback]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtDN1UL4X2_K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CAZGnWLbBZK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZI_zUBTIp3T"
      },
      "outputs": [],
      "source": [
        "#medium model error\n",
        "# CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 14.75 GiB total capacity;\n",
        "# 12.79 GiB already allocated; 832.00 KiB free; 13.72 GiB reserved in total by PyTorch)\n",
        "# If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\n",
        "# See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
        "\n",
        "#large model error\n",
        "# CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.75 GiB total capacity;\n",
        "#12.78 GiB already allocated; 832.00 KiB free; 13.72 GiB reserved in total by PyTorch)\n",
        "# If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\n",
        "# See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfybId6flPJC",
        "outputId": "ef122455-4fae-4de6-fe6d-408f69b3fad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 9.3 µs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "\n",
        "# Read documents from the directory\n",
        "combined_text = read_documents_from_directory(corpuspath)\n",
        "combined_text = re.sub(r'\\n+', '\\n', combined_text).strip()  # Remove excess newline characters\n",
        "\n",
        "# Split the text into training and validation sets\n",
        "train_fraction=0.8\n",
        "split_index = int(train_fraction * len(combined_text))\n",
        "train_text = combined_text[:split_index]\n",
        "val_text = combined_text[split_index:]\n",
        "\n",
        "# Save the training and validation data as text files\n",
        "with open(\"train.txt\", \"w\") as f:\n",
        "    f.write(train_text)\n",
        "with open(\"val.txt\", \"w\") as f:\n",
        "    f.write(val_text)\n",
        "\n",
        "# Set up the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").to(DEVICE)  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n",
        "\n",
        "# Prepare the dataset\n",
        "train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.txt\", block_size=128)\n",
        "val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"val.txt\", block_size=128)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "r09pTefOgGPt",
        "outputId": "dc8d61c4-64b0-4af7-fc36-fb53d08a4796"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14875' max='14875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14875/14875 2:21:19, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.335000</td>\n",
              "      <td>3.104804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.796400</td>\n",
              "      <td>3.033715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.467600</td>\n",
              "      <td>3.071103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.206100</td>\n",
              "      <td>3.186008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.976700</td>\n",
              "      <td>3.333058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.525100</td>\n",
              "      <td>3.546866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.335200</td>\n",
              "      <td>3.731298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.186600</td>\n",
              "      <td>3.997883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.054000</td>\n",
              "      <td>4.244314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.931300</td>\n",
              "      <td>4.451337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.717200</td>\n",
              "      <td>4.637264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.620200</td>\n",
              "      <td>4.781203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.558700</td>\n",
              "      <td>4.927917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.495100</td>\n",
              "      <td>5.028174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.446100</td>\n",
              "      <td>5.169712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.357400</td>\n",
              "      <td>5.224804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.316300</td>\n",
              "      <td>5.337255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>5.370447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.269500</td>\n",
              "      <td>5.439123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.247500</td>\n",
              "      <td>5.483391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.228800</td>\n",
              "      <td>5.520124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.199700</td>\n",
              "      <td>5.557323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.189200</td>\n",
              "      <td>5.584791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.183200</td>\n",
              "      <td>5.605595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.177200</td>\n",
              "      <td>5.606185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/Othercomputers/Spectre2/DBA/Data/MBBook-from-HTML-Chapters/Trainfiles/gpt2/tokenizer_config.json',\n",
              " '/content/drive/Othercomputers/Spectre2/DBA/Data/MBBook-from-HTML-Chapters/Trainfiles/gpt2/special_tokens_map.json',\n",
              " '/content/drive/Othercomputers/Spectre2/DBA/Data/MBBook-from-HTML-Chapters/Trainfiles/gpt2/vocab.json',\n",
              " '/content/drive/Othercomputers/Spectre2/DBA/Data/MBBook-from-HTML-Chapters/Trainfiles/gpt2/merges.txt',\n",
              " '/content/drive/Othercomputers/Spectre2/DBA/Data/MBBook-from-HTML-Chapters/Trainfiles/gpt2/added_tokens.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %time\n",
        "\n",
        "# # Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=modelpath,\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=25,\n",
        "    save_steps=10000, # Set the frequency to save the model\n",
        "    eval_steps=10000,  # Set the frequency to evaluate the model\n",
        "    save_total_limit=1, # Limit the number of saved checkpoints to 1 (best model)\n",
        "    logging_dir='./logs', # learning_rate=2e-5,\n",
        "    evaluation_strategy=\"epoch\",  # Change this to \"steps\"/ \"epoch\" if you want to evaluate at the end of each epoch\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,  # Set to True if you want to push the model to the Hugging Face Hub\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(modelpath)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(modelpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmmdUihqWdl6"
      },
      "source": [
        "## Earlier training\n",
        "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
        "Wall time: 7.39 µs\n",
        "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
        "  warnings.warn(\n",
        " [11198/59500 1:38:42 < 7:05:50, 1.89 it/s, Epoch 18.82/100]\n",
        "Step\tTraining Loss\n",
        "500\t3.334900\n",
        "1000\t2.793100\n",
        "1500\t2.456100\n",
        "2000\t2.196700\n",
        "2500\t1.940500\n",
        "3000\t1.720800\n",
        "3500\t1.466000\n",
        "4000\t1.260700\n",
        "4500\t1.096900\n",
        "5000\t0.949300\n",
        "5500\t0.818100\n",
        "6000\t0.700800\n",
        "6500\t0.584900\n",
        "7000\t0.490200\n",
        "7500\t0.422300\n",
        "8000\t0.365900\n",
        "8500\t0.320800\n",
        "9000\t0.281000\n",
        "9500\t0.244800\n",
        "10000\t0.207200\n",
        "10500\t0.187400\n",
        "11000\t0.171300\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU8r7-59QpaK"
      },
      "source": [
        "# Testing Model Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtVdesbVdTMM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQnO-P4_dSnF"
      },
      "outputs": [],
      "source": [
        "# The generate_response function takes a trained model, tokenizer, and a prompt string as input and generates a response using the GPT-2 model.\n",
        "\n",
        "def generate_response(model, tokenizer, prompt, max_length=250):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Create the attention mask and pad token id\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvQxmCimM97q"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(modelpath).to(DEVICE)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(modelpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHcQBzdJeB27"
      },
      "source": [
        "In the case of the GPT-2 tokenizer, the model uses a byte-pair encoding (BPE) algorithm, which tokenizes text into subword units. As a result, one word might be represented by multiple tokens.\n",
        "\n",
        "For example, if you set max_length to 50, the generated response will be limited to 50 tokens, which could be fewer than 50 words, depending on the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjKT9GGsV28b",
        "outputId": "d8eccb1f-ef66-40f4-c1c3-bc3dc7ee5fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated response: Who did the Muni ask for a bride and why? Who was the Rishi whom he  desired to select? Why \n",
            "was he so highly regarded by the king? Why also was he so hated by the  wicked Rishi? I wish to hear all \n",
            "these in detail.' \n",
            " \"Sauti said, 'O thou whose wealth is asceticism, the subjects of this \n",
            "history are diverse. This is but the beginning. I shall narrate the whole\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Who did the Muni ask for a bride and why?\"  # Replace with your desired prompt\n",
        "response = generate_response(model, tokenizer, prompt, max_length=100)  # This is small lenght question\n",
        "print(\"Generated response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7y14mtCc4He",
        "outputId": "24f95206-be10-4e8d-d92f-f977bc563962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated response: What did Mamata tell Vrihaspati regarding her condition? \n",
            " \"Vrihaspati said, 'O best of Brahmanas, the Asura (mamata) who brought forth thy child \n",
            "158 through the connection of a Brahmana's daughter, became, O best of \n",
            "Brahmanas, thy husband. She gave birth to thy child. The child became \n",
            "exceedingly devoted to thee and to his mother. Indeed, becau se he was so devoted to \n",
            "you, he became your son in righteousness. O thou o f fair smiles, he was devoted to \n",
            "virtue, honesty, and truth. Even in his boyhoo d, he was impetuous and vain. O \n",
            "beautiful one, he was virtuous and truthful. In splend our he was like unto a golden \n",
            "mountain. In splendour he was like a blazing fire;  in loveliness and affluence\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What did Mamata tell Vrihaspati regarding her condition?\"  # Replace with your desired prompt\n",
        "response = generate_response(model, tokenizer, prompt, max_length=200)  # This is small lenght question\n",
        "print(\"Generated response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXy235EzGvDO",
        "outputId": "d587d457-e3fd-44b9-fef1-650fa8b3a9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated response: Can you ask one question from page 20 of this book? \n",
            " \"Asked by a disciple of Vyasa, the illustrious Rishi  replied, 'O Bhi shma, the sun hath no \n",
            "power to set in the usual time, if the moon is down. O thou of ascetic wealth, \n",
            "the answer is simple: the four orders (celestials,  Asuras, Rakshasas, \n",
            "and men) depend on the lightness of mind which ser veth for success. If the \n",
            "person seeking to do good to his relatives by making a gift, or by \n",
            "sacrifices, or by attending to the poor, should the  sun go down, he can never \n",
            "succeed in accomplishing his object. The wise say that a person in \n",
            "woe should never yield to influence of evil,  or to the injunctions of his ancestors. The \n",
            "wise also say that a person\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Can you ask one question from page 20 of this book?\"  # Replace with your desired prompt\n",
        "response = generate_response(model, tokenizer, prompt, max_length=200)  # This is small lenght question\n",
        "print(\"Generated response:\", response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uU8r7-59QpaK"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
