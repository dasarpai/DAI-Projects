{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"_uuid":"06ca322f-fe43-49b6-83de-851f68436a90","_cell_guid":"83f99357-4795-49b2-a97d-7a84befd7a87","collapsed":false,"id":"_bwe_CP7MZhJ","outputId":"e9cbae19-26b6-4f8e-a30a-3f90e998847c","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:43:41.935511Z","iopub.execute_input":"2023-10-19T17:43:41.935825Z","iopub.status.idle":"2023-10-19T17:43:41.940381Z","shell.execute_reply.started":"2023-10-19T17:43:41.935796Z","shell.execute_reply":"2023-10-19T17:43:41.939384Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# @title Load Configuraiton \n# import hbqaconfig\nkaggle=True\n# conf = hbqaconfig.setEnv('local')","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:43:41.977003Z","iopub.execute_input":"2023-10-19T17:43:41.977747Z","iopub.status.idle":"2023-10-19T17:43:41.986476Z","shell.execute_reply.started":"2023-10-19T17:43:41.977718Z","shell.execute_reply":"2023-10-19T17:43:41.985522Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Setting for Kaggle & TPU","metadata":{}},{"cell_type":"code","source":"# !pip install torch_xla","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:43:42.009658Z","iopub.execute_input":"2023-10-19T17:43:42.009950Z","iopub.status.idle":"2023-10-19T17:43:42.013558Z","shell.execute_reply.started":"2023-10-19T17:43:42.009924Z","shell.execute_reply":"2023-10-19T17:43:42.012603Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.distributed.xla_multiprocessing as xmp","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:43:42.042657Z","iopub.execute_input":"2023-10-19T17:43:42.042934Z","iopub.status.idle":"2023-10-19T17:43:42.046623Z","shell.execute_reply.started":"2023-10-19T17:43:42.042913Z","shell.execute_reply":"2023-10-19T17:43:42.045835Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"conf = {'DATA_FOLDER': '/kaggle/input/',\n 'PE_FOLDER': '/kaggle/input/',\n 'QAGS_FOLDER': '/kaggle/working/',\n 'DRS_FOLDER': '/kaggle/working/',\n 'AGS_FOLDER': '/kaggle/working/',\n 'REPORT_FOLDER': '/kaggle/working/',\n 'CORPUS_SECTIONS_FOLDER': '/kaggle/working/',\n 'CORPUS_CHAPTER_FOLDER': '/kaggle/working/'}\n\nfor k,v in conf.items(): print (k,\":\",v)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:43:42.073961Z","iopub.execute_input":"2023-10-19T17:43:42.074191Z","iopub.status.idle":"2023-10-19T17:43:42.079320Z","shell.execute_reply.started":"2023-10-19T17:43:42.074172Z","shell.execute_reply":"2023-10-19T17:43:42.078503Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"DATA_FOLDER : /kaggle/input/\nPE_FOLDER : /kaggle/input/\nQAGS_FOLDER : /kaggle/working/\nDRS_FOLDER : /kaggle/working/\nAGS_FOLDER : /kaggle/working/\nREPORT_FOLDER : /kaggle/working/\nCORPUS_SECTIONS_FOLDER : /kaggle/working/\nCORPUS_CHAPTER_FOLDER : /kaggle/working/\n","output_type":"stream"}]},{"cell_type":"code","source":"# @title Select Processing Device\nimport torch\n\n# # Detect and initialize TPU\n# tpu_available = tf.config.experimental.list_logical_devices(\"TPU\")\n# if tpu_available:\n#     print(\"TPU available\")\n# else:\n#     print(\"No TPU available\")\n\nif kaggle:\n    DEVICE = torch.device(\"cuda\") #xm.xla_device(fold + 1)\nelif torch.cuda.is_available():\n    DEVICE = torch.device(\"cuda\")\nelse:\n    DEVICE = torch.device(\"cpu\")\n    \nprint(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:43:42.104778Z","iopub.execute_input":"2023-10-19T17:43:42.104999Z","iopub.status.idle":"2023-10-19T17:43:48.275938Z","shell.execute_reply.started":"2023-10-19T17:43:42.104980Z","shell.execute_reply":"2023-10-19T17:43:48.275009Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Zeroshot\n- T5\n- Flan-T5\n- GPT2\n- DistilBERT\n- RoBERTa\n- Llama2 (with PEFT & LoRA)","metadata":{"_uuid":"1d370948-9489-4491-823e-1fd62408e650","_cell_guid":"4d5889a4-5932-4312-a0fc-703f661567ac","id":"xhMxInMCGaYC","trusted":true}},{"cell_type":"code","source":"predmodels_list = {\"0\" :\"T5:t5\", \"1\" : \"Flan-T5:flant5\", \"2\":\"GPT2:gpt2\",\"3\":\"DistilBERT:distilbert\",\n                   \"4\":\"RoBERTa:roberta\",\"5\":\"Llama2:llama2\", \"6\":\"BERTSquad:bert\", \n                   \"7\":\"LongFormer:longformer\", \"8\":\"DistilBERT512:distilbert512\"}\n# Longformer doesn't work with latest Transformer\n\npredict_now=True\nembed_now=True\ncompute_metrics_now=True","metadata":{"_uuid":"65abddde-1bec-4400-a2cd-b7c6a93ee581","_cell_guid":"39ad45cd-5cec-44b7-945b-14ca17e0509f","collapsed":false,"id":"Bvglx-JnK6mf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:43:48.277404Z","iopub.execute_input":"2023-10-19T17:43:48.277759Z","iopub.status.idle":"2023-10-19T17:43:48.282313Z","shell.execute_reply.started":"2023-10-19T17:43:48.277735Z","shell.execute_reply":"2023-10-19T17:43:48.281512Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Example Questions","metadata":{"_uuid":"b7882f59-baa0-4ef8-b500-956236e9b635","_cell_guid":"15d2962c-e7f1-45ae-956a-4a297eb31525","id":"kxePEJraGPSv","trusted":true}},{"cell_type":"code","source":"q1 = \"What predicament does Yudhishthira face, and how does he seek guidance to resolve it?\"\n\nra1 = \"Yudhishthira faces the predicament of being unable to support the Brahmanas who are following him as he departs for the forest. \\\nHe seeks guidance to resolve this dilemma by approaching his priest, Dhaumya, and inquiring about the appropriate course of action.\"\n\nra1 = ra1.replace('\\n','')\n\nc1 = \"\"\"Section III\n\"Vaisampayana said, 'Yudhishthira the son of Kunti, thus addressed by Saunaka, approached his priest and in the midst of his brothers said,\n'The Brahmanas versed in the Vedas are following me who am departing for the forest. Afflicted with many calamities I am unable to support them.\nI cannot abandon them, nor have I the power to offer them sustenance: Tell me, O holy one, what should be done by me in such a pass.'\n\"Vaisampayana said, 'After reflecting for a moment seeking to find out the (proper) course by his yoga powers, Dhaumya, that foremost of all virtuous men,\naddressed Yudhishthira, in these words, 'In days of old, all living beings that had been created were sorely afflicted with hunger.\nAnd like a father (unto all of them), Savita (the sun) took compassion upon them. And going first into the northern declension,\nthe sun drew up water by his rays, and coming back to the southern declension, stayed over the earth, with his heat centered in himself.\nAnd while the sun so stayed over the earth, the lord of the vegetable world (the moon), converting the effects of the solar heat (vapours) into clouds\nand pouring them down in the shape of water, caused plants to spring up. Thus it is the sun himself, who, drenched by the lunar influence, is transformed,\nupon the sprouting of seeds, into holy vegetable furnished with the six tastes. And it is these which constitute the food of all creatures upon the earth.\nThus the food that supporteth the lives of creatures is instinct with solar energy, and the sun is, therefore, the father of all creatures.\nDo thou, hence, O Yudhishthira, take refuge even in him. All illustrious monarchs of pure descent and deeds are known to have delivered their people by practising high asceticism.\nThe great Karttavirya, and Vainya and Nahusha, had all, by virtue of ascetic meditation preceded by vows, delivered their people from heavy afflictions.\nTherefore, O virtuous one, as thou art purified by the acts do thou likewise, entering upon a file of austerities. O Bharata, virtuously support the regenerate ones.'\n\"Janamejaya said, 'How did that bull among the Kurus, king Yudhishthira, for the sake of the Brahmanas adore the sun of wonderful appearance?\"\n\"Vaisampayana said, 'Listen attentively, O king, purifying thyself and withdrawing thy mind from every other thing. And, O king of kings, appoint thou a time.\nI will tell thee everything in detail, And, O illustrious one, listen to the one hundred and eight names (of the sun)\nas they were disclosed of old by Dhaumya to the high-souled son of Pritha. Dhaumya said, 'Surya, Aryaman, Bhaga, Twastri, Pusha, Arka, Savitri.\nRavi, Gabhastimat, Aja, Kala, Mrityu, Dhatri, Prabhakara, Prithibi, Apa, Teja, Kha, Vayu, the sole stay, Soma, Vrihaspati, Sukra, Budha, Angaraka,\nIndra, Vivaswat, Diptanshu, Suchi, Sauri, Sanaichara, Brahma, Vishnu, Rudra, Skanda, Vaisravana, Yama, Vaidyutagni, Jatharagni, Aindhna, Tejasampati,\nDharmadhwaja, Veda-karttri, Vedanga, Vedavahana, Krita, Treta, Dwapara, Kali, full of every impurity, Kala, Kastha, Muhurtta, Kshapa, Yama, and Kshana;\nSamvatsara-kara, Aswattha, Kalachakra, Bibhavasu, Purusha, Saswata, Yogin, Vyaktavyakta, Sanatana, Kaladhyaksha, Prajadhyaksha, Viswakarma, Tamounda,\nVaruna, Sagara, Ansu, Jimuta, Jivana, Arihan, Bhutasraya, Bhutapati, Srastri, Samvartaka, Vanhi, Sarvadi, Alolupa, Ananta, Kapila, Bhanu, Kamada,\nSarvatomukha, Jaya, Visala, Varada, Manas, Suparna, Bhutadi, Sighraga, Prandharana, Dhanwantari, Dhumaketu, Adideva, Aditisuta, Dwadasatman, Aravindaksha,\nPitri, Matri, Pitamaha, Swarga-dwara, Prajadwara, Mokshadwara, Tripistapa, Dehakarti, Prasantatman, Viswatman, Viswatomukha, Characharatman, Sukhsmatman,\nthe merciful Maitreya. These are the hundred and eight names of Surya of immeasurable energy, as told by the self-create (Brahma). For the acquisition of prosperity,\nI bow down to thee, O Bhaskara, blazing like unto gold or fire, who is worshipped of the gods and the Pitris and the Yakshas, and who is adored by Asuras, Nisacharas,\nand Siddhas. He that with fixed attention reciteth this hymn at sunrise, obtaineth wife and offspring and riches and the memory of his former existence,\nand by reciting this hymn a person attaineth patience and memory. Let a man concentrating his mind, recite this hymn. By doing so,\nhe shall be proof against grief and forest-fire and ocean and every object of desire shall be his.'\n\"Vaisampayana continued, 'Having heard from Dhaumya these words suitable to the occasion, Yudhishthira the just, with heart concentrated within itself and purifying it duly,\nbecame engaged in austere meditation, moved by the desire of supporting the Brahmanas. And worshipping the maker of day with offerings of flowers and other articles,\nthe king performed his ablutions. And standing in the stream, he turned his face towards the god of day. And touching the water of the Ganges\nthe virtuous Yudhishthira with senses under complete control and depending upon air alone for his sustenance, stood there with rapt soul engaged in pranayama.\nAnd having purified himself and restrained his speech, he began to sing the hymn of praise (to the sun).'\n'Yudhishthira said, \"Thou art, O sun, the eye of the universe. Thou art the soul of all corporeal existences. Thou art the origin of all things.\nThou art the embodiment of the acts of all religious men. Thou art the refuge of those versed in the Sankhya philosophy (the mysteries of the 1.\"\"\"\n\nc1 = c1.replace('\\n','')","metadata":{"_uuid":"ef569fcc-1b42-4c8a-99d7-47c994caffe8","_cell_guid":"eb2d41c0-0319-4f8a-b67f-85ac41ff487e","collapsed":false,"id":"2_A-z0xwGSUQ","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:43:48.283691Z","iopub.execute_input":"2023-10-19T17:43:48.283939Z","iopub.status.idle":"2023-10-19T17:43:48.299624Z","shell.execute_reply.started":"2023-10-19T17:43:48.283919Z","shell.execute_reply":"2023-10-19T17:43:48.298771Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"q2 = \"What is the significance of performing the Agnihotra, and what consequences are mentioned for neglecting it?\"\n\nra2 = \"\"\"Performing the Agnihotra is considered important for eternal morality. Neglecting it, as mentioned in the text, leads to the consumption of sin.\nThose who do not perform the Agnihotra are said to suffer the consequences, including not waiting upon bulls and neglecting their kinsmen, guests, friends, sons, wives, and servants.\"\"\"\n\nra2 = ra2.replace('\\n','')\n\nc2 = \"\"\"Even this is eternal morality. They that perform not the Agnihotra  not wait upon bulls, nor cherish their kinsmen and guests and friends and\nsons and wives and servants, are consumed with sin for such neglect. None should cook his food for himself alone and none should slay an animal without dedicating it to the gods,\nthe pitris, and guests. Nor should one eat of that food which hath not been duly dedicated to the gods and pitris. By scattering food on the earth, morning and evening,\nfor (the behoof of) dogs and Chandalas and birds, should a person perform the Viswedeva sacrifice.  He that eateth the Vighasa, is regarded as eating ambrosia.\nWhat remaineth in a sacrifice after dedication to the gods and the pitris is regarded as ambrosia; and what remaineth after feeding the guest is called Vighasa\nand is equivalent to ambrosia itself. Feeding a guest is equivalent to a sacrifice, and the pleasant looks the host casteth upon the guest, the attention he devoteth to him,\nthe sweet words in which he addresseth him, the respect he payeth by following him, and the food and drink with which he treateth him, are the five Dakshinas\nin that sacrifice. He who giveth without stint food to a fatigued wayfarer never seen before, obtaineth merit that is great, and he who leading a domestic life,\nfolloweth such practices, acquireth religious merit that is said to be very great. O Brahmana, what is thy opinion on this?\"\n\"Saunaka said, 'Alas, this world is full of contradictions! That which shameth the good, gratifieth the wicked! Alas, moved by ignorance and passion\nand slaves of their own senses, even fools perform many acts of (apparent merit) to gratify in after-life their appetites! With eyes open are these\nmen led astray by their seducing senses, even as a charioteer, who hath lost his senses, by restive and wicked steeds! When any of the six senses\nfindeth its particular object, the desire springeth up in the heart to enjoy that particular object. And thus when one's heart proceedeth to enjoy the\nobjects of any particular sense a wish is entertained which in its turn giveth birth to a resolve. And finally, like unto an insect falling into a flame from\nlove of light, the man falleth into the fire of temptation, pierced by the shafts of the object of enjoyment discharged by the desire constituting the seed of the resolve!\nAnd thenceforth blinded by sensual pleasure which he seeketh without stint, and steeped in dark ignorance and folly which he mistaketh for a state of happiness, he\nknoweth not himself! And like unto a wheel that is incessantly rolling, every creature, from ignorance and deed and desire, falleth into various states in this world,\nwandering from one birth to another, and rangeth the entire circle of existences from a Brahma to the point of a blade of grass, now in water, now on land, and now against in the air!\n'This then is the career of those that are without knowledge. Listen now to the course of the wise they that are intent on profitable virtue,\nand are desirous of emancipation! The Vedas enjoin act but renounce (interest in) action. Therefore, shouldst thou act, renouncing Abhimana,  performance of sacrifices, study (of the Vedas),\ngifts, penance, truth (in both speech and act), forgiveness, subduing the senses, and renunciation of desire,--these have been declared to be the eight (cardinal) duties constituting the true path.\nOf these, the four first pave the way to the world of the pitris. And these should be practised without Abhimana. The four last are always observed by the pious, to attain the heaven of the gods.\nAnd the pure in spirit should ever follow these eight paths. Those who wish to subdue the world for purpose of salvation, should ever act fully renouncing motives, effectually subduing their senses,\nrigidly observing particular vows, devotedly serving their preceptors, austerely regulating their fare, diligently studying the Vedas, renouncing action as mean and restraining their hearts.\nBy renouncing desire and aversion the gods have attained prosperity. It is by virtue of their wealth of yoga  that the Rudras, and the Sadhyas, and the Adityas and the Vasus, and the twin Aswins,\nrule the creatures. Therefore, O son of Kunti, like unto them, do thou, O Bharata, entirely refraining from action with motive, strive to attain success in yoga and by ascetic austerities.\nThou hast already achieved such success so far as thy debts to thy ancestors, both male and female concerned, and that success also which is derived from action (sacrifices). Do thou,\nfor serving the regenerate ones endeavour to attain success in penances. Those that are crowned with ascetic success, can, by virtue of that success, do whatever they list; do thou, therefore,\npractising asceticism realise all thy wishes.\"\"\"\n\nc2 = c2.replace('\\n','')","metadata":{"_uuid":"3dcd2b1b-833c-49be-b791-8648ec11aacb","_cell_guid":"90ca5865-95a2-4f34-be05-5cbbcec22d67","collapsed":false,"id":"_1in5rC6GdwA","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:43:48.301956Z","iopub.execute_input":"2023-10-19T17:43:48.302194Z","iopub.status.idle":"2023-10-19T17:43:48.316984Z","shell.execute_reply.started":"2023-10-19T17:43:48.302174Z","shell.execute_reply":"2023-10-19T17:43:48.316040Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# # @title huggingface login\n# !pip install huggingface-cli\n\n# !huggingface-cli login","metadata":{"_uuid":"cc7330f1-4f09-4f1b-94a8-6f24307a695a","_cell_guid":"c9bfe870-4efa-4060-b828-84300d6321e0","collapsed":false,"cellView":"form","id":"6RPNSfNWjH2b","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:43:48.318054Z","iopub.execute_input":"2023-10-19T17:43:48.318277Z","iopub.status.idle":"2023-10-19T17:43:48.331734Z","shell.execute_reply.started":"2023-10-19T17:43:48.318258Z","shell.execute_reply":"2023-10-19T17:43:48.330778Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"_uuid":"60b1f25d-7b94-4985-972c-f57ae7161454","_cell_guid":"5991635e-d2ba-4074-b522-3f13b180a947","id":"V4yo_L89jF_k","trusted":true}},{"cell_type":"markdown","source":"# Install Necessary Libararies","metadata":{"_uuid":"54fd2a31-b0d8-4aca-9364-b28f0b8026b0","_cell_guid":"8bd07ce5-01c6-4911-bcb0-81d97bbcd51d","id":"2jZzbP7b8iw1","trusted":true}},{"cell_type":"code","source":"!pip install -Uq transformers\n!pip install -Uq evaluate\n!pip install -Uq SentencePiece\n!pip install -Uq sentence-transformers\n!pip install rouge-score","metadata":{"_uuid":"1dbad3f4-1925-4b69-94d9-3a49e23f3919","_cell_guid":"2b57fefd-1a1e-4785-932b-d6f6b144bb73","collapsed":false,"id":"2VacrsXBLqEc","outputId":"31b7a4e1-c3c4-4b91-b4e2-816271ebb152","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:43:48.332841Z","iopub.execute_input":"2023-10-19T17:43:48.333071Z","iopub.status.idle":"2023-10-19T17:44:48.078796Z","shell.execute_reply.started":"2023-10-19T17:43:48.333052Z","shell.execute_reply":"2023-10-19T17:44:48.077715Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=2d50e56299958b347faf20cdf6f1e04945d3b9d0fab8124db8425b197f14ebbf\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install accelerate>=0.20.1\n# !pip install transformers[torch]\n# # You need to restart the kernel after this step","metadata":{"_uuid":"2a6dba50-df1a-4cc8-a4ba-4a55b99aa898","_cell_guid":"9b20f6ba-830a-4733-bb6a-9ac393cdd1af","collapsed":false,"id":"WbDoP7u6NSeJ","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:44:48.080315Z","iopub.execute_input":"2023-10-19T17:44:48.080806Z","iopub.status.idle":"2023-10-19T17:44:48.085225Z","shell.execute_reply.started":"2023-10-19T17:44:48.080772Z","shell.execute_reply":"2023-10-19T17:44:48.084410Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Load Libraries and Configurations","metadata":{"_uuid":"dbc8649b-1336-4b5c-b8cc-45523f53d99b","_cell_guid":"86a0495b-4600-4973-a969-9808d9269c8c","id":"sCruv7vhoP2w","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport ast\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom torch.nn.functional import cosine_similarity\nimport torch.nn.functional as F\n\nfrom sentence_transformers import SentenceTransformer, util\n\nimport tensorflow as tf\n\n# import spacy\n# import string\n\nfrom sklearn.model_selection import train_test_split\n\nimport transformers\nfrom transformers import pipeline\nimport evaluate  # Bleu\n\n\nimport nltk\nnltk.download('punkt')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"732b9063-28c8-42e2-82c1-06894b6db3a6","_cell_guid":"5cad9c2b-51ac-4a27-94f9-143f682a991d","collapsed":false,"id":"YtLfF1XwKOIh","outputId":"225047c4-45bc-4934-afba-ac1ab829f659","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:44:48.086311Z","iopub.execute_input":"2023-10-19T17:44:48.086570Z","iopub.status.idle":"2023-10-19T17:45:08.131560Z","shell.execute_reply.started":"2023-10-19T17:44:48.086551Z","shell.execute_reply":"2023-10-19T17:45:08.130520Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"_uuid":"1c75ba28-aa59-4f39-908e-a913e2aaedeb","_cell_guid":"3285ef30-6222-4566-bf80-38ebbffa3d70","collapsed":false,"id":"IFfYxOWUM50w","outputId":"7946bb06-c6a2-4e95-f684-578f293ddb94","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load QA Dataset","metadata":{"_uuid":"89981379-c22a-4a38-9e06-b62b9c8eac2c","_cell_guid":"bac83983-57a6-4754-b56c-1ac6d624fac3","id":"lpBtIrI9M7hv","trusted":true}},{"cell_type":"code","source":"# df = pd.read_csv(datapath + '06-HBQA_Manual_with_Chunk.csv') # for local machine/vedavit colab\n\n# Uncomment below code\n# !pip install -q gdown\n# import gdown\n\n# # Replace the shared link with the actual link to your file\n# file_url = 'https://drive.google.com/uc?id=1Euvnmp8yJ2LGlL2uDvDjER87PYz9RVvS'\n# output_path = '/content/hbqa-colab.txt'  # Specify the desired file name and path\n\n# gdown.download(file_url, output_path, quiet=False)\n\nfile='06-HBQA_Manual_with_Chunk.csv'\nif kaggle: file = file.split(\".\")[0].lower().replace('_','-') +'/' + file\ndf = pd.read_csv(conf['PE_FOLDER'] + file)","metadata":{"_uuid":"7af5d116-d831-4200-8560-194962f742d6","_cell_guid":"dcd342e1-1a49-41bf-a5c7-760b47f7c098","collapsed":false,"id":"bsJcYRO2ZdMH","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:45:08.132856Z","iopub.execute_input":"2023-10-19T17:45:08.133588Z","iopub.status.idle":"2023-10-19T17:45:08.295424Z","shell.execute_reply.started":"2023-10-19T17:45:08.133553Z","shell.execute_reply":"2023-10-19T17:45:08.294755Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(df.shape)\ndf.head(2)","metadata":{"_uuid":"760ba861-be40-4ea7-9a97-fe622f43626d","_cell_guid":"a1185230-d7f5-4e0f-8453-dc0ed505b585","collapsed":false,"id":"1QCj3v0vKOIi","outputId":"dfe78693-77bd-4a87-ca8d-8c083884c67e","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:45:08.297838Z","iopub.execute_input":"2023-10-19T17:45:08.298094Z","iopub.status.idle":"2023-10-19T17:45:08.325418Z","shell.execute_reply.started":"2023-10-19T17:45:08.298073Z","shell.execute_reply":"2023-10-19T17:45:08.324677Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(1102, 11)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"   Ques_Id  Chunk_Id  Section_Id  \\\n0        0       389  Book03_002   \n1        1       390  Book03_003   \n\n                                            Question  \\\n0  What is the significance of performing the Agn...   \n1  What predicament does Yudhishthira face, and h...   \n\n                                          Ref_Answer  \\\n0  Performing the Agnihotra is considered importa...   \n1  Yudhishthira faces the predicament of being un...   \n\n                                               Chunk  \\\n0  Even this is eternal morality. They that perfo...   \n1  Section III\\n\"Vaisampayana said, 'Yudhishthira...   \n\n                                           Reference  WordsInQues  WordsInAns  \\\n0  The significance of the Agnihotra and the cons...           16          50   \n1  Yudhishthira's predicament and his consultatio...           14          41   \n\n   WordsInRef  WordsInChunk  \n0          50           809  \n1          53           852  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ques_Id</th>\n      <th>Chunk_Id</th>\n      <th>Section_Id</th>\n      <th>Question</th>\n      <th>Ref_Answer</th>\n      <th>Chunk</th>\n      <th>Reference</th>\n      <th>WordsInQues</th>\n      <th>WordsInAns</th>\n      <th>WordsInRef</th>\n      <th>WordsInChunk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>389</td>\n      <td>Book03_002</td>\n      <td>What is the significance of performing the Agn...</td>\n      <td>Performing the Agnihotra is considered importa...</td>\n      <td>Even this is eternal morality. They that perfo...</td>\n      <td>The significance of the Agnihotra and the cons...</td>\n      <td>16</td>\n      <td>50</td>\n      <td>50</td>\n      <td>809</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>390</td>\n      <td>Book03_003</td>\n      <td>What predicament does Yudhishthira face, and h...</td>\n      <td>Yudhishthira faces the predicament of being un...</td>\n      <td>Section III\\n\"Vaisampayana said, 'Yudhishthira...</td>\n      <td>Yudhishthira's predicament and his consultatio...</td>\n      <td>14</td>\n      <td>41</td>\n      <td>53</td>\n      <td>852</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.set_index('Ques_Id',inplace=True)\ndf.head(2)","metadata":{"_uuid":"961cc91f-e895-4971-a9be-16fed0db4e59","_cell_guid":"535ef555-7490-494a-aa7a-3f9d8994d00f","collapsed":false,"id":"4BtQbY4CUzHi","outputId":"9ab459f6-11ce-4c8f-ccd5-ea460277132d","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:45:08.326360Z","iopub.execute_input":"2023-10-19T17:45:08.326610Z","iopub.status.idle":"2023-10-19T17:45:08.342072Z","shell.execute_reply.started":"2023-10-19T17:45:08.326590Z","shell.execute_reply":"2023-10-19T17:45:08.341187Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"         Chunk_Id  Section_Id  \\\nQues_Id                         \n0             389  Book03_002   \n1             390  Book03_003   \n\n                                                  Question  \\\nQues_Id                                                      \n0        What is the significance of performing the Agn...   \n1        What predicament does Yudhishthira face, and h...   \n\n                                                Ref_Answer  \\\nQues_Id                                                      \n0        Performing the Agnihotra is considered importa...   \n1        Yudhishthira faces the predicament of being un...   \n\n                                                     Chunk  \\\nQues_Id                                                      \n0        Even this is eternal morality. They that perfo...   \n1        Section III\\n\"Vaisampayana said, 'Yudhishthira...   \n\n                                                 Reference  WordsInQues  \\\nQues_Id                                                                   \n0        The significance of the Agnihotra and the cons...           16   \n1        Yudhishthira's predicament and his consultatio...           14   \n\n         WordsInAns  WordsInRef  WordsInChunk  \nQues_Id                                        \n0                50          50           809  \n1                41          53           852  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Chunk_Id</th>\n      <th>Section_Id</th>\n      <th>Question</th>\n      <th>Ref_Answer</th>\n      <th>Chunk</th>\n      <th>Reference</th>\n      <th>WordsInQues</th>\n      <th>WordsInAns</th>\n      <th>WordsInRef</th>\n      <th>WordsInChunk</th>\n    </tr>\n    <tr>\n      <th>Ques_Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>389</td>\n      <td>Book03_002</td>\n      <td>What is the significance of performing the Agn...</td>\n      <td>Performing the Agnihotra is considered importa...</td>\n      <td>Even this is eternal morality. They that perfo...</td>\n      <td>The significance of the Agnihotra and the cons...</td>\n      <td>16</td>\n      <td>50</td>\n      <td>50</td>\n      <td>809</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>390</td>\n      <td>Book03_003</td>\n      <td>What predicament does Yudhishthira face, and h...</td>\n      <td>Yudhishthira faces the predicament of being un...</td>\n      <td>Section III\\n\"Vaisampayana said, 'Yudhishthira...</td>\n      <td>Yudhishthira's predicament and his consultatio...</td>\n      <td>14</td>\n      <td>41</td>\n      <td>53</td>\n      <td>852</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\n# df['WordsInChunk'] = df.Chunk.str.split(' ').apply(len)\n# df['CharInChunk'] = df.Chunk.apply(len)\n# df.to_csv(r'H:\\My Drive\\HBQA\\Data\\06-HBQA_Manual_with_Chunk.csv')","metadata":{"_uuid":"b97e9a1e-9bd5-47a3-b5bc-9c33563cf660","_cell_guid":"3dcb05fd-f972-477d-82e7-eabe13c7ff3b","collapsed":false,"id":"nBJWpcOJZiRc","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:45:08.343163Z","iopub.execute_input":"2023-10-19T17:45:08.343388Z","iopub.status.idle":"2023-10-19T17:45:08.352188Z","shell.execute_reply.started":"2023-10-19T17:45:08.343368Z","shell.execute_reply":"2023-10-19T17:45:08.351510Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# sample code\n# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\n# TOKENIZER = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")\n# MODEL = AutoModelForQuestionAnswering.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")\n# MODEL.to(DEVICE)","metadata":{"_uuid":"1c3b1748-b31d-4132-ab25-d8d3e21cc055","_cell_guid":"7de778f4-ef98-4fb5-9aab-306346b1c491","collapsed":false,"id":"qYZeJ16zoGDQ","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:45:08.353293Z","iopub.execute_input":"2023-10-19T17:45:08.353925Z","iopub.status.idle":"2023-10-19T17:45:08.363760Z","shell.execute_reply.started":"2023-10-19T17:45:08.353900Z","shell.execute_reply":"2023-10-19T17:45:08.362964Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Common Setting for Training","metadata":{"_uuid":"a41ea5cf-8623-443e-8e51-2241f2fc5ac0","_cell_guid":"3aa7a401-9a68-4d00-986c-444640b79108","id":"sVUo67UeySRL","trusted":true}},{"cell_type":"code","source":"Question_Len = int(max(len(ques) for ques in df.Question)/4)\nAnswer_Len = int(max(len(ans) for ans in df.Ref_Answer)/4)\nChunk_Len = int(max(len(chunk) for chunk in df.Chunk)/4)\nChunk_Len, Question_Len,Answer_Len # in Tokens","metadata":{"_uuid":"ad1f9fb2-5b30-4ef9-b87c-61996c95d13c","_cell_guid":"75821b96-8540-416a-bc02-b902ceb48db7","collapsed":false,"id":"Zt2QQ5J7Ccvn","outputId":"e7459bbb-2971-442f-9278-d2c0879c4966","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:45:08.364684Z","iopub.execute_input":"2023-10-19T17:45:08.364924Z","iopub.status.idle":"2023-10-19T17:45:08.378375Z","shell.execute_reply.started":"2023-10-19T17:45:08.364904Z","shell.execute_reply":"2023-10-19T17:45:08.377593Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(1499, 51, 326)"},"metadata":{}}]},{"cell_type":"code","source":"Q_LEN =  1500 #256   # Question Length\nT_LEN =  500 #32  # Target Length\nBATCH_SIZE = 4\n# DEVICE = \"cuda:0\"","metadata":{"_uuid":"6e159505-e7fa-49d5-93ad-d33386105174","_cell_guid":"f9785494-7afe-4bfb-99f3-4ade52ef5e72","collapsed":false,"id":"uCXnNdcA9fsP","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:45:08.379301Z","iopub.execute_input":"2023-10-19T17:45:08.379548Z","iopub.status.idle":"2023-10-19T17:45:08.393487Z","shell.execute_reply.started":"2023-10-19T17:45:08.379528Z","shell.execute_reply":"2023-10-19T17:45:08.392960Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Load Zeroshot Prediction Model","metadata":{"_uuid":"4ee19cec-8748-464f-9792-0958fad12cef","_cell_guid":"27ec94a8-9b12-460a-9291-488a6d313cbe","id":"A6itUc94KOIl","trusted":true}},{"cell_type":"code","source":"# @title Function to Load Zeroshot Transformers/Models\ndef load_model(predmodel_name):\n    if predmodel_name==\"t5\":\n        from transformers import T5Tokenizer,  T5ForConditionalGeneration #, T5Model , T5TokenizerFast\n        tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n        model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n        model.to(DEVICE)\n    elif predmodel_name == \"roberta\":\n        from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n        model_name = \"deepset/roberta-base-squad2\"\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n        model.to(DEVICE)\n\n    elif predmodel_name==\"gpt2\":\n        from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\",  padding_side=\"left\")  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n        tokenizer.pad_token = tokenizer.eos_token\n        model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").to(DEVICE)  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n\n    elif predmodel_name==\"bert\":\n        from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n        model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n        # model_name = \"t5-base\"\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n        model.to(DEVICE)\n    elif predmodel_name==\"longformer\":\n        print (\"It doesnot work with new transformers\")\n        #     from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n        #     model_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\n        #     tokenizer = LongformerTokenizer.from_pretrained(model_name) #.to(DEVICE)\n        #     model = LongformerForQuestionAnswering.from_pretrained(model_name).to(DEVICE)\n\n    elif predmodel_name==\"distilbert\":\n        # from transformers import DistilBertTokenizer, DistilBertModel\n        # tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n        # model = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n        # model.to(DEVICE)\n\n        from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n        model_name = 'distilbert-base-cased-distilled-squad'\n        tokenizer = AutoTokenizer.from_pretrained(model_name) #.to(DEVICE)\n        model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(DEVICE)\n    elif predmodel_name==\"distilbert512\":   \n        from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n        model_name = 'distilbert-base-cased-distilled-squad'\n        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n        model = DistilBertForQuestionAnswering.from_pretrained(model_name).to(DEVICE)\n\n\n    return tokenizer, model","metadata":{"_uuid":"3febc918-2afe-4de8-863d-e5c3cf35ec8a","_cell_guid":"20188403-7727-4508-871c-8b100c3d5fde","collapsed":false,"id":"a_6EBav9KOIl","cellView":"form","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T18:07:24.628660Z","iopub.execute_input":"2023-10-19T18:07:24.628977Z","iopub.status.idle":"2023-10-19T18:07:24.637502Z","shell.execute_reply.started":"2023-10-19T18:07:24.628953Z","shell.execute_reply":"2023-10-19T18:07:24.636525Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"predmodels_list # done: 0,3,6,8","metadata":{"_uuid":"7b59a140-20b2-4d8a-b92d-2dbd931bd915","_cell_guid":"304aef2a-aa62-4efe-8e98-a2eb70be3afb","collapsed":false,"id":"QPE4iqoGGpA_","outputId":"5cafef57-f2bf-4fbb-897a-73a4d89c1b42","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:48:22.110501Z","iopub.execute_input":"2023-10-19T17:48:22.111552Z","iopub.status.idle":"2023-10-19T17:48:22.117140Z","shell.execute_reply.started":"2023-10-19T17:48:22.111507Z","shell.execute_reply":"2023-10-19T17:48:22.116317Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'0': 'T5:t5',\n '1': 'Flan-T5:flant5',\n '2': 'GPT2:gpt2',\n '3': 'DistilBERT:distilbert',\n '4': 'RoBERTa:roberta',\n '5': 'Llama2:llama2',\n '6': 'BERTSquad:bert',\n '7': 'LongFormer:longformer',\n '8': 'DistilBERT512:distilbert512'}"},"metadata":{}}]},{"cell_type":"code","source":"# @title Select prediction model\npredmodel_name = predmodels_list['2'].split(':')[1]\nprint(predmodel_name)\ntokenizer, model = load_model(predmodel_name)","metadata":{"_uuid":"48b0ac67-b0f0-4508-8eeb-22eef85391d3","_cell_guid":"a665bec3-1a86-4710-bf50-dbb06c3b7bd5","collapsed":false,"id":"Pyg3Qnok-gEl","outputId":"1ef1d956-8cee-4bb0-c19f-80f86059d793","cellView":"form","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T18:04:46.373435Z","iopub.execute_input":"2023-10-19T18:04:46.374283Z","iopub.status.idle":"2023-10-19T18:05:01.120338Z","shell.execute_reply.started":"2023-10-19T18:04:46.374253Z","shell.execute_reply":"2023-10-19T18:05:01.119646Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"gpt2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51165991baa84f6ca54d8263e2cbf681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27c921f3cb64db8a509cdd4421192a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"446e15b10915489785009b4734e71b87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e7e0658ec0f49be945683d3cc4da64a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"112a087244e64aa38fe6e318f1d54b54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e534edb6fc45fa8b6808f7988bf806"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Function to predict Answers","metadata":{"_uuid":"68c16602-4c7b-4d70-92e8-179fb0495bbf","_cell_guid":"797db71d-189e-4d52-86bf-a5a884189a93","id":"832Ayr0HC95L","trusted":true}},{"cell_type":"code","source":"# @title answer_from_long_sentence_distilbert\ndef answer_from_long_sentence_distilbert(inputs,model):\n  # Check if the input sequence length exceeds the model's maximum\n  if len(inputs[\"input_ids\"][0]) > 512:\n      # Split the input into segments that fit the model's maximum sequence length\n      input_ids = inputs[\"input_ids\"][0]\n      attention_mask = inputs[\"attention_mask\"][0]\n\n      # Create a tensor with zeros\n      pad_len = 512 - len(input_ids) % 512\n      zeros = torch.zeros(pad_len, dtype=input_ids.dtype).to(DEVICE)\n\n      # Append the zeros to the input_ids tensor and attention_mask\n      input_ids = torch.cat((input_ids, zeros))\n      attention_mask = torch.cat((attention_mask, zeros)).to(DEVICE)\n\n      start_positions = []\n      end_positions = []\n\n      while len(input_ids) >= 512:\n          # Extract the first 512 tokens\n          segment_input_ids = input_ids[:512]\n          segment_attention_mask = attention_mask[:512]\n          # print(len(segment_input_ids))\n\n          with torch.no_grad():\n              segment_outputs = model(input_ids=segment_input_ids, attention_mask=segment_attention_mask)\n\n          start_logits = segment_outputs.start_logits\n          end_logits = segment_outputs.end_logits\n\n          # Find the answer span within the segment\n          answer_start = torch.argmax(start_logits)\n          answer_end = torch.argmax(end_logits) + 1\n\n          # Add the segment positions to the overall positions\n          start_positions.append(answer_start)\n          end_positions.append(answer_end)\n\n          # Remove the processed tokens\n          input_ids = input_ids[512:]\n          attention_mask = attention_mask[512:]\n\n      # Determine the final answer based on the segment positions\n      answer_start = min(start_positions)\n      answer_end = max(end_positions)\n  else:\n      # Use the original answer extraction approach\n      with torch.no_grad():\n          outputs = model(**inputs)\n      answer_start = torch.argmax(outputs.start_logits)\n      answer_end = torch.argmax(outputs.end_logits) + 1\n\n\n  # Get the answer text from the input tokens\n  answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end])\n\n  if predicted_answer[0:5]=='[CLS]' or  predicted_answer[0:5]=='[SEP]' or predicted_answer[0:3]=='<s>' or len(predicted_answer)<3 :\n     predicted_answer=\"xxx\"\n\n  return answer","metadata":{"_uuid":"c47b0703-8335-45c1-808d-b4aa45ea95b7","_cell_guid":"92eefc59-83c1-44b7-9121-db81d674dd0a","collapsed":false,"id":"nc2Wc--I2_Qf","cellView":"form","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T18:06:00.192114Z","iopub.execute_input":"2023-10-19T18:06:00.192939Z","iopub.status.idle":"2023-10-19T18:06:00.201855Z","shell.execute_reply.started":"2023-10-19T18:06:00.192908Z","shell.execute_reply":"2023-10-19T18:06:00.200841Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"9bcf0dc6-19e1-4d55-8d63-12e14a381743","_cell_guid":"bff30116-c89e-4efc-b612-80a6bf5523e9","collapsed":false,"cellView":"form","id":"acyGOIHMTRZM","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T18:06:00.771002Z","iopub.execute_input":"2023-10-19T18:06:00.771346Z","iopub.status.idle":"2023-10-19T18:06:00.776937Z","shell.execute_reply.started":"2023-10-19T18:06:00.771320Z","shell.execute_reply":"2023-10-19T18:06:00.775975Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"Q_LEN","metadata":{"execution":{"iopub.status.busy":"2023-10-19T18:06:01.049817Z","iopub.execute_input":"2023-10-19T18:06:01.050796Z","iopub.status.idle":"2023-10-19T18:06:01.055761Z","shell.execute_reply.started":"2023-10-19T18:06:01.050764Z","shell.execute_reply":"2023-10-19T18:06:01.054831Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"1500"},"metadata":{}}]},{"cell_type":"code","source":"# @title Predict Answer Function\ndef predict_answer(context, question, tokenizer, model):\n    Q_LEN=1500\n    \n    if predmodel_name==\"t5\":\n        inputs = tokenizer(question, context, max_length= Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n\n        input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n        attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n\n        with torch.no_grad():\n            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=100)\n\n        predicted_answer = tokenizer.decode(outputs.flatten(), skip_special_tokens=True)\n    elif predmodel_name==\"roberta\":\n        inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True, max_length=512, truncation=\"only_second\").to(DEVICE)\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        start_scores = outputs.start_logits\n        end_scores = outputs.end_logits\n\n        answer_start = torch.argmax(start_scores)\n        answer_end = torch.argmax(end_scores)+50\n        predicted_answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end+1])\n        \n    elif predmodel_name==\"gpt2\":\n#         Q_LEN=1024\n        # inputs = tokenizer(context, question, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n        # input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n        # attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n        # outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=100)\n\n        # Tokenize the input and set pad_token_id\n\n        input_text = f\"{context} [SEP] Q: {question} A:\"\n        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=Q_LEN, \n                                   truncation=True, padding=\"max_length\", pad_to_max_length=True).to(DEVICE)\n\n        # Set pad_token_id to eos_token_id\n        model.config.pad_token_id = model.config.eos_token_id\n\n        # Generate text\n        with torch.no_grad():\n            outputs = model.generate(input_ids, max_length=20, num_return_sequences=1, no_repeat_ngram_size=2)\n\n        predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    elif predmodel_name==\"longformer\":\n        print(\"It does not work. So removed\")\n\n    elif predmodel_name==\"bert\":\n      # inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True, max_length=1500, truncation=\"only_second\")\n      # predicted_answer = answer_from_long_sentence_bert(inputs,model)\n\n      inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True, max_length=512, truncation=True).to(DEVICE)\n      with torch.no_grad():\n          outputs = model(**inputs)\n\n\n      start_scores = outputs.start_logits\n      end_scores = outputs.end_logits\n\n      answer_start = torch.argmax(start_scores)\n      answer_end = torch.argmax(end_scores)+50\n      predicted_answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end+1])\n\n    elif predmodel_name==\"distilbert\":\n      inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True, max_length=1500, truncation=\"only_second\").to(DEVICE)\n\n      predicted_answer = answer_from_long_sentence_distilbert(inputs,model)\n    elif predmodel_name==\"distilbert512\":\n      inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True, max_length=512, truncation=\"only_second\").to(DEVICE)\n\n      with torch.no_grad():\n          outputs = model(**inputs)\n\n      start_scores = outputs.start_logits\n      end_scores = outputs.end_logits\n\n      answer_start = torch.argmax(start_scores)\n      answer_end = torch.argmax(end_scores)+50\n      predicted_answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end+1])\n\n\n    if len(predicted_answer)<3:\n       predicted_answer=\"xxx\" \n    elif predicted_answer[0:5]=='[CLS]' or  predicted_answer[0:5]=='[SEP]' or predicted_answer[0:3]=='<s>' :\n        predicted_answer=\"xxx\"\n    return predicted_answer","metadata":{"_uuid":"d5412ded-29f3-4f33-a5d9-ddafdfb9d139","_cell_guid":"f8f694c0-18c4-4293-b0fd-4feb2ef29254","collapsed":false,"id":"Z4Gq3uQTA0DS","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T18:18:04.885372Z","iopub.execute_input":"2023-10-19T18:18:04.886061Z","iopub.status.idle":"2023-10-19T18:18:04.898447Z","shell.execute_reply.started":"2023-10-19T18:18:04.886032Z","shell.execute_reply":"2023-10-19T18:18:04.897603Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# @title Example Prediction with a Selcted Model\n\n# from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n# inputs = tokenizer(q2, c2, return_tensors=\"pt\", padding=True, max_length=1500, truncation=\"only_second\")\n\n# pred_answer = predict_answer(c2, q2, tokenizer,model)\n# print(pred_answer)","metadata":{"_uuid":"6c220816-9ba1-4c30-9d08-7cd7a375f852","_cell_guid":"11a1baa3-73a6-4ebb-bff9-bdf1f53ef8e3","cellView":"form","id":"l8GsRLVa4BJt","outputId":"89cd4295-f703-44e2-d177-ea1a25e2645b","execution":{"iopub.status.busy":"2023-10-19T18:18:05.339084Z","iopub.execute_input":"2023-10-19T18:18:05.339781Z","iopub.status.idle":"2023-10-19T18:18:05.343600Z","shell.execute_reply.started":"2023-10-19T18:18:05.339751Z","shell.execute_reply":"2023-10-19T18:18:05.342712Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# inputs = tokenizer(q2, c2, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n# with torch.no_grad():\n#     outputs = model(**inputs)","metadata":{"_uuid":"9fd54003-676c-45e6-852d-8a07179a0925","_cell_guid":"bf138275-e4df-44e1-bcee-097a0fd8f35d","collapsed":false,"id":"qjeaHmtjBgfA","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T18:18:05.782243Z","iopub.execute_input":"2023-10-19T18:18:05.782991Z","iopub.status.idle":"2023-10-19T18:18:05.786985Z","shell.execute_reply.started":"2023-10-19T18:18:05.782960Z","shell.execute_reply":"2023-10-19T18:18:05.785936Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"# inputs.keys()","metadata":{"_uuid":"5ec0e6d6-ed93-4437-8038-87401426b78d","_cell_guid":"1674ac26-56b8-434d-8f9e-cbedfdac4df0","collapsed":false,"id":"KQPkaE-2DPlf","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T18:18:06.317800Z","iopub.execute_input":"2023-10-19T18:18:06.318124Z","iopub.status.idle":"2023-10-19T18:18:06.321843Z","shell.execute_reply.started":"2023-10-19T18:18:06.318099Z","shell.execute_reply":"2023-10-19T18:18:06.321052Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"# type(outputs)\n# outputs.keys()","metadata":{"_uuid":"2f966808-ab76-4700-aae8-7a9d46c4ae48","_cell_guid":"ca635607-b676-4ef2-857d-00b0795e425d","collapsed":false,"id":"i6ULgjX-D1ma","outputId":"0c049db8-c0bd-4978-e51d-2de62c8cf4d9","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T18:18:06.843394Z","iopub.execute_input":"2023-10-19T18:18:06.844261Z","iopub.status.idle":"2023-10-19T18:18:06.847909Z","shell.execute_reply.started":"2023-10-19T18:18:06.844232Z","shell.execute_reply":"2023-10-19T18:18:06.847036Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# @title Predict Answer Function\nfrom IPython.display import clear_output\nclear_output()\n\nfrom IPython.display import display\nfrom IPython.display import HTML\n\nif predict_now:\n  df_pred = pd.DataFrame(columns=['Ques_Id','Question','Ref_Answer','Pred_Answer'])\n\ndef predict_all_answers(df_pred, sample=True, verbose=False):\n  import random\n  if sample:\n    qno=random.sample(set(df.index),10)\n  else:\n    qno = df.index\n\n  total_items = len(qno)\n  j=0\n\n  # progress_bar = tqdm(total=total_items, position=0, leave=True)\n\n  progress_bar = tqdm(total=total_items, position=0, leave=True, dynamic_ncols=True)\n  progress_html = HTML(\"\"\"<progress value=\"0\" max=\"{0}\" style=\"width: 100%\"></progress>\"\"\".format(total_items))\n\n\n  for i  in qno:\n      progress_bar.update(j)\n      progress_html.value = \"\"\"<progress value=\"{0}\" max=\"{1}\" style=\"width: 100%\"></progress>\"\"\".format(i, total_items)\n      progress_bar = tqdm(total=total_items, position=0, leave=True)\n      ques_id  = df.index[i]\n      chunk    = df.iloc[i]['Chunk']\n      # print(chunk)\n      # chunk = chunk.replace(\"'\",\"\").replace(\"\\\",\"\")\n      ques     = df.iloc[i]['Question']\n      ref_ans  = df.iloc[i]['Ref_Answer']\n\n      pred_ans = predict_answer(chunk, ques, tokenizer, model)\n\n      df_pred.loc[j] = ques_id, ques, ref_ans, pred_ans\n      j+=1\n      if verbose:\n        print('Question  :', ques)\n        print(\"Ref Answer:\", ref_ans)\n        print(\"Pred Ans  :\", pred_ans)\n        print('--------')\n\n      # print(f\"Prediting Answer {i}/{df.shape[0]}\")\n\n  # progress_bar.close()","metadata":{"_uuid":"d56c972e-78b3-448a-a1eb-10ee89c93958","_cell_guid":"fb3b2856-e29e-4494-8bc2-ff975bb14b35","collapsed":false,"id":"kWcT2z_ENMtx","cellView":"form","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T18:18:07.628152Z","iopub.execute_input":"2023-10-19T18:18:07.628977Z","iopub.status.idle":"2023-10-19T18:18:07.639146Z","shell.execute_reply.started":"2023-10-19T18:18:07.628944Z","shell.execute_reply":"2023-10-19T18:18:07.638408Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"# @title Start Predicting Answer\nfilenm = '09.11-' + predmodel_name +'Predicted_Ans-Zeroshot.csv'\nif predict_now:\n  predict_all_answers(df_pred, sample=False, verbose=True)\n\nif predict_now:\n  print('Saving Predictions')\n  df_pred.to_csv(conf['AGS_FOLDER'] + filenm, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T18:18:09.200281Z","iopub.execute_input":"2023-10-19T18:18:09.201110Z","iopub.status.idle":"2023-10-19T18:18:10.019014Z","shell.execute_reply.started":"2023-10-19T18:18:09.201079Z","shell.execute_reply":"2023-10-19T18:18:10.017564Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":104,"outputs":[{"name":"stderr","text":"  0%|          | 0/1102 [00:00<?, ?it/s]\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [34,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[104], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m filenm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09.11-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m predmodel_name \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Ans-Zeroshot.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predict_now:\n\u001b[0;32m----> 4\u001b[0m   \u001b[43mpred_ans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predict_now:\n\u001b[1;32m      7\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaving Predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[103], line 38\u001b[0m, in \u001b[0;36mpred_ans\u001b[0;34m(df_pred, sample, verbose)\u001b[0m\n\u001b[1;32m     35\u001b[0m ques     \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m ref_ans  \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRef_Answer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m pred_ans \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mques\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m df_pred\u001b[38;5;241m.\u001b[39mloc[j] \u001b[38;5;241m=\u001b[39m ques_id, ques, ref_ans, pred_ans\n\u001b[1;32m     41\u001b[0m j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n","Cell \u001b[0;32mIn[98], line 46\u001b[0m, in \u001b[0;36mpredict_answer\u001b[0;34m(context, question, tokenizer, model)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 46\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     predicted_answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m predmodel_name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1606\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1590\u001b[0m         input_ids,\n\u001b[1;32m   1591\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1602\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1603\u001b[0m     )\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2454\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2454\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2462\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:201\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"code","source":"df_pred.to_csv(conf['AGS_FOLDER'] + filenm, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T18:13:17.677272Z","iopub.status.idle":"2023-10-19T18:13:17.677782Z","shell.execute_reply.started":"2023-10-19T18:13:17.677544Z","shell.execute_reply":"2023-10-19T18:13:17.677566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# df_pred = pd.read_csv(conf['AGS_FOLDER'] + filenm)\ndf_pred.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:50:37.221053Z","iopub.execute_input":"2023-10-19T17:50:37.222004Z","iopub.status.idle":"2023-10-19T17:50:37.227291Z","shell.execute_reply.started":"2023-10-19T17:50:37.221972Z","shell.execute_reply":"2023-10-19T17:50:37.226387Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(1102, 4)"},"metadata":{}}]},{"cell_type":"code","source":"# @title Load Predicted Asnwer file\n# if kaggle: file = filenm.split(\".\")[0].lower().replace('_','-') +'/' + file\ndf_pred = pd.read_csv(conf['AGS_FOLDER'] + filenm)\nprint(df_pred.shape)\ndf_pred.head(2)","metadata":{"_uuid":"26f95a4f-24c5-431a-93c3-ff567e5ce706","_cell_guid":"9d988501-89c1-4996-a1e6-28c87fa7388e","collapsed":false,"id":"nCE-n97SYVda","outputId":"7ae5d453-cdfa-4824-e32b-27453d9572eb","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:50:51.586303Z","iopub.execute_input":"2023-10-19T17:50:51.587215Z","iopub.status.idle":"2023-10-19T17:50:51.605235Z","shell.execute_reply.started":"2023-10-19T17:50:51.587182Z","shell.execute_reply":"2023-10-19T17:50:51.604234Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"(1102, 4)\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"   Ques_Id                                           Question  \\\n0        0  What is the significance of performing the Agn...   \n1        1  What predicament does Yudhishthira face, and h...   \n\n                                          Ref_Answer  \\\n0  Performing the Agnihotra is considered importa...   \n1  Yudhishthira faces the predicament of being un...   \n\n                                         Pred_Answer  \n0                                                xxx  \n1   hunger. And like a father (unto all of them),...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ques_Id</th>\n      <th>Question</th>\n      <th>Ref_Answer</th>\n      <th>Pred_Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>What is the significance of performing the Agn...</td>\n      <td>Performing the Agnihotra is considered importa...</td>\n      <td>xxx</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>What predicament does Yudhishthira face, and h...</td>\n      <td>Yudhishthira faces the predicament of being un...</td>\n      <td>hunger. And like a father (unto all of them),...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load Embedding Model","metadata":{"_uuid":"83296611-5413-40f0-9602-6f2dec7de24d","_cell_guid":"5e0415d8-252f-4271-9217-b0a1ab94c6ef","id":"3YR8gxGwKOIn","trusted":true}},{"cell_type":"code","source":"#Select Model Function\n\n# https://www.sbert.net/docs/pretrained_models.html\n\n#250MB, multi-qa-distilbert-cos-v1',  Max Sequence Length:\t512, Dimensions:768, Normalized Embeddings:\ttrue\n#80MB, all-MiniLM-L6-v2, Max Sequence Length:\t256, Dimensions:\t384, Normalized Embeddings:\ttrue\n#290MB, all-distilroberta-v1, Max Sequence Length:\t512, Dimensions:\t768, Normalized Embeddings:\ttrue\n#420MB, all-mpnet-base-v2, Max Sequence Length:\t384, Dimensions:\t768, Normalized Embeddings:\ttrue\n#1.36GB, all-roberta-large-v1, Max Sequence Length:\t256, Dimensions: 1024, Normalized Embeddings:\ttrue\n\ndef select_embmodel(num):\n    emb_modelshortlist = ['distilbert','minilm','distilroberta','mpnet','roberta']\n\n    emb_modellist = ['multi-qa-distilbert-cos-v1',\n                'all-MiniLM-L6-v2',\n                'all-distilroberta-v1',\n                'multi-qa-mpnet-base-dot-v1',\n                'all-roberta-large-v1']\n\n    embmodelname = emb_modellist[num]\n    embmodelshort = emb_modelshortlist[num]\n    embmodelname1 = \"_\" + embmodelname\n\n    print (embmodelname,'\\t',embmodelshort,'\\t', embmodelname1)\n    return embmodelname, embmodelshort, embmodelname1","metadata":{"_uuid":"3ca4fd63-d917-4127-8f26-493adfa3e926","_cell_guid":"ff824367-54f7-45f0-8e63-9bfbe1a9e3af","collapsed":false,"id":"_hJs8L6DKOIn","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:50:51.621764Z","iopub.execute_input":"2023-10-19T17:50:51.622009Z","iopub.status.idle":"2023-10-19T17:50:51.627446Z","shell.execute_reply.started":"2023-10-19T17:50:51.621988Z","shell.execute_reply":"2023-10-19T17:50:51.626437Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"embmodelname, embmodelshort, embmodelname1 = select_embmodel(3)","metadata":{"_uuid":"63d07ac8-b76c-4394-90a5-c7e81f7a13bc","_cell_guid":"06832c47-5085-4fc7-9589-842e680cb80c","collapsed":false,"id":"6n5NSwK0PBfv","outputId":"0e237d99-cb4d-4262-99c8-306dc21e80b9","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:50:51.655203Z","iopub.execute_input":"2023-10-19T17:50:51.655442Z","iopub.status.idle":"2023-10-19T17:50:51.660551Z","shell.execute_reply.started":"2023-10-19T17:50:51.655422Z","shell.execute_reply":"2023-10-19T17:50:51.659601Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"multi-qa-mpnet-base-dot-v1 \t mpnet \t _multi-qa-mpnet-base-dot-v1\n","output_type":"stream"}]},{"cell_type":"code","source":"if embed_now:\n  embmodel = SentenceTransformer(embmodelname)","metadata":{"_uuid":"f768815b-1015-4976-be4f-10bf4d5250f8","_cell_guid":"84fea102-f356-4d29-a31b-b73c90a91cd7","collapsed":false,"id":"lGzzleoxPWsk","outputId":"9ca31c62-df3d-4b67-9b14-298fac3ae9b7","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:50:51.693975Z","iopub.execute_input":"2023-10-19T17:50:51.694196Z","iopub.status.idle":"2023-10-19T17:50:57.662226Z","shell.execute_reply.started":"2023-10-19T17:50:51.694177Z","shell.execute_reply":"2023-10-19T17:50:57.661499Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)16ebc/.gitattributes:   0%|          | 0.00/737 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10011c92d72244968a99b0d014f9e5e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4bc7af46ddf42488d4998fa18bbe4da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)b6b5d16ebc/README.md:   0%|          | 0.00/8.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b06f564478b345eea0eb5409ef44e708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)b5d16ebc/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f14cc6d63d0c43a5b115cdfcc0f9b4b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b29203069ca435faa4a443ebbde6e27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ebc/data_config.json:   0%|          | 0.00/25.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b7239effaea47bc85086c0af9b30f2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e3c0e53caa74a9185db7a49551eeb2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe871423a72f44b7af16695f12f761e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"635ed75eba2845a9a9f7bb505601360b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)16ebc/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"182aa266642546558f1fe9bfc8c5f3fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6df42dcd82d4bd09a1c0f622115cb35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)6ebc/train_script.py:   0%|          | 0.00/13.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a02b4e08a9b243ea962cea3c23e23337"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)b6b5d16ebc/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbbf230d6e8d4b63b222815c3fc1d78a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)5d16ebc/modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d7a992771af43b1b7e21f70788180f1"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Vectorize Answer","metadata":{"_uuid":"b4d270be-064d-431c-8c34-610db96f9796","_cell_guid":"e2cb1b06-ce51-476a-b5cd-52dda17a0072","id":"hflQRBBNYOhb","trusted":true}},{"cell_type":"code","source":"filenm ='09.11-' + predmodel_name +'Predicted_Ans-Zeroshot.csv'\ndf_pred =  pd.read_csv(conf['AGS_FOLDER'] + filenm)\n\ndf_pred.set_index('Ques_Id',inplace=True)","metadata":{"_uuid":"3003c864-031c-46d2-8467-ee57d6d868fa","_cell_guid":"4355bd95-9d96-483c-a8d8-4656630269be","collapsed":false,"id":"vI0565KwYMTr","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:50:57.663644Z","iopub.execute_input":"2023-10-19T17:50:57.663883Z","iopub.status.idle":"2023-10-19T17:50:57.676494Z","shell.execute_reply.started":"2023-10-19T17:50:57.663862Z","shell.execute_reply":"2023-10-19T17:50:57.675630Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"df_pred.head(3)","metadata":{"_uuid":"10e31719-e0ed-48fb-93e3-3f58570b8e1f","_cell_guid":"90769b40-cb0c-40b7-86e1-c36d3161b554","collapsed":false,"id":"my04KErIUhP8","outputId":"97025a34-decc-4a40-f51e-645ad74ce34a","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:50:57.677757Z","iopub.execute_input":"2023-10-19T17:50:57.678506Z","iopub.status.idle":"2023-10-19T17:50:57.686838Z","shell.execute_reply.started":"2023-10-19T17:50:57.678447Z","shell.execute_reply":"2023-10-19T17:50:57.685941Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"                                                  Question  \\\nQues_Id                                                      \n0        What is the significance of performing the Agn...   \n1        What predicament does Yudhishthira face, and h...   \n2        What advice does Dhaumya offer to Yudhishthira...   \n\n                                                Ref_Answer  \\\nQues_Id                                                      \n0        Performing the Agnihotra is considered importa...   \n1        Yudhishthira faces the predicament of being un...   \n2        Dhaumya advises Yudhishthira to take refuge in...   \n\n                                               Pred_Answer  \nQues_Id                                                     \n0                                                      xxx  \n1         hunger. And like a father (unto all of them),...  \n2         Tell me, O holy one, what should be done by m...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Ref_Answer</th>\n      <th>Pred_Answer</th>\n    </tr>\n    <tr>\n      <th>Ques_Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the significance of performing the Agn...</td>\n      <td>Performing the Agnihotra is considered importa...</td>\n      <td>xxx</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What predicament does Yudhishthira face, and h...</td>\n      <td>Yudhishthira faces the predicament of being un...</td>\n      <td>hunger. And like a father (unto all of them),...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What advice does Dhaumya offer to Yudhishthira...</td>\n      <td>Dhaumya advises Yudhishthira to take refuge in...</td>\n      <td>Tell me, O holy one, what should be done by m...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def create_ans_vector():\n\n    Ans_Sentences = df_pred.Pred_Answer.tolist()\n    Ans_Embeddings = embmodel.encode(Ans_Sentences)\n\n    print (\"Answer Embedding: \", Ans_Embeddings.shape)\n    # return (Ques_Embeddings, Ans_Embeddings)\n\n    return (Ans_Embeddings)","metadata":{"_uuid":"7eb2e1e5-ac80-4a1e-8e4d-ed8590234bd5","_cell_guid":"71c984ce-311f-4e22-8041-c26bd7ee2fb6","collapsed":false,"id":"wKV1ytidgDCb","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:50:57.688677Z","iopub.execute_input":"2023-10-19T17:50:57.688921Z","iopub.status.idle":"2023-10-19T17:50:57.696152Z","shell.execute_reply.started":"2023-10-19T17:50:57.688900Z","shell.execute_reply":"2023-10-19T17:50:57.695585Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# @title Answer not found these questions\ndf_pred['Pred_Answer'] = df_pred.Pred_Answer.fillna(\"xxx\")\ndf_pred.loc[df_pred['Pred_Answer'].str.contains(r'\\?\\?\\?'), 'Pred_Answer'] = \"xxx\"\n\n# Model couldn't fine any any answer for this\ndf_pred.loc[df_pred['Pred_Answer'].str.contains(r\"xxx\")].shape","metadata":{"_uuid":"e0b667a9-85eb-48f1-b0fb-45afaba416be","_cell_guid":"d86760b5-fac3-4389-bc09-bc02a7567777","collapsed":false,"id":"h41dWQUBZDKe","outputId":"f4b3d65f-c8d5-4e78-ce06-af0f1f913d63","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:50:57.696964Z","iopub.execute_input":"2023-10-19T17:50:57.697173Z","iopub.status.idle":"2023-10-19T17:50:57.715136Z","shell.execute_reply.started":"2023-10-19T17:50:57.697155Z","shell.execute_reply":"2023-10-19T17:50:57.714501Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"(681, 3)"},"metadata":{}}]},{"cell_type":"code","source":"if embed_now:\n  Ans_Embeddings = create_ans_vector()\n  df_pred['Pred_Answer_Vector'] = torch.tensor(Ans_Embeddings, dtype=torch.float).tolist()","metadata":{"_uuid":"0317d908-c6fa-4763-8722-5066b681b44f","_cell_guid":"98e84299-e54f-4826-b83a-340ed522ae6f","collapsed":false,"id":"CWy7UqdigGLY","outputId":"62f704c6-be5e-47be-a0c9-0f2b65dd1c87","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:50:57.715876Z","iopub.execute_input":"2023-10-19T17:50:57.716066Z","iopub.status.idle":"2023-10-19T17:51:00.843675Z","shell.execute_reply.started":"2023-10-19T17:50:57.716049Z","shell.execute_reply":"2023-10-19T17:51:00.842789Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2b56a7713904f82b22345f04a557c91"}},"metadata":{}},{"name":"stdout","text":"Answer Embedding:  (1102, 768)\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfilenm = '09.11-' + predmodel_name +'Predicted_AnsVec-Zeroshot.csv'\nif embed_now:\n  df_pred.to_csv(conf['AGS_FOLDER'] + filenm)\n\n# df_pred = pd.read_csv(datapath + '09.11-' + predmodel_name +'Predicted_AnsVec-Zeroshot.csv')\nprint(df_pred.shape)\ndf_pred.head(2)","metadata":{"_uuid":"ef228811-0137-47dc-a115-d8350927d1f3","_cell_guid":"062b4393-bf3f-45f3-b3d2-b5665ba8cfc7","collapsed":false,"id":"7WtNurcbEctC","outputId":"8e7e6c14-3bd5-48d3-db6e-10f9b512b853","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:00.844688Z","iopub.execute_input":"2023-10-19T17:51:00.844945Z","iopub.status.idle":"2023-10-19T17:51:01.856067Z","shell.execute_reply.started":"2023-10-19T17:51:00.844924Z","shell.execute_reply":"2023-10-19T17:51:01.855226Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"(1102, 4)\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"                                                  Question  \\\nQues_Id                                                      \n0        What is the significance of performing the Agn...   \n1        What predicament does Yudhishthira face, and h...   \n\n                                                Ref_Answer  \\\nQues_Id                                                      \n0        Performing the Agnihotra is considered importa...   \n1        Yudhishthira faces the predicament of being un...   \n\n                                               Pred_Answer  \\\nQues_Id                                                      \n0                                                      xxx   \n1         hunger. And like a father (unto all of them),...   \n\n                                        Pred_Answer_Vector  \nQues_Id                                                     \n0        [-0.3637753129005432, -0.34021222591400146, -0...  \n1        [0.19928711652755737, -0.4345826804637909, -0....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Ref_Answer</th>\n      <th>Pred_Answer</th>\n      <th>Pred_Answer_Vector</th>\n    </tr>\n    <tr>\n      <th>Ques_Id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the significance of performing the Agn...</td>\n      <td>Performing the Agnihotra is considered importa...</td>\n      <td>xxx</td>\n      <td>[-0.3637753129005432, -0.34021222591400146, -0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What predicament does Yudhishthira face, and h...</td>\n      <td>Yudhishthira faces the predicament of being un...</td>\n      <td>hunger. And like a father (unto all of them),...</td>\n      <td>[0.19928711652755737, -0.4345826804637909, -0....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Calculate Metrics","metadata":{"_uuid":"450d843b-6016-497f-97bc-502cf5b09776","_cell_guid":"c23da2e5-268f-4679-b31b-8df90e938a0b","id":"3DFkqupdP78P","trusted":true}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom sklearn.metrics import precision_score, recall_score\nfrom nltk.tokenize import word_tokenize\nfrom rouge_score import rouge_scorer\nimport numpy as np\n\nsmoother = SmoothingFunction()\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n\ndef get_nlp_metrics(ques_id, ref_ans, pred_ans):\n\n  # Calculate ROUGE-1 and ROUGE-L scores\n  scores = scorer.score(ref_ans, pred_ans)\n\n  # Access individual ROUGE scores\n  rouge_1_precision = scores['rouge1'].precision\n  rouge_1_recall = scores['rouge1'].recall\n  rouge_1_f1 = scores['rouge1'].fmeasure\n\n  rouge_l_precision = scores['rougeL'].precision\n  rouge_l_recall = scores['rougeL'].recall\n  rouge_l_f1 = scores['rougeL'].fmeasure\n\n  rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1,\n\n  ref_tokens = word_tokenize(ref_ans)\n  pred_tokens = word_tokenize(pred_ans)\n\n  # Calculate BLEU score for a single sentence\n  bleu_score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoother.method2)\n\n  # Calculate BLEU score for multiple sentences\n  # corpus_bleu_score = corpus_bleu([[ref_tokens]], [pred_tokens],  smoothing_function=smoother.method2)\n\n  # print(set(ref_tokens).intersection(set(pred_tokens)))\n  tp = len(set(ref_tokens).intersection(set(pred_tokens)))\n  precision = tp / len(pred_tokens)\n  recall =  tp / len(ref_tokens)\n\n  return ques_id, bleu_score, rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1, precision, recall","metadata":{"_uuid":"dd1bd2ba-038c-46a5-a2a0-70a4e0c4b94f","_cell_guid":"9aa48d6e-ece7-4989-aa1a-baae0fcc1b21","collapsed":false,"id":"VwxedCIOKOIo","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:01.857066Z","iopub.execute_input":"2023-10-19T17:51:01.857319Z","iopub.status.idle":"2023-10-19T17:51:01.871832Z","shell.execute_reply.started":"2023-10-19T17:51:01.857281Z","shell.execute_reply":"2023-10-19T17:51:01.871109Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# from datasets import load_metric\nif compute_metrics_now:\n  df_metrics = pd.DataFrame(columns = ['Ques_Id','BLEU1', 'ROUGE1_P', 'ROUGE1_R', 'ROUGE1_F1',\n                              'ROUGEL_P', 'ROUGEL_R', 'ROUGEL_F1', 'Precision', 'Recall'])\n  N= df_pred.shape[0]\n\n  for i in range(N):\n    # bleu_score1 = calculate_score( df1.iloc[i]['ref_answer'],df1.iloc[i]['pred_answer'])\n    ques_id = df_pred.index[i]\n    ref_ans  = df_pred.loc[df_pred.index==ques_id,'Ref_Answer'].values[0]\n    pred_ans = df_pred.loc[df_pred.index==ques_id,'Pred_Answer'].values[0]\n    results = get_nlp_metrics(ques_id, ref_ans, pred_ans)\n    df_metrics.loc[i] = results","metadata":{"_uuid":"3bc1e49c-b973-42e9-a30d-7ff9c45f21a4","_cell_guid":"0f5456bb-1040-45ee-a72f-13ee918489c3","collapsed":false,"id":"rhEK-0Acx_PA","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:01.872786Z","iopub.execute_input":"2023-10-19T17:51:01.873040Z","iopub.status.idle":"2023-10-19T17:51:05.256851Z","shell.execute_reply.started":"2023-10-19T17:51:01.873020Z","shell.execute_reply":"2023-10-19T17:51:05.255938Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"metricsfile = '09.12-' + predmodel_name +'Predicted_Ans_Score_Zeroshot.csv'\nprint(metricsfile)\n\nif compute_metrics_now:\n  df_metrics.to_csv(conf['REPORT_FOLDER'] + metricsfile, index=False)\n\n# df_metrics = pd.read_csv(datapath + metricsfile)\nprint(df_metrics.shape)\ndf_metrics.head(3)","metadata":{"_uuid":"21b0b799-9a72-4ac1-8d77-ce6c447cd7e3","_cell_guid":"c0ed1805-0a5b-4914-9e64-d2e365eb9ae6","collapsed":false,"id":"4lr5975TJx3s","outputId":"03e8346a-728f-473f-fd7c-9270a47f90db","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:05.259296Z","iopub.execute_input":"2023-10-19T17:51:05.259581Z","iopub.status.idle":"2023-10-19T17:51:05.290348Z","shell.execute_reply.started":"2023-10-19T17:51:05.259552Z","shell.execute_reply":"2023-10-19T17:51:05.289566Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"09.12-robertaPredicted_Ans_Score_Zeroshot.csv\n(1102, 10)\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"   Ques_Id     BLEU1  ROUGE1_P  ROUGE1_R  ROUGE1_F1  ROUGEL_P  ROUGEL_R  \\\n0      0.0  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n1      1.0  0.048441  0.236842  0.219512   0.227848  0.184211  0.170732   \n2      2.0  0.045872  0.285714  0.235294   0.258065  0.166667  0.137255   \n\n   ROUGE1_F1  Precision    Recall  \n0   0.000000   0.000000  0.000000  \n1   0.177215   0.166667  0.177778  \n2   0.150538   0.245283  0.216667  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ques_Id</th>\n      <th>BLEU1</th>\n      <th>ROUGE1_P</th>\n      <th>ROUGE1_R</th>\n      <th>ROUGE1_F1</th>\n      <th>ROUGEL_P</th>\n      <th>ROUGEL_R</th>\n      <th>ROUGE1_F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.048441</td>\n      <td>0.236842</td>\n      <td>0.219512</td>\n      <td>0.227848</td>\n      <td>0.184211</td>\n      <td>0.170732</td>\n      <td>0.177215</td>\n      <td>0.166667</td>\n      <td>0.177778</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>0.045872</td>\n      <td>0.285714</td>\n      <td>0.235294</td>\n      <td>0.258065</td>\n      <td>0.166667</td>\n      <td>0.137255</td>\n      <td>0.150538</td>\n      <td>0.245283</td>\n      <td>0.216667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_metrics = pd.read_csv(conf['REPORT_FOLDER'] + metricsfile)\nprint(df_metrics.shape)\ndf_metrics.head(3)","metadata":{"_uuid":"44be979a-91a7-415a-855c-a1c0f6649f9c","_cell_guid":"b53e311d-a9a7-46f0-a240-013b877cc064","collapsed":false,"id":"XnnnOCeInQSy","outputId":"c1d8f005-3c57-4588-9f53-ce3332e59288","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:05.291722Z","iopub.execute_input":"2023-10-19T17:51:05.292469Z","iopub.status.idle":"2023-10-19T17:51:05.321939Z","shell.execute_reply.started":"2023-10-19T17:51:05.292422Z","shell.execute_reply":"2023-10-19T17:51:05.321096Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"(1102, 10)\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"   Ques_Id     BLEU1  ROUGE1_P  ROUGE1_R  ROUGE1_F1  ROUGEL_P  ROUGEL_R  \\\n0      0.0  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n1      1.0  0.048441  0.236842  0.219512   0.227848  0.184211  0.170732   \n2      2.0  0.045872  0.285714  0.235294   0.258065  0.166667  0.137255   \n\n   ROUGE1_F1.1  Precision    Recall  \n0     0.000000   0.000000  0.000000  \n1     0.177215   0.166667  0.177778  \n2     0.150538   0.245283  0.216667  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ques_Id</th>\n      <th>BLEU1</th>\n      <th>ROUGE1_P</th>\n      <th>ROUGE1_R</th>\n      <th>ROUGE1_F1</th>\n      <th>ROUGEL_P</th>\n      <th>ROUGEL_R</th>\n      <th>ROUGE1_F1.1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.048441</td>\n      <td>0.236842</td>\n      <td>0.219512</td>\n      <td>0.227848</td>\n      <td>0.184211</td>\n      <td>0.170732</td>\n      <td>0.177215</td>\n      <td>0.166667</td>\n      <td>0.177778</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>0.045872</td>\n      <td>0.285714</td>\n      <td>0.235294</td>\n      <td>0.258065</td>\n      <td>0.166667</td>\n      <td>0.137255</td>\n      <td>0.150538</td>\n      <td>0.245283</td>\n      <td>0.216667</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Calculate Cosine between Predicted and Reference Answer","metadata":{"_uuid":"7efe73e8-9ec6-4f8d-84d4-6ba6a7691b4d","_cell_guid":"c685e3b4-1965-4562-9a48-94f0db2f6bf9","id":"lYkxrY-LF4HN","trusted":true}},{"cell_type":"code","source":"filenm = '07.2-HBQA_QA_Vector' + embmodelname1 + '.csv'\n","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:51:05.323158Z","iopub.execute_input":"2023-10-19T17:51:05.323874Z","iopub.status.idle":"2023-10-19T17:51:05.327669Z","shell.execute_reply.started":"2023-10-19T17:51:05.323840Z","shell.execute_reply":"2023-10-19T17:51:05.326775Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# filenm = r'07.2-HBQA_Cosine_multi-qa-mpnet-base-dot-v1.csv'\nfilenm = '07.2-HBQA_QA_Vector' + embmodelname1 + '.csv'\nfilenm = '07-2-hbqa-cosine-multi-qa-mpnet-base-dot-v1/07.2-HBQA_Cosine_multi-qa-mpnet-base-dot-v1.csv'\n# if kaggle: filenm = filenm.split(\".\")[0].lower().replace('_','-') +'/' + filenm\n    \ndf_ref_ans = pd.read_csv(conf['PE_FOLDER'] + filenm)\n\ndf_ref_pred= df_pred.merge(df_ref_ans, on = \"Ques_Id\", how=\"inner\")[['Ques_Id','Pred_Answer_Vector','AnsVector_multi-qa-mpnet-base-dot-v1']]","metadata":{"_uuid":"e3326bd2-d63d-4d18-a7ab-cc6df5e0b836","_cell_guid":"df3e1227-178c-449c-b4c4-b3dc8dba56d6","collapsed":false,"id":"MWGLBQNnwndJ","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:05.328803Z","iopub.execute_input":"2023-10-19T17:51:05.329040Z","iopub.status.idle":"2023-10-19T17:51:06.097812Z","shell.execute_reply.started":"2023-10-19T17:51:05.329021Z","shell.execute_reply":"2023-10-19T17:51:06.097039Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def calculate_cosine(row):\n\n    if type(row['Pred_Answer_Vector'])!=type(list()):\n      predAns_vector_values = ast.literal_eval(row['Pred_Answer_Vector'])\n    else:\n      predAns_vector_values = row['Pred_Answer_Vector']\n\n    if type(row['AnsVector' + embmodelname1])!=type(list()):\n      refAns_vector_values = ast.literal_eval(row['AnsVector' + embmodelname1])\n    else:\n      refAns_vector_values = row['AnsVector' + embmodelname1]\n\n\n    # Convert the string values to floats\n    predAns_vector_values = [float(value) for value in predAns_vector_values]\n    refAns_vector_values = [float(value) for value in refAns_vector_values]\n\n    # Convert the lists to PyTorch tensors\n    predAns_vector_values = torch.tensor(predAns_vector_values).reshape(1, -1)\n    refAns_vector_values = torch.tensor(refAns_vector_values).reshape(1, -1)\n\n    similarity = cosine_similarity(predAns_vector_values, refAns_vector_values)\n    return similarity.item()","metadata":{"_uuid":"c5c31abe-6970-40f7-aac2-1b36ba580b76","_cell_guid":"b374c618-ee2d-46ba-a74f-fdfdbb4c370f","collapsed":false,"id":"n1RZRZePCdwG","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:06.098867Z","iopub.execute_input":"2023-10-19T17:51:06.099125Z","iopub.status.idle":"2023-10-19T17:51:06.105294Z","shell.execute_reply.started":"2023-10-19T17:51:06.099103Z","shell.execute_reply":"2023-10-19T17:51:06.104421Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"df_ref_pred['Cosine'] = df_ref_pred.apply(calculate_cosine, axis=1)","metadata":{"_uuid":"17b021f6-e781-4980-9b56-e677a97a92fc","_cell_guid":"50b87da7-0fe9-4bac-a7c0-7c82dfeae8b4","collapsed":false,"id":"_rvUtkwJChL_","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:06.106685Z","iopub.execute_input":"2023-10-19T17:51:06.106954Z","iopub.status.idle":"2023-10-19T17:51:09.889728Z","shell.execute_reply.started":"2023-10-19T17:51:06.106934Z","shell.execute_reply":"2023-10-19T17:51:09.888449Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"df_ref_pred.head(2)","metadata":{"_uuid":"c9848fb1-bf29-4a8a-ab28-d1b458a7c43c","_cell_guid":"a877c1f7-bb5c-4183-be04-be403dc780ea","collapsed":false,"id":"Dm521szWDEae","outputId":"fd6f8cc0-3ad7-4597-b3ef-f5f15cc5d6c6","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:09.891294Z","iopub.execute_input":"2023-10-19T17:51:09.892234Z","iopub.status.idle":"2023-10-19T17:51:09.905210Z","shell.execute_reply.started":"2023-10-19T17:51:09.892196Z","shell.execute_reply":"2023-10-19T17:51:09.904636Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"   Ques_Id                                 Pred_Answer_Vector  \\\n0        0  [-0.3637753129005432, -0.34021222591400146, -0...   \n1        1  [0.19928711652755737, -0.4345826804637909, -0....   \n\n                AnsVector_multi-qa-mpnet-base-dot-v1    Cosine  \n0  [-0.316677063703537, 0.04548855870962143, -0.2...  0.240315  \n1  [-0.013333199545741081, 0.05390685796737671, -...  0.429423  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ques_Id</th>\n      <th>Pred_Answer_Vector</th>\n      <th>AnsVector_multi-qa-mpnet-base-dot-v1</th>\n      <th>Cosine</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[-0.3637753129005432, -0.34021222591400146, -0...</td>\n      <td>[-0.316677063703537, 0.04548855870962143, -0.2...</td>\n      <td>0.240315</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[0.19928711652755737, -0.4345826804637909, -0....</td>\n      <td>[-0.013333199545741081, 0.05390685796737671, -...</td>\n      <td>0.429423</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_metrics = df_metrics.merge(df_ref_pred, on=\"Ques_Id\", how=\"left\")[['Ques_Id', 'BLEU1', 'ROUGE1_P', 'ROUGE1_R', 'ROUGE1_F1', 'ROUGEL_P',\n       'ROUGEL_R', 'ROUGE1_F1.1', 'Precision', 'Recall','Cosine']]","metadata":{"_uuid":"c03497bd-4fad-47d9-a6c3-ded8abe05270","_cell_guid":"dc5ff956-a385-4c03-8bbd-7e530482711a","collapsed":false,"id":"QEguQujGD5Nc","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:09.906492Z","iopub.execute_input":"2023-10-19T17:51:09.906852Z","iopub.status.idle":"2023-10-19T17:51:11.543670Z","shell.execute_reply.started":"2023-10-19T17:51:09.906817Z","shell.execute_reply":"2023-10-19T17:51:11.542554Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"# Save File Metrics & Show Summary","metadata":{"_uuid":"8c5006c6-8e8a-45f5-af53-a35fe13c1bc2","_cell_guid":"a8f995b3-fe3a-4168-8220-abf4e85c4d66","id":"G_Ww_QeMGCV6","trusted":true}},{"cell_type":"code","source":"metricsfile = '09.12-' + predmodel_name +'Predicted_Ans_Score_Zeroshot.csv'\nprint(metricsfile)\ndf_metrics.to_csv(conf['REPORT_FOLDER']+ metricsfile, index=False)\ndf_metrics.head(2)","metadata":{"_uuid":"3a2f26dd-da99-4422-a1c9-2d967f08be90","_cell_guid":"d0d08398-5ca7-42b4-9a69-b34589f9807d","collapsed":false,"id":"UTO0g10sEpz4","outputId":"ab9c9ee7-daec-4f18-89cd-6f27f19ec234","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:11.544873Z","iopub.execute_input":"2023-10-19T17:51:11.545145Z","iopub.status.idle":"2023-10-19T17:51:11.581166Z","shell.execute_reply.started":"2023-10-19T17:51:11.545124Z","shell.execute_reply":"2023-10-19T17:51:11.580349Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"09.12-robertaPredicted_Ans_Score_Zeroshot.csv\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"   Ques_Id     BLEU1  ROUGE1_P  ROUGE1_R  ROUGE1_F1  ROUGEL_P  ROUGEL_R  \\\n0      0.0  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000   \n1      1.0  0.048441  0.236842  0.219512   0.227848  0.184211  0.170732   \n\n   ROUGE1_F1.1  Precision    Recall    Cosine  \n0     0.000000   0.000000  0.000000  0.240315  \n1     0.177215   0.166667  0.177778  0.429423  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ques_Id</th>\n      <th>BLEU1</th>\n      <th>ROUGE1_P</th>\n      <th>ROUGE1_R</th>\n      <th>ROUGE1_F1</th>\n      <th>ROUGEL_P</th>\n      <th>ROUGEL_R</th>\n      <th>ROUGE1_F1.1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Cosine</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.240315</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.048441</td>\n      <td>0.236842</td>\n      <td>0.219512</td>\n      <td>0.227848</td>\n      <td>0.184211</td>\n      <td>0.170732</td>\n      <td>0.177215</td>\n      <td>0.166667</td>\n      <td>0.177778</td>\n      <td>0.429423</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"all_metrics = pd.DataFrame()\nall_metrics[predmodel_name] = df_metrics.mean()[1:]","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:54:58.669558Z","iopub.execute_input":"2023-10-19T17:54:58.670250Z","iopub.status.idle":"2023-10-19T17:54:58.677692Z","shell.execute_reply.started":"2023-10-19T17:54:58.670224Z","shell.execute_reply":"2023-10-19T17:54:58.676825Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"all_metrics","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:55:10.286902Z","iopub.execute_input":"2023-10-19T17:55:10.287279Z","iopub.status.idle":"2023-10-19T17:55:10.295524Z","shell.execute_reply.started":"2023-10-19T17:55:10.287241Z","shell.execute_reply":"2023-10-19T17:55:10.294500Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"              roberta\nBLEU1        0.030853\nROUGE1_P     0.131937\nROUGE1_R     0.122343\nROUGE1_F1    0.119923\nROUGEL_P     0.088812\nROUGEL_R     0.082219\nROUGE1_F1.1  0.080415\nPrecision    0.096112\nRecall       0.092252\nCosine       0.370905","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>roberta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BLEU1</th>\n      <td>0.030853</td>\n    </tr>\n    <tr>\n      <th>ROUGE1_P</th>\n      <td>0.131937</td>\n    </tr>\n    <tr>\n      <th>ROUGE1_R</th>\n      <td>0.122343</td>\n    </tr>\n    <tr>\n      <th>ROUGE1_F1</th>\n      <td>0.119923</td>\n    </tr>\n    <tr>\n      <th>ROUGEL_P</th>\n      <td>0.088812</td>\n    </tr>\n    <tr>\n      <th>ROUGEL_R</th>\n      <td>0.082219</td>\n    </tr>\n    <tr>\n      <th>ROUGE1_F1.1</th>\n      <td>0.080415</td>\n    </tr>\n    <tr>\n      <th>Precision</th>\n      <td>0.096112</td>\n    </tr>\n    <tr>\n      <th>Recall</th>\n      <td>0.092252</td>\n    </tr>\n    <tr>\n      <th>Cosine</th>\n      <td>0.370905</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"all_metrics.to_csv(\"all_metrics1.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:55:20.207497Z","iopub.execute_input":"2023-10-19T17:55:20.208108Z","iopub.status.idle":"2023-10-19T17:55:20.213348Z","shell.execute_reply.started":"2023-10-19T17:55:20.208073Z","shell.execute_reply":"2023-10-19T17:55:20.212432Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"df_metrics.mean()","metadata":{"_uuid":"9026e357-f053-4f7e-a24b-9813d594cc41","_cell_guid":"2df52e5f-0e84-4182-a55a-ce6efeef372e","collapsed":false,"id":"2sTPBYLYFw8F","outputId":"085388e4-f387-4815-f5f5-129a5cddf31e","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-19T17:51:12.293333Z","iopub.status.idle":"2023-10-19T17:51:12.293613Z","shell.execute_reply.started":"2023-10-19T17:51:12.293481Z","shell.execute_reply":"2023-10-19T17:51:12.293493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:55:36.029838Z","iopub.execute_input":"2023-10-19T17:55:36.030602Z","iopub.status.idle":"2023-10-19T17:55:38.179544Z","shell.execute_reply.started":"2023-10-19T17:55:36.030573Z","shell.execute_reply":"2023-10-19T17:55:38.178684Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/09.11-robertaPredicted_Ans-Zeroshot.csv (deflated 69%)\n  adding: kaggle/working/09.12-robertaPredicted_Ans_Score_Zeroshot.csv (deflated 74%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/all_metrics1.csv (deflated 38%)\n  adding: kaggle/working/09.11-robertaPredicted_AnsVec-Zeroshot.csv (deflated 73%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2023-10-19T17:55:41.783581Z","iopub.execute_input":"2023-10-19T17:55:41.783961Z","iopub.status.idle":"2023-10-19T17:55:41.791053Z","shell.execute_reply.started":"2023-10-19T17:55:41.783935Z","shell.execute_reply":"2023-10-19T17:55:41.790085Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}]}]}