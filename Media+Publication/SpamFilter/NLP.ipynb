{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://www.gutenberg.org/files/11/11-0.txt\"\n",
    "import requests\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "alice= requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.downloader as downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloader.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alice.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alice.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_words = alice.text.split()\n",
    "fd = FreqDist(alice_words)\n",
    "most_words = fd.most_common(10)\n",
    "import seaborn as sns\n",
    "x = [element[0] for element in most_words]\n",
    "y = [element[1] for element in most_words]\n",
    "sns.barplot(x=x, y=y)\n",
    "#print(fd.keys(), fd.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/files/16/16-0.txt\"\n",
    "peter_pan = requests.get(url).text\n",
    "\n",
    "# break the book into different words using the split() method\n",
    "peter_pan_words = peter_pan.split(' ')# write your code here\n",
    "\n",
    "# build frequency distribution using NLTK's FreqDist() function\n",
    "word_frequency = FreqDist(peter_pan_words)# write your code here\n",
    "\n",
    "# extract the frequency of third most frequent word\n",
    "freq = word_frequency.most_common(3)\n",
    "\n",
    "# print the third most frequent word - don't change the following code, it is used to evaluate the code\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2030), ('and', 1212), ('to', 1100)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = word_frequency.most_common(3)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2030"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-75762b5acd74>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-75762b5acd74>\"\u001b[1;36m, line \u001b[1;32m19\u001b[0m\n\u001b[1;33m    no_stops = # write code here\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load the ebook\n",
    "url = \"https://www.gutenberg.org/files/16/16-0.txt\"\n",
    "peter_pan = requests.get(url).text\n",
    "\n",
    "# break the book into different words using the split() method\n",
    "peter_pan_words = peter_pan.split()\n",
    "\n",
    "# build frequency distribution using NLTK's FreqDist() function\n",
    "word_frequency = FreqDist(peter_pan_words)\n",
    "\n",
    "# extract nltk stop word list\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# remove 'stopwords' from 'peter_pan_words'\n",
    "no_stops = [word for word in word_frequency if word not in stopwords]  # write code here\n",
    "\n",
    "# create word frequency of no_stops\n",
    "word_frequency = for # write code here\n",
    "\n",
    "# extract the most frequent word and its frequency\n",
    "frequency = word_frequency.most_common(1)[0][1]\n",
    "\n",
    "# print the third most frequent word - don't change the following code, it is used to evaluate the code\n",
    "print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpora'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import ast, sys\n",
    "sentence = sys.stdin.read()\n",
    "\n",
    "# tokenise sentence into words\n",
    "words = word_tokenize(sentence)# write your code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#Write a piece of code that breaks a given sentence into words and stores them in a list. \n",
    "#Then remove the stop words from this list and then print the list as well as the length of the list. \n",
    "#Again, use the NLTK tokeniser to do this.\n",
    "\n",
    "#Sample input: \n",
    "#“Education is the most powerful weapon that you can use to change the world”\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast, sys\n",
    "sentence = sys.stdin.read()\n",
    "sentence=\"Education is the most powerful weapon that you can use to change the world\"\n",
    "# change sentence to lowercase\n",
    "sentence = sentence.lower()# write code here\n",
    "\n",
    "# tokenise sentence into words\n",
    "words = word_tokenize(sentence) # write code here\n",
    "\n",
    "# extract nltk stop word list\n",
    "stopwords =stopwords.words(\"english\") # write code here\n",
    "\n",
    "# remove stop words\n",
    "no_stops = [word for word in words if word not in stopwords]# write code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(no_stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "#Description\n",
    "#Write a Python code using the NLTK library that breaks a given piece of text containing multiple sentences \n",
    "#into different sentences. Finally print the total number of sentences in the text.\n",
    "\n",
    "#Sample input: \n",
    "#Develop a passion for your learning. If you do, you’ll never cease to grow.\n",
    "\n",
    "#Expected output:\n",
    "#2\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast, sys\n",
    "text = sys.stdin.read()\n",
    "\n",
    "text=\"Develop a passion for your learning. If you do, you’ll never cease to grow.\"\n",
    "# change sentence to lowercase\n",
    "text = text.lower() #write code here\n",
    "\n",
    "# tokenise sentence into words\n",
    "sentences = sent_tokenize(text)# write code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use NLTK’s regex tokeniser to extract all the mentions from a given tweet and then print the total number of mentions. \n",
    "#A mention comprises of a ‘@’ symbol followed by a username containing either alphabets, numbers or underscores.\n",
    "\n",
    "#Sample tweet:\n",
    "#So excited to be a part of machine learning and artificial intelligence program made by @upgrad and @iiitb\n",
    "\n",
    "#Expected output:\n",
    "#2 (because there are two mentions - ‘@upgrad’ and ‘@iiitb’ )\n",
    "\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ast, sys\n",
    "text = sys.stdin.read()\n",
    "\n",
    "# change text to lowercase\n",
    "text = text.lower()# write code here\n",
    "\n",
    "# pattern to extract mentions\n",
    "pattern = \"@[\\w]+\"# write regex pattern here\n",
    "\n",
    "# extract mentions by using regex tokeniser\n",
    "mentions = regexp_tokenize( text, pattern)# write code here\n",
    "\n",
    "# print length - don't change the following piece of code\n",
    "print(len(mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
